{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "deep-learning",
   "display_name": "deep-learning"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_c = DeformConv(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DeformConv(\n  (offset_net): Conv2d(28, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (deform_conv): DeformConv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n)\n"
    }
   ],
   "source": [
    "print(d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0702, -0.0019,  0.0386,  0.0121, -0.0670, -0.0550,  0.0443,  0.0633,\n         -0.0153, -0.0361]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_ = Model()\n",
    "x = torch.ones([1, 1, 28, 28])\n",
    "net_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_tforms(rotation=0, translation=(0,0)):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomAffine(rotation, translate=translation),\n",
    "        transforms.RandomCrop(30, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.RandomAffine(rotation, translate=translation),\n",
    "        transforms.CenterCrop(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    return train_transform, test_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(rotation=0, translation=(0,0), dataset=datasets.MNIST):\n",
    "    if dataset == datasets.CIFAR10:\n",
    "        train_tform, test_tform = get_cifar10_tforms(rotation=rotation, translation=translation)\n",
    "    elif dataset == datasets.MNIST:\n",
    "        train_tform = transforms.Compose([\n",
    "                transforms.RandomAffine(rotation, translate=translation),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "        test_tform = transforms.Compose([\n",
    "                transforms.RandomAffine(0, translate=translation),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown Dataset type!\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset(\n",
    "            root='../../../data/', \n",
    "            train=True, \n",
    "            transform=train_tform\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "        \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset(\n",
    "            root='../../../data/', \n",
    "            train=False, \n",
    "            transform=test_tform\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader):\n",
    "    num_correct = 0\n",
    "    avg_loss = 0\n",
    "    for step, (data, targets) in enumerate(test_loader):\n",
    "        data = data.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(data)\n",
    "\n",
    "        loss = loss_fn(preds, targets)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        _, pred_labels = torch.max(preds, dim=1)\n",
    "        \n",
    "        num_correct += pred_labels.eq(targets).sum().item()\n",
    "\n",
    "    # Because used a sum, make sure to divide by the length of total number in dataset\n",
    "    acc = num_correct/len(test_loader.dataset)\n",
    "    avg_loss /= len(test_loader)\n",
    "\n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, train_loader, test_loader, writer, num_epochs, log_freq):\n",
    "    batch_count = 0\n",
    "    for epoch in range (num_epochs):\n",
    "        tot_loss = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            batch_count += 1\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(inputs)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if i % log_freq == (log_freq - 1):\n",
    "                writer.add_scalar('train_loss', loss.item(), global_step=batch_count)\n",
    "        \n",
    "        acc, avg_loss = test(model, loss_fn, test_loader)\n",
    "        writer.add_scalar('test_loss', avg_loss, global_step=epoch)\n",
    "        writer.add_scalar('test_acc', acc, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, run_name, train_loader, test_loader, inp_channels=1, log_freq=100):\n",
    "    net = model(inp_channels).cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter('./runs/%s'%run_name)\n",
    "    train(net, loss_fn, optimizer, train_loader, test_loader, writer, 10, log_freq)\n",
    "    acc, _ = test(net, loss_fn, test_loader)\n",
    "    print('Accuracy: %s'%(acc*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 98.82%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders()\n",
    "train_model(Model, 'MNIST_normal', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 99.1%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders()\n",
    "train_model(DeformedConvModel, 'MNIST_deformed', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 95.78%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=120)\n",
    "train_model(Model, 'rotated_MNIST_normal', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 97.1%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=120)\n",
    "train_model(DeformedConvModel, 'rotated_MNIST_deformed', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 95.71%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(translation=(0.3, 0.3))\n",
    "train_model(Model, 'translated_MNIST_normal', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 98.27%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(translation=(0.3, 0.3))\n",
    "train_model(DeformedConvModel, 'translated_MNIST_deformed', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 87.28%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=120, translation=(0.3, 0.3))\n",
    "train_model(Model, 'trans_rot_MNIST_normal', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 94.03%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=120, translation=(0.3, 0.3))\n",
    "train_model(DeformedConvModel, 'trans_rot_MNIST_deformed', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model(\n  (features): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n    (4): ReLU()\n    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (7): ReLU()\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n    (10): ReLU()\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): AvgPool2d(kernel_size=4, stride=4, padding=0)\n  )\n  (classify): Linear(in_features=128, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 10])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(inp_channels=3)(torch.ones(64, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 67.36%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(dataset=datasets.CIFAR10)\n",
    "train_model(Model, 'CIFAR10_normal', train_loader, test_loader, inp_channels=3, log_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 128, 1, 1])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeformedConvModel(inp_channels=1).features(torch.ones(64, 1, 30, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 68.52000000000001%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(dataset=datasets.CIFAR10)\n",
    "train_model(DeformedConvModel, 'CIFAR10_deformed', train_loader, test_loader, inp_channels=3, log_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 53.010000000000005%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=90, dataset=datasets.CIFAR10)\n",
    "train_model(Model, 'rotated_CIFAR10_normal', train_loader, test_loader, inp_channels=3, log_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy: 55.16%\n"
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(rotation=90, dataset=datasets.CIFAR10)\n",
    "train_model(DeformedConvModel, 'rotated_CIFAR10_deformed', train_loader, test_loader, inp_channels=3, log_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}