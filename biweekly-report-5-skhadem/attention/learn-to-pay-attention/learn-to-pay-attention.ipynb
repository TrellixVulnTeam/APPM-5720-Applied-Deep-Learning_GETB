{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "deep-learning",
   "display_name": "deep-learning"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code largely adapted from: https://github.com/SaoYan/LearnToPayAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_xavierUniform(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, val=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Defining a block that is conv-> batch norm -> Relu base on the dimensions passed in\"\"\"\n",
    "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        features = [in_features] + [out_features for i in range(num_conv)]\n",
    "        layers = []\n",
    "        for i in range(len(features)-1):\n",
    "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
    "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU())\n",
    "            if pool:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        self.op = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class ProjectorBlock(nn.Module):\n",
    "    \"\"\"Block to project to different dimensions. Essentially just wraps a 2D Conv\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProjectorBlock, self).__init__()\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1, padding=0, bias=False)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.op(inputs)\n",
    "\n",
    "class LinearAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates the 2D matrix that compares local to global features\"\"\"\n",
    "    def __init__(self, in_features, normalize_attn=True):\n",
    "        super(LinearAttentionBlock, self).__init__()\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n",
    "    \n",
    "    def forward(self, l, g):\n",
    "        \"\"\"\n",
    "        l: local features\n",
    "        g:: global features (at end of network)\n",
    "        \"\"\"\n",
    "        N, C, W, H = l.size()\n",
    "        c = self.op(l+g) # batch_sizex1xWxH\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        g = torch.mul(a.expand_as(l), l)\n",
    "        if self.normalize_attn:\n",
    "            g = g.view(N,C,-1).sum(dim=2) # batch_sizexC\n",
    "        else:\n",
    "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
    "            \n",
    "        return c.view(N,1,W,H), g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnVGG(nn.Module):\n",
    "    \"\"\"Main network\"\"\"\n",
    "    def __init__(self, im_size, num_classes, normalize_attn=True):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        # conv blocks\n",
    "        self.conv_block1 = ConvBlock(3, 64, 2)\n",
    "        self.conv_block2 = ConvBlock(64, 128, 2)\n",
    "        self.conv_block3 = ConvBlock(128, 256, 3)\n",
    "        self.conv_block4 = ConvBlock(256, 512, 3)\n",
    "        self.conv_block5 = ConvBlock(512, 512, 3)\n",
    "        self.conv_block6 = ConvBlock(512, 512, 2, pool=True)\n",
    "        self.dense = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=int(im_size/32), padding=0, bias=True)\n",
    "\n",
    "        # Projectors & Compatibility functions\n",
    "        self.projector = ProjectorBlock(256, 512)\n",
    "        self.attn1 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
    "        self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
    "        self.attn3 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
    "\n",
    "        # final classification layer, using the combination of local features and attention map\n",
    "        self.classify = nn.Linear(in_features=512*3, out_features=num_classes, bias=True)\n",
    "        \n",
    "        # initialize\n",
    "        weights_init_xavierUniform(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feed forward\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        l1 = self.conv_block3(x) # /1\n",
    "        x = F.max_pool2d(l1, kernel_size=2, stride=2, padding=0) # /2\n",
    "        l2 = self.conv_block4(x) # /2\n",
    "        x = F.max_pool2d(l2, kernel_size=2, stride=2, padding=0) # /4\n",
    "        l3 = self.conv_block5(x) # /4\n",
    "        x = F.max_pool2d(l3, kernel_size=2, stride=2, padding=0) # /8\n",
    "        x = self.conv_block6(x) # /32\n",
    "        g = self.dense(x) # batch_sizex512x1x1\n",
    "        # pay attention\n",
    "        c1, g1 = self.attn1(self.projector(l1), g)\n",
    "        c2, g2 = self.attn2(l2, g)\n",
    "        c3, g3 = self.attn3(l3, g)\n",
    "        g = torch.cat((g1,g2,g3), dim=1) # batch_sizexC\n",
    "        # classification layer\n",
    "        x = self.classify(g) # batch_sizexnum_classes\n",
    "        \n",
    "        return [x, c1, c2, c3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "num_aug = 3\n",
    "im_size = 32\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(im_size, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR100(root='../../../data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8) #, worker_init_fn=_init_fn)\n",
    "test_set = torchvision.datasets.CIFAR100(root='../../../data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnVGG(im_size=im_size, num_classes=100, normalize_attn=False)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[60/390] loss 0.1156 accuracy 98.44%\n[epoch 71][90/390] loss 0.1252 accuracy 100.00%\n[epoch 71][120/390] loss 0.1499 accuracy 98.44%\n[epoch 71][150/390] loss 0.1691 accuracy 99.22%\n[epoch 71][180/390] loss 0.1174 accuracy 99.22%\n[epoch 71][210/390] loss 0.1216 accuracy 96.88%\n[epoch 71][240/390] loss 0.1416 accuracy 96.09%\n[epoch 71][270/390] loss 0.2021 accuracy 97.66%\n[epoch 71][300/390] loss 0.1426 accuracy 94.53%\n[epoch 71][330/390] loss 0.3314 accuracy 88.28%\n[epoch 71][360/390] loss 0.2231 accuracy 97.66%\n[epoch 71][390/390] loss 0.1460 accuracy 98.75%\n----------------------------------------\n\n[epoch 71] accuracy on test data: 67.49%\n\n----------------------------------------\n\nepoch 72 learning rate 0.025000\n\n[epoch 72][0/390] loss 0.1722 accuracy 96.09%\n[epoch 72][30/390] loss 0.1870 accuracy 96.09%\n[epoch 72][60/390] loss 0.0925 accuracy 90.62%\n[epoch 72][90/390] loss 0.1590 accuracy 97.66%\n[epoch 72][120/390] loss 0.2195 accuracy 98.44%\n[epoch 72][150/390] loss 0.0955 accuracy 97.66%\n[epoch 72][180/390] loss 0.1520 accuracy 96.88%\n[epoch 72][210/390] loss 0.2805 accuracy 96.88%\n[epoch 72][240/390] loss 0.1314 accuracy 98.44%\n[epoch 72][270/390] loss 0.2075 accuracy 96.88%\n[epoch 72][300/390] loss 0.2093 accuracy 96.09%\n[epoch 72][330/390] loss 0.2205 accuracy 95.31%\n[epoch 72][360/390] loss 0.2191 accuracy 96.09%\n[epoch 72][390/390] loss 0.2693 accuracy 98.75%\n----------------------------------------\n\n[epoch 72] accuracy on test data: 65.83%\n\n----------------------------------------\n\nepoch 73 learning rate 0.025000\n\n[epoch 73][0/390] loss 0.1783 accuracy 92.97%\n[epoch 73][30/390] loss 0.1840 accuracy 94.53%\n[epoch 73][60/390] loss 0.1487 accuracy 95.31%\n[epoch 73][90/390] loss 0.0953 accuracy 99.22%\n[epoch 73][120/390] loss 0.1405 accuracy 100.00%\n[epoch 73][150/390] loss 0.2148 accuracy 96.88%\n[epoch 73][180/390] loss 0.1513 accuracy 100.00%\n[epoch 73][210/390] loss 0.2318 accuracy 89.84%\n[epoch 73][240/390] loss 0.2375 accuracy 94.53%\n[epoch 73][270/390] loss 0.1665 accuracy 96.09%\n[epoch 73][300/390] loss 0.1944 accuracy 96.09%\n[epoch 73][330/390] loss 0.2110 accuracy 99.22%\n[epoch 73][360/390] loss 0.2787 accuracy 91.41%\n[epoch 73][390/390] loss 0.2875 accuracy 93.75%\n----------------------------------------\n\n[epoch 73] accuracy on test data: 64.70%\n\n----------------------------------------\n\nepoch 74 learning rate 0.012500\n\n[epoch 74][0/390] loss 0.2612 accuracy 94.53%\n[epoch 74][30/390] loss 0.0868 accuracy 98.44%\n[epoch 74][60/390] loss 0.0992 accuracy 100.00%\n[epoch 74][90/390] loss 0.0475 accuracy 100.00%\n[epoch 74][120/390] loss 0.0571 accuracy 100.00%\n[epoch 74][150/390] loss 0.0816 accuracy 99.22%\n[epoch 74][180/390] loss 0.0790 accuracy 100.00%\n[epoch 74][210/390] loss 0.0829 accuracy 100.00%\n[epoch 74][240/390] loss 0.0379 accuracy 100.00%\n[epoch 74][270/390] loss 0.0595 accuracy 99.22%\n[epoch 74][300/390] loss 0.0286 accuracy 100.00%\n[epoch 74][330/390] loss 0.0586 accuracy 100.00%\n[epoch 74][360/390] loss 0.0166 accuracy 100.00%\n[epoch 74][390/390] loss 0.0280 accuracy 100.00%\n----------------------------------------\n\n[epoch 74] accuracy on test data: 76.95%\n\n----------------------------------------\n\nepoch 75 learning rate 0.012500\n\n[epoch 75][0/390] loss 0.0218 accuracy 100.00%\n[epoch 75][30/390] loss 0.0253 accuracy 100.00%\n[epoch 75][60/390] loss 0.0267 accuracy 100.00%\n[epoch 75][90/390] loss 0.0332 accuracy 100.00%\n[epoch 75][120/390] loss 0.0383 accuracy 100.00%\n[epoch 75][150/390] loss 0.0266 accuracy 100.00%\n[epoch 75][180/390] loss 0.0312 accuracy 100.00%\n[epoch 75][210/390] loss 0.0241 accuracy 100.00%\n[epoch 75][240/390] loss 0.0214 accuracy 100.00%\n[epoch 75][270/390] loss 0.0278 accuracy 100.00%\n[epoch 75][300/390] loss 0.0416 accuracy 100.00%\n[epoch 75][330/390] loss 0.0233 accuracy 100.00%\n[epoch 75][360/390] loss 0.0220 accuracy 100.00%\n[epoch 75][390/390] loss 0.0171 accuracy 100.00%\n----------------------------------------\n\n[epoch 75] accuracy on test data: 77.38%\n\n----------------------------------------\n\nepoch 76 learning rate 0.012500\n\n[epoch 76][0/390] loss 0.0271 accuracy 100.00%\n[epoch 76][30/390] loss 0.0256 accuracy 100.00%\n[epoch 76][60/390] loss 0.0307 accuracy 100.00%\n[epoch 76][90/390] loss 0.0161 accuracy 100.00%\n[epoch 76][120/390] loss 0.0278 accuracy 100.00%\n[epoch 76][150/390] loss 0.0263 accuracy 100.00%\n[epoch 76][180/390] loss 0.0137 accuracy 100.00%\n[epoch 76][210/390] loss 0.0290 accuracy 100.00%\n[epoch 76][240/390] loss 0.0351 accuracy 100.00%\n[epoch 76][270/390] loss 0.0186 accuracy 100.00%\n[epoch 76][300/390] loss 0.0302 accuracy 100.00%\n[epoch 76][330/390] loss 0.0255 accuracy 100.00%\n[epoch 76][360/390] loss 0.0128 accuracy 100.00%\n[epoch 76][390/390] loss 0.0514 accuracy 100.00%\n----------------------------------------\n\n[epoch 76] accuracy on test data: 77.72%\n\n----------------------------------------\n\nepoch 77 learning rate 0.012500\n\n[epoch 77][0/390] loss 0.0207 accuracy 100.00%\n[epoch 77][30/390] loss 0.0126 accuracy 100.00%\n[epoch 77][60/390] loss 0.0434 accuracy 99.22%\n[epoch 77][90/390] loss 0.0170 accuracy 100.00%\n[epoch 77][120/390] loss 0.0212 accuracy 100.00%\n[epoch 77][150/390] loss 0.0212 accuracy 100.00%\n[epoch 77][180/390] loss 0.0183 accuracy 100.00%\n[epoch 77][210/390] loss 0.0133 accuracy 100.00%\n[epoch 77][240/390] loss 0.0126 accuracy 100.00%\n[epoch 77][270/390] loss 0.0142 accuracy 100.00%\n[epoch 77][300/390] loss 0.0182 accuracy 100.00%\n[epoch 77][330/390] loss 0.0198 accuracy 100.00%\n[epoch 77][360/390] loss 0.0155 accuracy 100.00%\n[epoch 77][390/390] loss 0.0145 accuracy 100.00%\n----------------------------------------\n\n[epoch 77] accuracy on test data: 77.21%\n\n----------------------------------------\n\nepoch 78 learning rate 0.012500\n\n[epoch 78][0/390] loss 0.0287 accuracy 100.00%\n[epoch 78][30/390] loss 0.0453 accuracy 99.22%\n[epoch 78][60/390] loss 0.0088 accuracy 100.00%\n[epoch 78][90/390] loss 0.0163 accuracy 100.00%\n[epoch 78][120/390] loss 0.0105 accuracy 100.00%\n[epoch 78][150/390] loss 0.0145 accuracy 100.00%\n[epoch 78][180/390] loss 0.0157 accuracy 100.00%\n[epoch 78][210/390] loss 0.0107 accuracy 100.00%\n[epoch 78][240/390] loss 0.0130 accuracy 100.00%\n[epoch 78][270/390] loss 0.0136 accuracy 100.00%\n[epoch 78][300/390] loss 0.0102 accuracy 100.00%\n[epoch 78][330/390] loss 0.0158 accuracy 100.00%\n[epoch 78][360/390] loss 0.0135 accuracy 100.00%\n[epoch 78][390/390] loss 0.0173 accuracy 100.00%\n----------------------------------------\n\n[epoch 78] accuracy on test data: 78.01%\n\n----------------------------------------\n\nepoch 79 learning rate 0.012500\n\n[epoch 79][0/390] loss 0.0116 accuracy 100.00%\n[epoch 79][30/390] loss 0.0165 accuracy 100.00%\n[epoch 79][60/390] loss 0.0113 accuracy 100.00%\n[epoch 79][90/390] loss 0.0090 accuracy 100.00%\n[epoch 79][120/390] loss 0.0184 accuracy 100.00%\n[epoch 79][150/390] loss 0.0113 accuracy 100.00%\n[epoch 79][180/390] loss 0.0111 accuracy 100.00%\n[epoch 79][210/390] loss 0.0207 accuracy 100.00%\n[epoch 79][240/390] loss 0.0139 accuracy 100.00%\n[epoch 79][270/390] loss 0.0111 accuracy 100.00%\n[epoch 79][300/390] loss 0.0196 accuracy 100.00%\n[epoch 79][330/390] loss 0.0121 accuracy 100.00%\n[epoch 79][360/390] loss 0.0134 accuracy 100.00%\n[epoch 79][390/390] loss 0.0208 accuracy 100.00%\n----------------------------------------\n\n[epoch 79] accuracy on test data: 77.80%\n\n----------------------------------------\n\nepoch 80 learning rate 0.012500\n\n[epoch 80][0/390] loss 0.0159 accuracy 100.00%\n[epoch 80][30/390] loss 0.0107 accuracy 100.00%\n[epoch 80][60/390] loss 0.0098 accuracy 100.00%\n[epoch 80][90/390] loss 0.0079 accuracy 100.00%\n[epoch 80][120/390] loss 0.0154 accuracy 100.00%\n[epoch 80][150/390] loss 0.0163 accuracy 100.00%\n[epoch 80][180/390] loss 0.0173 accuracy 100.00%\n[epoch 80][210/390] loss 0.0161 accuracy 100.00%\n[epoch 80][240/390] loss 0.0125 accuracy 100.00%\n[epoch 80][270/390] loss 0.0158 accuracy 100.00%\n[epoch 80][300/390] loss 0.0145 accuracy 100.00%\n[epoch 80][330/390] loss 0.0199 accuracy 100.00%\n[epoch 80][360/390] loss 0.0156 accuracy 100.00%\n[epoch 80][390/390] loss 0.0193 accuracy 100.00%\n----------------------------------------\n\n[epoch 80] accuracy on test data: 78.01%\n\n----------------------------------------\n\nepoch 81 learning rate 0.012500\n\n[epoch 81][0/390] loss 0.0254 accuracy 99.22%\n[epoch 81][30/390] loss 0.0084 accuracy 100.00%\n[epoch 81][60/390] loss 0.0119 accuracy 100.00%\n[epoch 81][90/390] loss 0.0160 accuracy 100.00%\n[epoch 81][120/390] loss 0.0124 accuracy 100.00%\n[epoch 81][150/390] loss 0.0136 accuracy 100.00%\n[epoch 81][180/390] loss 0.0103 accuracy 100.00%\n[epoch 81][210/390] loss 0.0108 accuracy 100.00%\n[epoch 81][240/390] loss 0.0172 accuracy 100.00%\n[epoch 81][270/390] loss 0.0155 accuracy 100.00%\n[epoch 81][300/390] loss 0.0437 accuracy 99.22%\n[epoch 81][330/390] loss 0.0081 accuracy 100.00%\n[epoch 81][360/390] loss 0.0344 accuracy 100.00%\n[epoch 81][390/390] loss 0.0210 accuracy 100.00%\n----------------------------------------\n\n[epoch 81] accuracy on test data: 78.29%\n\n----------------------------------------\n\nepoch 82 learning rate 0.012500\n\n[epoch 82][0/390] loss 0.0134 accuracy 100.00%\n[epoch 82][30/390] loss 0.0178 accuracy 100.00%\n[epoch 82][60/390] loss 0.0440 accuracy 99.22%\n[epoch 82][90/390] loss 0.0238 accuracy 100.00%\n[epoch 82][120/390] loss 0.0157 accuracy 100.00%\n[epoch 82][150/390] loss 0.0093 accuracy 100.00%\n[epoch 82][180/390] loss 0.0144 accuracy 100.00%\n[epoch 82][210/390] loss 0.0174 accuracy 100.00%\n[epoch 82][240/390] loss 0.0131 accuracy 100.00%\n[epoch 82][270/390] loss 0.0237 accuracy 100.00%\n[epoch 82][300/390] loss 0.0166 accuracy 100.00%\n[epoch 82][330/390] loss 0.0412 accuracy 99.22%\n[epoch 82][360/390] loss 0.0116 accuracy 100.00%\n[epoch 82][390/390] loss 0.0225 accuracy 100.00%\n----------------------------------------\n\n[epoch 82] accuracy on test data: 78.36%\n\n----------------------------------------\n\nepoch 83 learning rate 0.012500\n\n[epoch 83][0/390] loss 0.0133 accuracy 100.00%\n[epoch 83][30/390] loss 0.0222 accuracy 100.00%\n[epoch 83][60/390] loss 0.0071 accuracy 100.00%\n[epoch 83][90/390] loss 0.0139 accuracy 100.00%\n[epoch 83][120/390] loss 0.0180 accuracy 100.00%\n[epoch 83][150/390] loss 0.0113 accuracy 100.00%\n[epoch 83][180/390] loss 0.0149 accuracy 100.00%\n[epoch 83][210/390] loss 0.0119 accuracy 100.00%\n[epoch 83][240/390] loss 0.0152 accuracy 100.00%\n[epoch 83][270/390] loss 0.0108 accuracy 100.00%\n[epoch 83][300/390] loss 0.0157 accuracy 100.00%\n[epoch 83][330/390] loss 0.0277 accuracy 100.00%\n[epoch 83][360/390] loss 0.0203 accuracy 100.00%\n[epoch 83][390/390] loss 0.0204 accuracy 100.00%\n----------------------------------------\n\n[epoch 83] accuracy on test data: 78.40%\n\n----------------------------------------\n\nepoch 84 learning rate 0.012500\n\n[epoch 84][0/390] loss 0.0268 accuracy 100.00%\n[epoch 84][30/390] loss 0.0125 accuracy 100.00%\n[epoch 84][60/390] loss 0.0118 accuracy 100.00%\n[epoch 84][90/390] loss 0.0426 accuracy 100.00%\n[epoch 84][120/390] loss 0.0128 accuracy 100.00%\n[epoch 84][150/390] loss 0.0141 accuracy 100.00%\n[epoch 84][180/390] loss 0.0148 accuracy 100.00%\n[epoch 84][210/390] loss 0.0202 accuracy 100.00%\n[epoch 84][240/390] loss 0.0167 accuracy 100.00%\n[epoch 84][270/390] loss 0.0418 accuracy 99.22%\n[epoch 84][300/390] loss 0.0108 accuracy 100.00%\n[epoch 84][330/390] loss 0.0116 accuracy 100.00%\n[epoch 84][360/390] loss 0.0108 accuracy 100.00%\n[epoch 84][390/390] loss 0.0212 accuracy 100.00%\n----------------------------------------\n\n[epoch 84] accuracy on test data: 78.35%\n\n----------------------------------------\n\nepoch 85 learning rate 0.012500\n\n[epoch 85][0/390] loss 0.0200 accuracy 100.00%\n[epoch 85][30/390] loss 0.0129 accuracy 100.00%\n[epoch 85][60/390] loss 0.0125 accuracy 100.00%\n[epoch 85][90/390] loss 0.0140 accuracy 100.00%\n[epoch 85][120/390] loss 0.0170 accuracy 100.00%\n[epoch 85][150/390] loss 0.0430 accuracy 99.22%\n[epoch 85][180/390] loss 0.0115 accuracy 100.00%\n[epoch 85][210/390] loss 0.0124 accuracy 100.00%\n[epoch 85][240/390] loss 0.0162 accuracy 100.00%\n[epoch 85][270/390] loss 0.0231 accuracy 100.00%\n[epoch 85][300/390] loss 0.0140 accuracy 100.00%\n[epoch 85][330/390] loss 0.0084 accuracy 100.00%\n[epoch 85][360/390] loss 0.0204 accuracy 100.00%\n[epoch 85][390/390] loss 0.0324 accuracy 100.00%\n----------------------------------------\n\n[epoch 85] accuracy on test data: 78.33%\n\n----------------------------------------\n\nepoch 86 learning rate 0.012500\n\n[epoch 86][0/390] loss 0.0217 accuracy 100.00%\n[epoch 86][30/390] loss 0.0096 accuracy 100.00%\n[epoch 86][60/390] loss 0.0116 accuracy 100.00%\n[epoch 86][90/390] loss 0.0102 accuracy 100.00%\n[epoch 86][120/390] loss 0.0137 accuracy 100.00%\n[epoch 86][150/390] loss 0.0145 accuracy 100.00%\n[epoch 86][180/390] loss 0.0129 accuracy 100.00%\n[epoch 86][210/390] loss 0.0127 accuracy 100.00%\n[epoch 86][240/390] loss 0.0163 accuracy 100.00%\n[epoch 86][270/390] loss 0.0156 accuracy 100.00%\n[epoch 86][300/390] loss 0.0109 accuracy 100.00%\n[epoch 86][330/390] loss 0.0171 accuracy 100.00%\n[epoch 86][360/390] loss 0.0170 accuracy 100.00%\n[epoch 86][390/390] loss 0.0350 accuracy 100.00%\n----------------------------------------\n\n[epoch 86] accuracy on test data: 78.06%\n\n----------------------------------------\n\nepoch 87 learning rate 0.012500\n\n[epoch 87][0/390] loss 0.0174 accuracy 100.00%\n[epoch 87][30/390] loss 0.0173 accuracy 100.00%\n[epoch 87][60/390] loss 0.0109 accuracy 100.00%\n[epoch 87][90/390] loss 0.0139 accuracy 100.00%\n[epoch 87][120/390] loss 0.0149 accuracy 100.00%\n[epoch 87][150/390] loss 0.0210 accuracy 100.00%\n[epoch 87][180/390] loss 0.0254 accuracy 100.00%\n[epoch 87][210/390] loss 0.0241 accuracy 100.00%\n[epoch 87][240/390] loss 0.0125 accuracy 100.00%\n[epoch 87][270/390] loss 0.0203 accuracy 100.00%\n[epoch 87][300/390] loss 0.0178 accuracy 100.00%\n[epoch 87][330/390] loss 0.0271 accuracy 100.00%\n[epoch 87][360/390] loss 0.0177 accuracy 100.00%\n[epoch 87][390/390] loss 0.0335 accuracy 100.00%\n----------------------------------------\n\n[epoch 87] accuracy on test data: 77.79%\n\n----------------------------------------\n\nepoch 88 learning rate 0.012500\n\n[epoch 88][0/390] loss 0.0144 accuracy 100.00%\n[epoch 88][30/390] loss 0.0192 accuracy 100.00%\n[epoch 88][60/390] loss 0.0121 accuracy 100.00%\n[epoch 88][90/390] loss 0.0142 accuracy 100.00%\n[epoch 88][120/390] loss 0.0181 accuracy 100.00%\n[epoch 88][150/390] loss 0.0136 accuracy 100.00%\n[epoch 88][180/390] loss 0.0157 accuracy 100.00%\n[epoch 88][210/390] loss 0.0223 accuracy 100.00%\n[epoch 88][240/390] loss 0.0112 accuracy 100.00%\n[epoch 88][270/390] loss 0.0124 accuracy 100.00%\n[epoch 88][300/390] loss 0.0137 accuracy 100.00%\n[epoch 88][330/390] loss 0.0171 accuracy 100.00%\n[epoch 88][360/390] loss 0.0179 accuracy 100.00%\n[epoch 88][390/390] loss 0.0150 accuracy 100.00%\n----------------------------------------\n\n[epoch 88] accuracy on test data: 78.16%\n\n----------------------------------------\n\nepoch 89 learning rate 0.012500\n\n[epoch 89][0/390] loss 0.0136 accuracy 100.00%\n[epoch 89][30/390] loss 0.0148 accuracy 100.00%\n[epoch 89][60/390] loss 0.0179 accuracy 100.00%\n[epoch 89][90/390] loss 0.0164 accuracy 100.00%\n[epoch 89][120/390] loss 0.0224 accuracy 100.00%\n[epoch 89][150/390] loss 0.0114 accuracy 100.00%\n[epoch 89][180/390] loss 0.0145 accuracy 100.00%\n[epoch 89][210/390] loss 0.0346 accuracy 100.00%\n[epoch 89][240/390] loss 0.0141 accuracy 100.00%\n[epoch 89][270/390] loss 0.0326 accuracy 100.00%\n[epoch 89][300/390] loss 0.0140 accuracy 100.00%\n[epoch 89][330/390] loss 0.0237 accuracy 100.00%\n[epoch 89][360/390] loss 0.0126 accuracy 100.00%\n[epoch 89][390/390] loss 0.0198 accuracy 100.00%\n----------------------------------------\n\n[epoch 89] accuracy on test data: 77.69%\n\n----------------------------------------\n\nepoch 90 learning rate 0.012500\n\n[epoch 90][0/390] loss 0.0206 accuracy 100.00%\n[epoch 90][30/390] loss 0.0087 accuracy 100.00%\n[epoch 90][60/390] loss 0.0369 accuracy 99.22%\n[epoch 90][90/390] loss 0.0154 accuracy 100.00%\n[epoch 90][120/390] loss 0.0154 accuracy 100.00%\n[epoch 90][150/390] loss 0.0178 accuracy 100.00%\n[epoch 90][180/390] loss 0.0211 accuracy 99.22%\n[epoch 90][210/390] loss 0.0177 accuracy 100.00%\n[epoch 90][240/390] loss 0.0249 accuracy 100.00%\n[epoch 90][270/390] loss 0.0149 accuracy 100.00%\n[epoch 90][300/390] loss 0.0148 accuracy 100.00%\n[epoch 90][330/390] loss 0.0235 accuracy 100.00%\n[epoch 90][360/390] loss 0.0145 accuracy 100.00%\n[epoch 90][390/390] loss 0.0271 accuracy 100.00%\n----------------------------------------\n\n[epoch 90] accuracy on test data: 77.64%\n\n----------------------------------------\n\nepoch 91 learning rate 0.012500\n\n[epoch 91][0/390] loss 0.0125 accuracy 100.00%\n[epoch 91][30/390] loss 0.0183 accuracy 100.00%\n[epoch 91][60/390] loss 0.0083 accuracy 100.00%\n[epoch 91][90/390] loss 0.0178 accuracy 100.00%\n[epoch 91][120/390] loss 0.0164 accuracy 100.00%\n[epoch 91][150/390] loss 0.0120 accuracy 100.00%\n[epoch 91][180/390] loss 0.0204 accuracy 100.00%\n[epoch 91][210/390] loss 0.0155 accuracy 100.00%\n[epoch 91][240/390] loss 0.0174 accuracy 100.00%\n[epoch 91][270/390] loss 0.0207 accuracy 100.00%\n[epoch 91][300/390] loss 0.0186 accuracy 100.00%\n[epoch 91][330/390] loss 0.0249 accuracy 100.00%\n[epoch 91][360/390] loss 0.0224 accuracy 100.00%\n[epoch 91][390/390] loss 0.0340 accuracy 100.00%\n----------------------------------------\n\n[epoch 91] accuracy on test data: 77.34%\n\n----------------------------------------\n\nepoch 92 learning rate 0.012500\n\n[epoch 92][0/390] loss 0.0133 accuracy 100.00%\n[epoch 92][30/390] loss 0.0103 accuracy 100.00%\n[epoch 92][60/390] loss 0.0132 accuracy 100.00%\n[epoch 92][90/390] loss 0.0166 accuracy 100.00%\n[epoch 92][120/390] loss 0.0222 accuracy 100.00%\n[epoch 92][150/390] loss 0.0577 accuracy 99.22%\n[epoch 92][180/390] loss 0.0291 accuracy 100.00%\n[epoch 92][210/390] loss 0.0183 accuracy 100.00%\n[epoch 92][240/390] loss 0.0250 accuracy 100.00%\n[epoch 92][270/390] loss 0.0278 accuracy 100.00%\n[epoch 92][300/390] loss 0.0234 accuracy 100.00%\n[epoch 92][330/390] loss 0.0139 accuracy 100.00%\n[epoch 92][360/390] loss 0.0212 accuracy 100.00%\n[epoch 92][390/390] loss 0.0467 accuracy 100.00%\n----------------------------------------\n\n[epoch 92] accuracy on test data: 77.82%\n\n----------------------------------------\n\nepoch 93 learning rate 0.012500\n\n[epoch 93][0/390] loss 0.0884 accuracy 99.22%\n[epoch 93][30/390] loss 0.0162 accuracy 100.00%\n[epoch 93][60/390] loss 0.0199 accuracy 100.00%\n[epoch 93][90/390] loss 0.0204 accuracy 99.22%\n[epoch 93][120/390] loss 0.0166 accuracy 100.00%\n[epoch 93][150/390] loss 0.0293 accuracy 100.00%\n[epoch 93][180/390] loss 0.0157 accuracy 100.00%\n[epoch 93][210/390] loss 0.0186 accuracy 100.00%\n[epoch 93][240/390] loss 0.0351 accuracy 99.22%\n[epoch 93][270/390] loss 0.0200 accuracy 100.00%\n[epoch 93][300/390] loss 0.0162 accuracy 100.00%\n[epoch 93][330/390] loss 0.0181 accuracy 100.00%\n[epoch 93][360/390] loss 0.0224 accuracy 100.00%\n[epoch 93][390/390] loss 0.0287 accuracy 100.00%\n----------------------------------------\n\n[epoch 93] accuracy on test data: 74.97%\n\n----------------------------------------\n\nepoch 94 learning rate 0.012500\n\n[epoch 94][0/390] loss 0.0424 accuracy 100.00%\n[epoch 94][30/390] loss 0.0186 accuracy 100.00%\n[epoch 94][60/390] loss 0.0499 accuracy 100.00%\n[epoch 94][90/390] loss 0.0170 accuracy 100.00%\n[epoch 94][120/390] loss 0.0142 accuracy 100.00%\n[epoch 94][150/390] loss 0.0164 accuracy 100.00%\n[epoch 94][180/390] loss 0.0147 accuracy 100.00%\n[epoch 94][210/390] loss 0.0143 accuracy 100.00%\n[epoch 94][240/390] loss 0.0416 accuracy 100.00%\n[epoch 94][270/390] loss 0.0420 accuracy 100.00%\n[epoch 94][300/390] loss 0.0310 accuracy 100.00%\n[epoch 94][330/390] loss 0.0258 accuracy 100.00%\n[epoch 94][360/390] loss 0.0320 accuracy 100.00%\n[epoch 94][390/390] loss 0.0298 accuracy 100.00%\n----------------------------------------\n\n[epoch 94] accuracy on test data: 76.25%\n\n----------------------------------------\n\nepoch 95 learning rate 0.012500\n\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3247ff9b4e58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nepoch %d learning rate %f\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# run for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36m__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_spawning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         return (self._ignore_epipe, self._maxsize, self._reader, self._writer,\n\u001b[0;32m     60\u001b[0m                 self._rlock, self._wlock, self._sem, self._opid)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "log_freq = 30\n",
    "epochs = 100\n",
    "save_freq = 10\n",
    "writer = SummaryWriter('./runs/attempt_7')\n",
    "for epoch in range(epochs):\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    # run for one epoch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # warm up\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        pred, _, _, _ = model(inputs)\n",
    "\n",
    "        # backward\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display results\n",
    "        if i % log_freq == 0:\n",
    "            model.eval()\n",
    "            pred, __, __, __ = model(inputs)\n",
    "            predict = torch.argmax(pred, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = torch.eq(predict, labels).sum().double().item()\n",
    "            accuracy = correct / total\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "            writer.add_scalar('train/accuracy', accuracy, step)\n",
    "\n",
    "            print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
    "                % (epoch, i, len(train_loader)-1, loss.item(), (100*accuracy)))\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \n",
    "            \"./checkpoints/attn-net_epoch_%s.pth\" % epoch\n",
    "        )\n",
    "\n",
    "    print('-'*40)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # log scalars\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images_test, labels_test = data\n",
    "            images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "            pred_test, _, _, _ = model(images_test)\n",
    "            predict = torch.argmax(pred_test, 1)\n",
    "            total += labels_test.size(0)\n",
    "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "        writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
    "    \n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 0\n",
    "# log_freq = 10\n",
    "# writer = SummaryWriter('./runs/attempt_1')\n",
    "# for epoch in range(300):\n",
    "#     # images_disp = []\n",
    "#     # adjust learning rate\n",
    "#     scheduler.step()\n",
    "#     writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "#     print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "#     # run for one epoch\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "#         # warm up\n",
    "#         model.train()\n",
    "#         model.zero_grad()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # if (aug == 0) and (i == 0): # archive images in order to save to logs\n",
    "#         #     images_disp.append(inputs[0:36,:,:,:])\n",
    "\n",
    "#         # forward\n",
    "#         pred, _, _, _ = model(inputs)\n",
    "\n",
    "#         # backward\n",
    "#         loss = criterion(pred, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # display results\n",
    "#         if i % log_freq == 0:\n",
    "#             model.eval()\n",
    "#             pred, __, __, __ = model(inputs)\n",
    "#             predict = torch.argmax(pred, 1)\n",
    "#             total = labels.size(0)\n",
    "#             correct = torch.eq(predict, labels).sum().double().item()\n",
    "#             accuracy = correct / total\n",
    "#             writer.add_scalar('train/loss', loss.item(), step)\n",
    "#             writer.add_scalar('train/accuracy', accuracy, step)\n",
    "\n",
    "#             print(\"[epoch %d][aug %d/%d][%d/%d] loss %.4f accuracy %.2f%% running avg accuracy\"\n",
    "#                 % (epoch, aug, num_aug-1, i, len(trainloader)-1, loss.item(), (100*accuracy)))\n",
    "        \n",
    "#         step += 1\n",
    "\n",
    "#     # the end of each epoch: test & log\n",
    "#     print('-'*20)\n",
    "#     # torch.save(model.state_dict(), os.path.join(opt.outf, 'net.pth'))\n",
    "#     # if epoch == opt.epochs / 2:\n",
    "#     #     torch.save(model.state_dict(), os.path.join(opt.outf, 'net%d.pth' % epoch))\n",
    "    \n",
    "#     model.eval()\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         # log scalars\n",
    "#         for i, data in enumerate(test_loader):\n",
    "#             images_test, labels_test = data\n",
    "#             images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "#             # if i == 0: # archive images in order to save to logs\n",
    "#             #     images_disp.append(inputs[0:36,:,:,:])\n",
    "#             pred_test, _, _, _ = model(images_test)\n",
    "#             predict = torch.argmax(pred_test, 1)\n",
    "#             total += labels_test.size(0)\n",
    "#             correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "#         writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "#         print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
    "\n",
    "#         # log images\n",
    "#         # if opt.log_images:\n",
    "#         #     print('\\nlog images ...\\n')\n",
    "#         #     I_train = utils.make_grid(images_disp[0], nrow=6, normalize=True, scale_each=True)\n",
    "#         #     writer.add_image('train/image', I_train, epoch)\n",
    "#         #     if epoch == 0:\n",
    "#         #         I_test = utils.make_grid(images_disp[1], nrow=6, normalize=True, scale_each=True)\n",
    "#         #         writer.add_image('test/image', I_test, epoch)\n",
    "#         # if opt.log_images and (not opt.no_attention):\n",
    "#         #     print('\\nlog attention maps ...\\n')\n",
    "#         #     # base factor\n",
    "#         #     if opt.attn_mode == 'before':\n",
    "#         #         min_up_factor = 1\n",
    "#         #     else:\n",
    "#         #         min_up_factor = 2\n",
    "#         #     # sigmoid or softmax\n",
    "#         #     if opt.normalize_attn:\n",
    "#         #         vis_fun = visualize_attn_softmax\n",
    "#         #     else:\n",
    "#         #         vis_fun = visualize_attn_sigmoid\n",
    "#         #     # training data\n",
    "#         #     __, c1, c2, c3 = model(images_disp[0])\n",
    "#         #     if c1 is not None:\n",
    "#         #         attn1 = vis_fun(I_train, c1, up_factor=min_up_factor, nrow=6)\n",
    "#         #         writer.add_image('train/attention_map_1', attn1, epoch)\n",
    "#         #     if c2 is not None:\n",
    "#         #         attn2 = vis_fun(I_train, c2, up_factor=min_up_factor*2, nrow=6)\n",
    "#         #         writer.add_image('train/attention_map_2', attn2, epoch)\n",
    "#         #     if c3 is not None:\n",
    "#         #         attn3 = vis_fun(I_train, c3, up_factor=min_up_factor*4, nrow=6)\n",
    "#         #         writer.add_image('train/attention_map_3', attn3, epoch)\n",
    "#         #     # test data\n",
    "#         #     __, c1, c2, c3 = model(images_disp[1])\n",
    "#         #     if c1 is not None:\n",
    "#         #         attn1 = vis_fun(I_test, c1, up_factor=min_up_factor, nrow=6)\n",
    "#         #         writer.add_image('test/attention_map_1', attn1, epoch)\n",
    "#         #     if c2 is not None:\n",
    "#         #         attn2 = vis_fun(I_test, c2, up_factor=min_up_factor*2, nrow=6)\n",
    "#         #         writer.add_image('test/attention_map_2', attn2, epoch)\n",
    "#         #     if c3 is not None:\n",
    "#         #         attn3 = vis_fun(I_test, c3, up_factor=min_up_factor*4, nrow=6)\n",
    "#         #         writer.add_image('test/attention_map_3', attn3, epoch)"
   ]
  }
 ]
}