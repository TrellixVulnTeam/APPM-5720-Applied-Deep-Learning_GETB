{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "deep-learning",
   "display_name": "deep-learning"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import ImageStat\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from attn_vgg import AttnVGG\n",
    "from cub2011 import Cub2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Cub2011(root='../../../data', train=True, download=False, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mean_tot = 0\n",
    "g_mean_tot = 0\n",
    "b_mean_tot = 0\n",
    "r_meansq_tot = 0\n",
    "g_meansq_tot = 0\n",
    "b_meansq_tot = 0\n",
    "\n",
    "for data in train_set:\n",
    "    data = transforms.ToTensor()(data[0])\n",
    "    r_mean_tot += data[0,:,:].mean()\n",
    "    g_mean_tot += data[1,:,:].mean()\n",
    "    b_mean_tot += data[2,:,:].mean()\n",
    "    r_meansq_tot += (data[0,:,:]**2).mean()\n",
    "    g_meansq_tot += (data[1,:,:]**2).mean()\n",
    "    b_meansq_tot += (data[2,:,:]**2).mean()\n",
    "\n",
    "\n",
    "r_mean = r_mean_tot / len(train_set) \n",
    "g_mean = g_mean_tot / len(train_set) \n",
    "b_mean = b_mean_tot / len(train_set) \n",
    "r_std = torch.sqrt(r_meansq_tot - r_mean_tot**2) / len(train_set)\n",
    "g_std = torch.sqrt(g_meansq_tot - g_mean_tot**2) / len(train_set)\n",
    "b_std = torch.sqrt(b_meansq_tot - b_mean_tot**2) / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.4856)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((r_mean, g_mean, b_mean), (1, 1, 1))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((r_mean, g_mean, b_mean), (1, 1, 1))\n",
    "])\n",
    "\n",
    "train_set = Cub2011(root='../../../data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_set = Cub2011(root='../../../data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<PIL.Image.Image image mode=RGB size=112x112 at 0x26CE8F13EB8>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(next(iter(train_set))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnVGG(im_size=128, num_classes=200, normalize_attn=False)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "3472 accuracy 0.00%\n[epoch 0][600/749] loss 5.3443 accuracy 0.00%\n[epoch 0][630/749] loss 5.2641 accuracy 0.00%\n[epoch 0][660/749] loss 5.1671 accuracy 0.00%\n[epoch 0][690/749] loss 5.3130 accuracy 0.00%\n[epoch 0][720/749] loss 5.2833 accuracy 0.00%\n----------------------------------------\n\n[epoch 0] accuracy on test data: 0.48%\n\n----------------------------------------\n\nepoch 1 learning rate 0.100000\n\n[epoch 1][0/749] loss 5.1902 accuracy 0.00%\n[epoch 1][30/749] loss 5.4189 accuracy 0.00%\n[epoch 1][60/749] loss 5.2042 accuracy 0.00%\n[epoch 1][90/749] loss 5.3310 accuracy 0.00%\n[epoch 1][120/749] loss 5.2349 accuracy 0.00%\n[epoch 1][150/749] loss 5.2713 accuracy 0.00%\n[epoch 1][180/749] loss 5.3093 accuracy 0.00%\n[epoch 1][210/749] loss 5.3507 accuracy 0.00%\n[epoch 1][240/749] loss 5.3108 accuracy 0.00%\n[epoch 1][270/749] loss 5.4759 accuracy 0.00%\n[epoch 1][300/749] loss 5.2381 accuracy 0.00%\n[epoch 1][330/749] loss 5.4202 accuracy 0.00%\n[epoch 1][360/749] loss 5.3849 accuracy 0.00%\n[epoch 1][390/749] loss 5.3181 accuracy 0.00%\n[epoch 1][420/749] loss 5.3079 accuracy 0.00%\n[epoch 1][450/749] loss 5.4761 accuracy 0.00%\n[epoch 1][480/749] loss 5.3098 accuracy 12.50%\n[epoch 1][510/749] loss 5.3398 accuracy 0.00%\n[epoch 1][540/749] loss 5.3126 accuracy 0.00%\n[epoch 1][570/749] loss 5.3973 accuracy 0.00%\n[epoch 1][600/749] loss 5.4275 accuracy 0.00%\n[epoch 1][630/749] loss 5.3247 accuracy 0.00%\n[epoch 1][660/749] loss 5.2548 accuracy 0.00%\n[epoch 1][690/749] loss 5.3431 accuracy 12.50%\n[epoch 1][720/749] loss 5.3152 accuracy 0.00%\n----------------------------------------\n\n[epoch 1] accuracy on test data: 0.52%\n\n----------------------------------------\n\nepoch 2 learning rate 0.100000\n\n[epoch 2][0/749] loss 5.3589 accuracy 0.00%\n[epoch 2][30/749] loss 5.3151 accuracy 0.00%\n[epoch 2][60/749] loss 5.2976 accuracy 0.00%\n[epoch 2][90/749] loss 5.3382 accuracy 0.00%\n[epoch 2][120/749] loss 5.2454 accuracy 0.00%\n[epoch 2][150/749] loss 5.3715 accuracy 0.00%\n[epoch 2][180/749] loss 5.2912 accuracy 0.00%\n[epoch 2][210/749] loss 5.3129 accuracy 0.00%\n[epoch 2][240/749] loss 5.2844 accuracy 0.00%\n[epoch 2][270/749] loss 5.3088 accuracy 0.00%\n[epoch 2][300/749] loss 5.2843 accuracy 0.00%\n[epoch 2][330/749] loss 5.4688 accuracy 0.00%\n[epoch 2][360/749] loss 5.1690 accuracy 0.00%\n[epoch 2][390/749] loss 5.4277 accuracy 0.00%\n[epoch 2][420/749] loss 5.3402 accuracy 0.00%\n[epoch 2][450/749] loss 5.3571 accuracy 0.00%\n[epoch 2][480/749] loss 5.4736 accuracy 0.00%\n[epoch 2][510/749] loss 5.3581 accuracy 0.00%\n[epoch 2][540/749] loss 5.0336 accuracy 0.00%\n[epoch 2][570/749] loss 5.3261 accuracy 0.00%\n[epoch 2][600/749] loss 5.4114 accuracy 0.00%\n[epoch 2][630/749] loss 5.0615 accuracy 0.00%\n[epoch 2][660/749] loss 5.3715 accuracy 0.00%\n[epoch 2][690/749] loss 5.0364 accuracy 12.50%\n[epoch 2][720/749] loss 4.9650 accuracy 0.00%\n----------------------------------------\n\n[epoch 2] accuracy on test data: 0.62%\n\n----------------------------------------\n\nepoch 3 learning rate 0.100000\n\n[epoch 3][0/749] loss 5.2689 accuracy 0.00%\n[epoch 3][30/749] loss 5.4443 accuracy 0.00%\n[epoch 3][60/749] loss 5.2066 accuracy 0.00%\n[epoch 3][90/749] loss 5.3252 accuracy 0.00%\n[epoch 3][120/749] loss 5.1042 accuracy 0.00%\n[epoch 3][150/749] loss 5.2715 accuracy 0.00%\n[epoch 3][180/749] loss 5.1239 accuracy 0.00%\n[epoch 3][210/749] loss 5.3812 accuracy 0.00%\n[epoch 3][240/749] loss 5.4177 accuracy 0.00%\n[epoch 3][270/749] loss 5.3436 accuracy 0.00%\n[epoch 3][300/749] loss 5.2815 accuracy 0.00%\n[epoch 3][330/749] loss 5.3452 accuracy 0.00%\n[epoch 3][360/749] loss 5.4473 accuracy 0.00%\n[epoch 3][390/749] loss 5.4770 accuracy 0.00%\n[epoch 3][420/749] loss 5.2558 accuracy 0.00%\n[epoch 3][450/749] loss 5.4159 accuracy 0.00%\n[epoch 3][480/749] loss 5.0970 accuracy 12.50%\n[epoch 3][510/749] loss 5.3236 accuracy 0.00%\n[epoch 3][540/749] loss 5.2956 accuracy 0.00%\n[epoch 3][570/749] loss 5.3274 accuracy 0.00%\n[epoch 3][600/749] loss 5.3757 accuracy 0.00%\n[epoch 3][630/749] loss 4.9610 accuracy 0.00%\n[epoch 3][660/749] loss 5.4501 accuracy 0.00%\n[epoch 3][690/749] loss 5.3365 accuracy 0.00%\n[epoch 3][720/749] loss 5.2784 accuracy 0.00%\n----------------------------------------\n\n[epoch 3] accuracy on test data: 0.48%\n\n----------------------------------------\n\nepoch 4 learning rate 0.100000\n\n[epoch 4][0/749] loss 5.4431 accuracy 0.00%\n[epoch 4][30/749] loss 5.3605 accuracy 0.00%\n[epoch 4][60/749] loss 5.0228 accuracy 0.00%\n[epoch 4][90/749] loss 5.0971 accuracy 0.00%\n[epoch 4][120/749] loss 5.1874 accuracy 0.00%\n[epoch 4][150/749] loss 5.3971 accuracy 0.00%\n[epoch 4][180/749] loss 5.2602 accuracy 0.00%\n[epoch 4][210/749] loss 5.3512 accuracy 0.00%\n[epoch 4][240/749] loss 5.3324 accuracy 0.00%\n[epoch 4][270/749] loss 4.8404 accuracy 12.50%\n[epoch 4][300/749] loss 5.2739 accuracy 0.00%\n[epoch 4][330/749] loss 4.7292 accuracy 0.00%\n[epoch 4][360/749] loss 5.1830 accuracy 0.00%\n[epoch 4][390/749] loss 5.3052 accuracy 0.00%\n[epoch 4][420/749] loss 5.0595 accuracy 0.00%\n[epoch 4][450/749] loss 5.4497 accuracy 0.00%\n[epoch 4][480/749] loss 5.2346 accuracy 12.50%\n[epoch 4][510/749] loss 5.2782 accuracy 0.00%\n[epoch 4][540/749] loss 5.1347 accuracy 0.00%\n[epoch 4][570/749] loss 5.3391 accuracy 0.00%\n[epoch 4][600/749] loss 5.3172 accuracy 0.00%\n[epoch 4][630/749] loss 5.3248 accuracy 0.00%\n[epoch 4][660/749] loss 4.8905 accuracy 0.00%\n[epoch 4][690/749] loss 4.9644 accuracy 0.00%\n[epoch 4][720/749] loss 5.5975 accuracy 12.50%\n----------------------------------------\n\n[epoch 4] accuracy on test data: 1.62%\n\n----------------------------------------\n\nepoch 5 learning rate 0.100000\n\n[epoch 5][0/749] loss 5.2497 accuracy 0.00%\n[epoch 5][30/749] loss 5.4189 accuracy 0.00%\n[epoch 5][60/749] loss 5.0290 accuracy 0.00%\n[epoch 5][90/749] loss 5.0384 accuracy 0.00%\n[epoch 5][120/749] loss 5.4944 accuracy 0.00%\n[epoch 5][150/749] loss 5.1173 accuracy 0.00%\n[epoch 5][180/749] loss 5.0848 accuracy 0.00%\n[epoch 5][210/749] loss 5.0406 accuracy 0.00%\n[epoch 5][240/749] loss 5.0819 accuracy 12.50%\n[epoch 5][270/749] loss 5.4864 accuracy 0.00%\n[epoch 5][300/749] loss 5.3729 accuracy 0.00%\n[epoch 5][330/749] loss 5.2819 accuracy 0.00%\n[epoch 5][360/749] loss 5.0323 accuracy 0.00%\n[epoch 5][390/749] loss 5.3441 accuracy 12.50%\n[epoch 5][420/749] loss 5.6359 accuracy 0.00%\n[epoch 5][450/749] loss 5.2087 accuracy 0.00%\n[epoch 5][480/749] loss 5.2949 accuracy 0.00%\n[epoch 5][510/749] loss 5.4122 accuracy 0.00%\n[epoch 5][540/749] loss 5.4227 accuracy 0.00%\n[epoch 5][570/749] loss 5.3632 accuracy 0.00%\n[epoch 5][600/749] loss 5.5399 accuracy 0.00%\n[epoch 5][630/749] loss 5.2686 accuracy 0.00%\n[epoch 5][660/749] loss 5.2753 accuracy 0.00%\n[epoch 5][690/749] loss 5.2768 accuracy 0.00%\n[epoch 5][720/749] loss 5.3624 accuracy 0.00%\n----------------------------------------\n\n[epoch 5] accuracy on test data: 0.62%\n\n----------------------------------------\n\nepoch 6 learning rate 0.100000\n\n[epoch 6][0/749] loss 5.3984 accuracy 0.00%\n[epoch 6][30/749] loss 5.2362 accuracy 0.00%\n[epoch 6][60/749] loss 5.2495 accuracy 0.00%\n[epoch 6][90/749] loss 5.2614 accuracy 12.50%\n[epoch 6][120/749] loss 5.1974 accuracy 0.00%\n[epoch 6][150/749] loss 5.4187 accuracy 0.00%\n[epoch 6][180/749] loss 5.3658 accuracy 0.00%\n[epoch 6][210/749] loss 5.1781 accuracy 0.00%\n[epoch 6][240/749] loss 5.4163 accuracy 0.00%\n[epoch 6][270/749] loss 5.1959 accuracy 0.00%\n[epoch 6][300/749] loss 5.3934 accuracy 0.00%\n[epoch 6][330/749] loss 5.0677 accuracy 12.50%\n[epoch 6][360/749] loss 5.2466 accuracy 0.00%\n[epoch 6][390/749] loss 5.3974 accuracy 0.00%\n[epoch 6][420/749] loss 5.2179 accuracy 0.00%\n[epoch 6][450/749] loss 5.0851 accuracy 0.00%\n[epoch 6][480/749] loss 5.1116 accuracy 0.00%\n[epoch 6][510/749] loss 5.1147 accuracy 0.00%\n[epoch 6][540/749] loss 5.5836 accuracy 0.00%\n[epoch 6][570/749] loss 5.2415 accuracy 0.00%\n[epoch 6][600/749] loss 5.1038 accuracy 0.00%\n[epoch 6][630/749] loss 5.3478 accuracy 0.00%\n[epoch 6][660/749] loss 5.4579 accuracy 0.00%\n[epoch 6][690/749] loss 5.1232 accuracy 0.00%\n[epoch 6][720/749] loss 5.2633 accuracy 0.00%\n----------------------------------------\n\n[epoch 6] accuracy on test data: 1.48%\n\n----------------------------------------\n\nepoch 7 learning rate 0.100000\n\n[epoch 7][0/749] loss 5.1824 accuracy 12.50%\n[epoch 7][30/749] loss 4.8504 accuracy 0.00%\n[epoch 7][60/749] loss 4.9094 accuracy 0.00%\n[epoch 7][90/749] loss 5.2112 accuracy 0.00%\n[epoch 7][120/749] loss 5.3580 accuracy 0.00%\n[epoch 7][150/749] loss 5.3562 accuracy 0.00%\n[epoch 7][180/749] loss 5.2470 accuracy 0.00%\n[epoch 7][210/749] loss 5.1136 accuracy 0.00%\n[epoch 7][240/749] loss 5.0209 accuracy 12.50%\n[epoch 7][270/749] loss 5.4491 accuracy 0.00%\n[epoch 7][300/749] loss 5.1195 accuracy 0.00%\n[epoch 7][330/749] loss 5.3112 accuracy 0.00%\n[epoch 7][360/749] loss 5.2471 accuracy 0.00%\n[epoch 7][390/749] loss 4.8075 accuracy 0.00%\n[epoch 7][420/749] loss 4.9851 accuracy 12.50%\n[epoch 7][450/749] loss 4.9967 accuracy 0.00%\n[epoch 7][480/749] loss 5.2989 accuracy 0.00%\n[epoch 7][510/749] loss 5.5616 accuracy 0.00%\n[epoch 7][540/749] loss 5.3062 accuracy 0.00%\n[epoch 7][570/749] loss 5.0922 accuracy 0.00%\n[epoch 7][600/749] loss 5.1636 accuracy 0.00%\n[epoch 7][630/749] loss 5.2237 accuracy 12.50%\n[epoch 7][660/749] loss 5.1479 accuracy 0.00%\n[epoch 7][690/749] loss 4.8175 accuracy 12.50%\n[epoch 7][720/749] loss 5.5254 accuracy 0.00%\n----------------------------------------\n\n[epoch 7] accuracy on test data: 0.59%\n\n----------------------------------------\n\nepoch 8 learning rate 0.100000\n\n[epoch 8][0/749] loss 5.1356 accuracy 0.00%\n[epoch 8][30/749] loss 5.1891 accuracy 0.00%\n[epoch 8][60/749] loss 4.7910 accuracy 0.00%\n[epoch 8][90/749] loss 5.1303 accuracy 0.00%\n[epoch 8][120/749] loss 5.0117 accuracy 0.00%\n[epoch 8][150/749] loss 4.6870 accuracy 12.50%\n[epoch 8][180/749] loss 5.1808 accuracy 0.00%\n[epoch 8][210/749] loss 5.0806 accuracy 0.00%\n[epoch 8][240/749] loss 4.9811 accuracy 0.00%\n[epoch 8][270/749] loss 5.0029 accuracy 12.50%\n[epoch 8][300/749] loss 5.2160 accuracy 0.00%\n[epoch 8][330/749] loss 5.3043 accuracy 0.00%\n[epoch 8][360/749] loss 5.0974 accuracy 0.00%\n[epoch 8][390/749] loss 5.3561 accuracy 0.00%\n[epoch 8][420/749] loss 5.0319 accuracy 0.00%\n[epoch 8][450/749] loss 5.1355 accuracy 0.00%\n[epoch 8][480/749] loss 4.9725 accuracy 0.00%\n[epoch 8][510/749] loss 5.4186 accuracy 0.00%\n[epoch 8][540/749] loss 4.7058 accuracy 0.00%\n[epoch 8][570/749] loss 5.0628 accuracy 0.00%\n[epoch 8][600/749] loss 4.7317 accuracy 0.00%\n[epoch 8][630/749] loss 5.1105 accuracy 0.00%\n[epoch 8][660/749] loss 5.0426 accuracy 0.00%\n[epoch 8][690/749] loss 5.3531 accuracy 0.00%\n[epoch 8][720/749] loss 5.0652 accuracy 0.00%\n----------------------------------------\n\n[epoch 8] accuracy on test data: 0.90%\n\n----------------------------------------\n\nepoch 9 learning rate 0.100000\n\n[epoch 9][0/749] loss 5.2024 accuracy 0.00%\n[epoch 9][30/749] loss 5.2489 accuracy 0.00%\n[epoch 9][60/749] loss 5.5249 accuracy 0.00%\n[epoch 9][90/749] loss 5.3160 accuracy 0.00%\n[epoch 9][120/749] loss 5.4436 accuracy 0.00%\n[epoch 9][150/749] loss 5.2708 accuracy 0.00%\n[epoch 9][180/749] loss 5.3296 accuracy 12.50%\n[epoch 9][210/749] loss 5.2186 accuracy 0.00%\n[epoch 9][240/749] loss 5.2441 accuracy 0.00%\n[epoch 9][270/749] loss 4.9783 accuracy 0.00%\n[epoch 9][300/749] loss 5.4536 accuracy 0.00%\n[epoch 9][330/749] loss 5.0832 accuracy 0.00%\n[epoch 9][360/749] loss 5.2922 accuracy 0.00%\n[epoch 9][390/749] loss 5.3064 accuracy 12.50%\n[epoch 9][420/749] loss 5.0529 accuracy 0.00%\n[epoch 9][450/749] loss 5.2308 accuracy 0.00%\n[epoch 9][480/749] loss 5.4183 accuracy 0.00%\n[epoch 9][510/749] loss 5.3776 accuracy 0.00%\n[epoch 9][540/749] loss 5.1049 accuracy 0.00%\n[epoch 9][570/749] loss 5.2344 accuracy 0.00%\n[epoch 9][600/749] loss 5.0184 accuracy 0.00%\n[epoch 9][630/749] loss 5.2537 accuracy 0.00%\n[epoch 9][660/749] loss 5.1975 accuracy 0.00%\n[epoch 9][690/749] loss 5.2242 accuracy 0.00%\n[epoch 9][720/749] loss 5.4511 accuracy 0.00%\n----------------------------------------\n\n[epoch 9] accuracy on test data: 0.79%\n\n----------------------------------------\n\nepoch 10 learning rate 0.100000\n\n[epoch 10][0/749] loss 4.9518 accuracy 0.00%\n[epoch 10][30/749] loss 5.4007 accuracy 0.00%\n[epoch 10][60/749] loss 4.9317 accuracy 0.00%\n[epoch 10][90/749] loss 5.3936 accuracy 0.00%\n[epoch 10][120/749] loss 5.2660 accuracy 0.00%\n[epoch 10][150/749] loss 5.2463 accuracy 0.00%\n[epoch 10][180/749] loss 5.2456 accuracy 0.00%\n[epoch 10][210/749] loss 5.1008 accuracy 0.00%\n[epoch 10][240/749] loss 5.2641 accuracy 0.00%\n[epoch 10][270/749] loss 5.1458 accuracy 0.00%\n[epoch 10][300/749] loss 5.0615 accuracy 0.00%\n[epoch 10][330/749] loss 5.2318 accuracy 0.00%\n[epoch 10][360/749] loss 5.4355 accuracy 0.00%\n[epoch 10][390/749] loss 5.1293 accuracy 0.00%\n[epoch 10][420/749] loss 5.1656 accuracy 0.00%\n[epoch 10][450/749] loss 5.2728 accuracy 0.00%\n[epoch 10][480/749] loss 5.2309 accuracy 0.00%\n[epoch 10][510/749] loss 4.9367 accuracy 0.00%\n[epoch 10][540/749] loss 5.0314 accuracy 0.00%\n[epoch 10][570/749] loss 5.2174 accuracy 0.00%\n[epoch 10][600/749] loss 5.2164 accuracy 0.00%\n[epoch 10][630/749] loss 5.2711 accuracy 0.00%\n[epoch 10][660/749] loss 5.0497 accuracy 0.00%\n[epoch 10][690/749] loss 5.0782 accuracy 0.00%\n[epoch 10][720/749] loss 5.1717 accuracy 0.00%\n----------------------------------------\n\n[epoch 10] accuracy on test data: 0.86%\n\n----------------------------------------\n\nepoch 11 learning rate 0.100000\n\n[epoch 11][0/749] loss 5.2418 accuracy 0.00%\n[epoch 11][30/749] loss 5.1990 accuracy 0.00%\n[epoch 11][60/749] loss 4.8616 accuracy 0.00%\n[epoch 11][90/749] loss 5.0337 accuracy 0.00%\n[epoch 11][120/749] loss 4.9706 accuracy 0.00%\n[epoch 11][150/749] loss 5.0169 accuracy 12.50%\n[epoch 11][180/749] loss 5.1096 accuracy 0.00%\n[epoch 11][210/749] loss 5.1147 accuracy 0.00%\n[epoch 11][240/749] loss 5.0032 accuracy 0.00%\n[epoch 11][270/749] loss 5.3281 accuracy 0.00%\n[epoch 11][300/749] loss 5.5230 accuracy 0.00%\n[epoch 11][330/749] loss 5.0278 accuracy 0.00%\n[epoch 11][360/749] loss 5.0466 accuracy 0.00%\n[epoch 11][390/749] loss 5.1224 accuracy 0.00%\n[epoch 11][420/749] loss 5.2158 accuracy 0.00%\n[epoch 11][450/749] loss 5.4504 accuracy 0.00%\n[epoch 11][480/749] loss 5.3408 accuracy 0.00%\n[epoch 11][510/749] loss 5.0532 accuracy 12.50%\n[epoch 11][540/749] loss 5.2509 accuracy 0.00%\n[epoch 11][570/749] loss 5.2382 accuracy 0.00%\n[epoch 11][600/749] loss 4.6107 accuracy 12.50%\n[epoch 11][630/749] loss 5.3479 accuracy 0.00%\n[epoch 11][660/749] loss 4.7522 accuracy 12.50%\n[epoch 11][690/749] loss 5.4469 accuracy 0.00%\n[epoch 11][720/749] loss 4.9800 accuracy 0.00%\n----------------------------------------\n\n[epoch 11] accuracy on test data: 1.12%\n\n----------------------------------------\n\nepoch 12 learning rate 0.100000\n\n[epoch 12][0/749] loss 5.1057 accuracy 0.00%\n[epoch 12][30/749] loss 4.7215 accuracy 0.00%\n[epoch 12][60/749] loss 5.1804 accuracy 0.00%\n[epoch 12][90/749] loss 5.2325 accuracy 0.00%\n[epoch 12][120/749] loss 5.1408 accuracy 0.00%\n[epoch 12][150/749] loss 4.9642 accuracy 0.00%\n[epoch 12][180/749] loss 5.1749 accuracy 0.00%\n[epoch 12][210/749] loss 5.4350 accuracy 0.00%\n[epoch 12][240/749] loss 5.1333 accuracy 0.00%\n[epoch 12][270/749] loss 5.2989 accuracy 0.00%\n[epoch 12][300/749] loss 5.0827 accuracy 0.00%\n[epoch 12][330/749] loss 4.9536 accuracy 0.00%\n[epoch 12][360/749] loss 5.1440 accuracy 0.00%\n[epoch 12][390/749] loss 4.9677 accuracy 0.00%\n[epoch 12][420/749] loss 5.1131 accuracy 0.00%\n[epoch 12][450/749] loss 5.2247 accuracy 0.00%\n[epoch 12][480/749] loss 4.9417 accuracy 0.00%\n[epoch 12][510/749] loss 4.9090 accuracy 0.00%\n[epoch 12][540/749] loss 5.2685 accuracy 0.00%\n[epoch 12][570/749] loss 5.2450 accuracy 0.00%\n[epoch 12][600/749] loss 5.6893 accuracy 0.00%\n[epoch 12][630/749] loss 5.0761 accuracy 25.00%\n[epoch 12][660/749] loss 5.0009 accuracy 0.00%\n[epoch 12][690/749] loss 5.1927 accuracy 0.00%\n[epoch 12][720/749] loss 5.5929 accuracy 0.00%\n----------------------------------------\n\n[epoch 12] accuracy on test data: 1.05%\n\n----------------------------------------\n\nepoch 13 learning rate 0.100000\n\n[epoch 13][0/749] loss 4.9702 accuracy 0.00%\n[epoch 13][30/749] loss 5.2891 accuracy 0.00%\n[epoch 13][60/749] loss 5.1115 accuracy 0.00%\n[epoch 13][90/749] loss 4.9976 accuracy 0.00%\n[epoch 13][120/749] loss 5.0512 accuracy 0.00%\n[epoch 13][150/749] loss 5.0275 accuracy 0.00%\n[epoch 13][180/749] loss 5.0121 accuracy 12.50%\n[epoch 13][210/749] loss 5.2867 accuracy 0.00%\n[epoch 13][240/749] loss 5.4645 accuracy 0.00%\n[epoch 13][270/749] loss 4.6013 accuracy 12.50%\n[epoch 13][300/749] loss 4.9995 accuracy 0.00%\n[epoch 13][330/749] loss 5.0937 accuracy 0.00%\n[epoch 13][360/749] loss 5.0815 accuracy 12.50%\n[epoch 13][390/749] loss 4.5556 accuracy 0.00%\n[epoch 13][420/749] loss 5.1844 accuracy 0.00%\n[epoch 13][450/749] loss 5.1706 accuracy 0.00%\n[epoch 13][480/749] loss 5.4258 accuracy 0.00%\n[epoch 13][510/749] loss 5.0130 accuracy 0.00%\n[epoch 13][540/749] loss 5.2686 accuracy 0.00%\n[epoch 13][570/749] loss 5.1545 accuracy 0.00%\n[epoch 13][600/749] loss 5.4338 accuracy 0.00%\n[epoch 13][630/749] loss 4.8985 accuracy 0.00%\n[epoch 13][660/749] loss 4.9989 accuracy 0.00%\n[epoch 13][690/749] loss 4.9773 accuracy 0.00%\n[epoch 13][720/749] loss 4.8631 accuracy 12.50%\n----------------------------------------\n\n[epoch 13] accuracy on test data: 0.66%\n\n----------------------------------------\n\nepoch 14 learning rate 0.100000\n\n[epoch 14][0/749] loss 5.4798 accuracy 0.00%\n[epoch 14][30/749] loss 4.7296 accuracy 0.00%\n[epoch 14][60/749] loss 4.9818 accuracy 0.00%\n[epoch 14][90/749] loss 5.0411 accuracy 0.00%\n[epoch 14][120/749] loss 5.1067 accuracy 0.00%\n[epoch 14][150/749] loss 5.3561 accuracy 0.00%\n[epoch 14][180/749] loss 5.1476 accuracy 0.00%\n[epoch 14][210/749] loss 4.7308 accuracy 0.00%\n[epoch 14][240/749] loss 4.9905 accuracy 0.00%\n[epoch 14][270/749] loss 5.1509 accuracy 0.00%\n[epoch 14][300/749] loss 5.2440 accuracy 0.00%\n[epoch 14][330/749] loss 5.1339 accuracy 0.00%\n[epoch 14][360/749] loss 5.4801 accuracy 0.00%\n[epoch 14][390/749] loss 5.2686 accuracy 0.00%\n[epoch 14][420/749] loss 4.8252 accuracy 0.00%\n[epoch 14][450/749] loss 5.1872 accuracy 0.00%\n[epoch 14][480/749] loss 5.2518 accuracy 0.00%\n[epoch 14][510/749] loss 4.7324 accuracy 12.50%\n[epoch 14][540/749] loss 5.2670 accuracy 0.00%\n[epoch 14][570/749] loss 5.3566 accuracy 0.00%\n[epoch 14][600/749] loss 5.0477 accuracy 0.00%\n[epoch 14][630/749] loss 4.9648 accuracy 0.00%\n[epoch 14][660/749] loss 5.3458 accuracy 0.00%\n[epoch 14][690/749] loss 5.0394 accuracy 0.00%\n[epoch 14][720/749] loss 5.0251 accuracy 0.00%\n----------------------------------------\n\n[epoch 14] accuracy on test data: 1.28%\n\n----------------------------------------\n\nepoch 15 learning rate 0.100000\n\n[epoch 15][0/749] loss 5.3270 accuracy 0.00%\n[epoch 15][30/749] loss 4.9363 accuracy 0.00%\n[epoch 15][60/749] loss 5.0056 accuracy 0.00%\n[epoch 15][90/749] loss 4.9226 accuracy 0.00%\n[epoch 15][120/749] loss 5.0071 accuracy 12.50%\n[epoch 15][150/749] loss 5.3785 accuracy 0.00%\n[epoch 15][180/749] loss 5.1221 accuracy 0.00%\n[epoch 15][210/749] loss 5.1247 accuracy 0.00%\n[epoch 15][240/749] loss 5.2397 accuracy 0.00%\n[epoch 15][270/749] loss 5.1276 accuracy 0.00%\n[epoch 15][300/749] loss 4.9745 accuracy 0.00%\n[epoch 15][330/749] loss 4.7599 accuracy 0.00%\n[epoch 15][360/749] loss 5.1701 accuracy 0.00%\n[epoch 15][390/749] loss 5.0166 accuracy 12.50%\n[epoch 15][420/749] loss 5.3784 accuracy 0.00%\n[epoch 15][450/749] loss 5.1830 accuracy 0.00%\n[epoch 15][480/749] loss 4.8879 accuracy 0.00%\n[epoch 15][510/749] loss 5.1488 accuracy 0.00%\n[epoch 15][540/749] loss 5.3694 accuracy 0.00%\n[epoch 15][570/749] loss 5.4129 accuracy 0.00%\n[epoch 15][600/749] loss 5.2002 accuracy 0.00%\n[epoch 15][630/749] loss 5.0161 accuracy 0.00%\n[epoch 15][660/749] loss 4.9780 accuracy 12.50%\n[epoch 15][690/749] loss 5.1594 accuracy 0.00%\n[epoch 15][720/749] loss 5.3257 accuracy 0.00%\n----------------------------------------\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b6c005dc6311>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test/accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "log_freq = 30\n",
    "epochs = 100\n",
    "save_freq = 10\n",
    "writer = SummaryWriter('./runs/cub_attempt_3')\n",
    "for epoch in range(epochs):\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    # run for one epoch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # warm up\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        pred, _, _, _ = model(inputs)\n",
    "\n",
    "        # backward\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display results\n",
    "        if i % log_freq == 0:\n",
    "            model.eval()\n",
    "            pred, __, __, __ = model(inputs)\n",
    "            predict = torch.argmax(pred, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = torch.eq(predict, labels).sum().double().item()\n",
    "            accuracy = correct / total\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "            writer.add_scalar('train/accuracy', accuracy, step)\n",
    "\n",
    "            print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
    "                % (epoch, i, len(train_loader)-1, loss.item(), (100*accuracy)))\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \n",
    "            \"./checkpoints/cub_attn-net_epoch_%s.pth\" % epoch\n",
    "        )\n",
    "\n",
    "    print('-'*40)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # log scalars\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images_test, labels_test = data\n",
    "            images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "            pred_test, _, _, _ = model(images_test)\n",
    "            predict = torch.argmax(pred_test, 1)\n",
    "            total += labels_test.size(0)\n",
    "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "        writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
    "    \n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly not working... Looking at the paper, I realized that the model for CUB-2011 was pre-trained on CIFAR-100. Luckily, I just trained a CIFAR model, so let's see if simply downsizing the image by alot and feeding it into that network can make fine tuning work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = Cub2011(root='../../../data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_set = Cub2011(root='../../../data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "conv_block1.op.0.weight\nconv_block1.op.0.bias\nconv_block1.op.1.weight\nconv_block1.op.1.bias\nconv_block1.op.1.running_mean\nconv_block1.op.1.running_var\nconv_block1.op.1.num_batches_tracked\nconv_block1.op.3.weight\nconv_block1.op.3.bias\nconv_block1.op.4.weight\nconv_block1.op.4.bias\nconv_block1.op.4.running_mean\nconv_block1.op.4.running_var\nconv_block1.op.4.num_batches_tracked\nconv_block2.op.0.weight\nconv_block2.op.0.bias\nconv_block2.op.1.weight\nconv_block2.op.1.bias\nconv_block2.op.1.running_mean\nconv_block2.op.1.running_var\nconv_block2.op.1.num_batches_tracked\nconv_block2.op.3.weight\nconv_block2.op.3.bias\nconv_block2.op.4.weight\nconv_block2.op.4.bias\nconv_block2.op.4.running_mean\nconv_block2.op.4.running_var\nconv_block2.op.4.num_batches_tracked\nconv_block3.op.0.weight\nconv_block3.op.0.bias\nconv_block3.op.1.weight\nconv_block3.op.1.bias\nconv_block3.op.1.running_mean\nconv_block3.op.1.running_var\nconv_block3.op.1.num_batches_tracked\nconv_block3.op.3.weight\nconv_block3.op.3.bias\nconv_block3.op.4.weight\nconv_block3.op.4.bias\nconv_block3.op.4.running_mean\nconv_block3.op.4.running_var\nconv_block3.op.4.num_batches_tracked\nconv_block3.op.6.weight\nconv_block3.op.6.bias\nconv_block3.op.7.weight\nconv_block3.op.7.bias\nconv_block3.op.7.running_mean\nconv_block3.op.7.running_var\nconv_block3.op.7.num_batches_tracked\nconv_block4.op.0.weight\nconv_block4.op.0.bias\nconv_block4.op.1.weight\nconv_block4.op.1.bias\nconv_block4.op.1.running_mean\nconv_block4.op.1.running_var\nconv_block4.op.1.num_batches_tracked\nconv_block4.op.3.weight\nconv_block4.op.3.bias\nconv_block4.op.4.weight\nconv_block4.op.4.bias\nconv_block4.op.4.running_mean\nconv_block4.op.4.running_var\nconv_block4.op.4.num_batches_tracked\nconv_block4.op.6.weight\nconv_block4.op.6.bias\nconv_block4.op.7.weight\nconv_block4.op.7.bias\nconv_block4.op.7.running_mean\nconv_block4.op.7.running_var\nconv_block4.op.7.num_batches_tracked\nconv_block5.op.0.weight\nconv_block5.op.0.bias\nconv_block5.op.1.weight\nconv_block5.op.1.bias\nconv_block5.op.1.running_mean\nconv_block5.op.1.running_var\nconv_block5.op.1.num_batches_tracked\nconv_block5.op.3.weight\nconv_block5.op.3.bias\nconv_block5.op.4.weight\nconv_block5.op.4.bias\nconv_block5.op.4.running_mean\nconv_block5.op.4.running_var\nconv_block5.op.4.num_batches_tracked\nconv_block5.op.6.weight\nconv_block5.op.6.bias\nconv_block5.op.7.weight\nconv_block5.op.7.bias\nconv_block5.op.7.running_mean\nconv_block5.op.7.running_var\nconv_block5.op.7.num_batches_tracked\nconv_block6.op.0.weight\nconv_block6.op.0.bias\nconv_block6.op.1.weight\nconv_block6.op.1.bias\nconv_block6.op.1.running_mean\nconv_block6.op.1.running_var\nconv_block6.op.1.num_batches_tracked\nconv_block6.op.4.weight\nconv_block6.op.4.bias\nconv_block6.op.5.weight\nconv_block6.op.5.bias\nconv_block6.op.5.running_mean\nconv_block6.op.5.running_var\nconv_block6.op.5.num_batches_tracked\ndense.weight\ndense.bias\nprojector.op.weight\nattn1.op.weight\nattn2.op.weight\nattn3.op.weight\nclassify.weight\nclassify.bias\n"
    }
   ],
   "source": [
    "checkpoint = torch.load('./checkpoints/attn-net_epoch_90.pth')\n",
    "pretrained_dict = checkpoint['model_state_dict']\n",
    "for k,v in pretrained_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnVGG(im_size=32, num_classes=200, normalize_attn=False)\n",
    "\n",
    "# Load the last CIFAR-100 checkpoint. Note we filter out the layers that have different dimensions\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if 'dense' not in k and 'classify' not in k}\n",
    "\n",
    "model.load_state_dict(pretrained_dict, strict=False)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acy 0.00%\n[epoch 1][60/749] loss 5.4958 accuracy 0.00%\n[epoch 1][90/749] loss 5.3713 accuracy 0.00%\n[epoch 1][120/749] loss 5.1200 accuracy 0.00%\n[epoch 1][150/749] loss 5.0517 accuracy 0.00%\n[epoch 1][180/749] loss 5.2124 accuracy 0.00%\n[epoch 1][210/749] loss 5.1939 accuracy 0.00%\n[epoch 1][240/749] loss 5.0347 accuracy 0.00%\n[epoch 1][270/749] loss 4.9467 accuracy 0.00%\n[epoch 1][300/749] loss 4.9516 accuracy 0.00%\n[epoch 1][330/749] loss 5.1093 accuracy 0.00%\n[epoch 1][360/749] loss 4.7932 accuracy 0.00%\n[epoch 1][390/749] loss 5.1466 accuracy 0.00%\n[epoch 1][420/749] loss 4.9345 accuracy 0.00%\n[epoch 1][450/749] loss 5.4860 accuracy 0.00%\n[epoch 1][480/749] loss 5.0639 accuracy 0.00%\n[epoch 1][510/749] loss 4.9141 accuracy 0.00%\n[epoch 1][540/749] loss 5.2186 accuracy 0.00%\n[epoch 1][570/749] loss 5.2515 accuracy 0.00%\n[epoch 1][600/749] loss 5.2333 accuracy 0.00%\n[epoch 1][630/749] loss 5.1549 accuracy 0.00%\n[epoch 1][660/749] loss 5.3216 accuracy 0.00%\n[epoch 1][690/749] loss 4.8274 accuracy 0.00%\n[epoch 1][720/749] loss 5.2157 accuracy 0.00%\n----------------------------------------\n\n[epoch 1] accuracy on test data: 1.19%\n\n----------------------------------------\n\nepoch 2 learning rate 0.100000\n\n[epoch 2][0/749] loss 5.0144 accuracy 0.00%\n[epoch 2][30/749] loss 4.8191 accuracy 0.00%\n[epoch 2][60/749] loss 4.9499 accuracy 0.00%\n[epoch 2][90/749] loss 5.0589 accuracy 0.00%\n[epoch 2][120/749] loss 5.3976 accuracy 0.00%\n[epoch 2][150/749] loss 5.0123 accuracy 0.00%\n[epoch 2][180/749] loss 5.0508 accuracy 0.00%\n[epoch 2][210/749] loss 4.9266 accuracy 0.00%\n[epoch 2][240/749] loss 5.0573 accuracy 0.00%\n[epoch 2][270/749] loss 5.0928 accuracy 0.00%\n[epoch 2][300/749] loss 5.0851 accuracy 0.00%\n[epoch 2][330/749] loss 5.5033 accuracy 0.00%\n[epoch 2][360/749] loss 4.9505 accuracy 12.50%\n[epoch 2][390/749] loss 5.2111 accuracy 0.00%\n[epoch 2][420/749] loss 5.1119 accuracy 0.00%\n[epoch 2][450/749] loss 5.4761 accuracy 0.00%\n[epoch 2][480/749] loss 5.0160 accuracy 0.00%\n[epoch 2][510/749] loss 5.1995 accuracy 0.00%\n[epoch 2][540/749] loss 5.6931 accuracy 0.00%\n[epoch 2][570/749] loss 5.1850 accuracy 0.00%\n[epoch 2][600/749] loss 5.2685 accuracy 0.00%\n[epoch 2][630/749] loss 5.2718 accuracy 0.00%\n[epoch 2][660/749] loss 4.9587 accuracy 0.00%\n[epoch 2][690/749] loss 4.7869 accuracy 0.00%\n[epoch 2][720/749] loss 4.8765 accuracy 0.00%\n----------------------------------------\n\n[epoch 2] accuracy on test data: 1.35%\n\n----------------------------------------\n\nepoch 3 learning rate 0.100000\n\n[epoch 3][0/749] loss 4.8551 accuracy 0.00%\n[epoch 3][30/749] loss 5.0660 accuracy 0.00%\n[epoch 3][60/749] loss 5.3099 accuracy 0.00%\n[epoch 3][90/749] loss 4.9304 accuracy 0.00%\n[epoch 3][120/749] loss 5.1957 accuracy 0.00%\n[epoch 3][150/749] loss 5.0293 accuracy 0.00%\n[epoch 3][180/749] loss 5.2584 accuracy 0.00%\n[epoch 3][210/749] loss 4.8340 accuracy 0.00%\n[epoch 3][240/749] loss 4.8167 accuracy 0.00%\n[epoch 3][270/749] loss 5.3125 accuracy 0.00%\n[epoch 3][300/749] loss 5.2168 accuracy 0.00%\n[epoch 3][330/749] loss 5.3081 accuracy 0.00%\n[epoch 3][360/749] loss 5.0724 accuracy 0.00%\n[epoch 3][390/749] loss 5.1875 accuracy 0.00%\n[epoch 3][420/749] loss 4.9953 accuracy 12.50%\n[epoch 3][450/749] loss 5.2988 accuracy 0.00%\n[epoch 3][480/749] loss 4.9554 accuracy 0.00%\n[epoch 3][510/749] loss 5.2886 accuracy 0.00%\n[epoch 3][540/749] loss 5.4960 accuracy 0.00%\n[epoch 3][570/749] loss 5.2574 accuracy 0.00%\n[epoch 3][600/749] loss 4.9754 accuracy 0.00%\n[epoch 3][630/749] loss 5.4340 accuracy 0.00%\n[epoch 3][660/749] loss 5.3012 accuracy 0.00%\n[epoch 3][690/749] loss 4.9274 accuracy 0.00%\n[epoch 3][720/749] loss 5.0145 accuracy 0.00%\n----------------------------------------\n\n[epoch 3] accuracy on test data: 1.07%\n\n----------------------------------------\n\nepoch 4 learning rate 0.100000\n\n[epoch 4][0/749] loss 4.8704 accuracy 12.50%\n[epoch 4][30/749] loss 5.3227 accuracy 0.00%\n[epoch 4][60/749] loss 4.7544 accuracy 12.50%\n[epoch 4][90/749] loss 5.5043 accuracy 0.00%\n[epoch 4][120/749] loss 4.7711 accuracy 0.00%\n[epoch 4][150/749] loss 4.9386 accuracy 12.50%\n[epoch 4][180/749] loss 4.9177 accuracy 0.00%\n[epoch 4][210/749] loss 5.1675 accuracy 0.00%\n[epoch 4][240/749] loss 5.1103 accuracy 0.00%\n[epoch 4][270/749] loss 4.8171 accuracy 0.00%\n[epoch 4][300/749] loss 4.7664 accuracy 25.00%\n[epoch 4][330/749] loss 5.0470 accuracy 0.00%\n[epoch 4][360/749] loss 5.1560 accuracy 0.00%\n[epoch 4][390/749] loss 4.9540 accuracy 0.00%\n[epoch 4][420/749] loss 4.8542 accuracy 0.00%\n[epoch 4][450/749] loss 5.2592 accuracy 0.00%\n[epoch 4][480/749] loss 4.9402 accuracy 0.00%\n[epoch 4][510/749] loss 5.1920 accuracy 0.00%\n[epoch 4][540/749] loss 5.0297 accuracy 0.00%\n[epoch 4][570/749] loss 5.1119 accuracy 0.00%\n[epoch 4][600/749] loss 5.3134 accuracy 0.00%\n[epoch 4][630/749] loss 5.0187 accuracy 0.00%\n[epoch 4][660/749] loss 5.1295 accuracy 0.00%\n[epoch 4][690/749] loss 4.8996 accuracy 0.00%\n[epoch 4][720/749] loss 5.1777 accuracy 0.00%\n----------------------------------------\n\n[epoch 4] accuracy on test data: 1.23%\n\n----------------------------------------\n\nepoch 5 learning rate 0.100000\n\n[epoch 5][0/749] loss 5.1380 accuracy 0.00%\n[epoch 5][30/749] loss 5.0417 accuracy 0.00%\n[epoch 5][60/749] loss 5.2337 accuracy 0.00%\n[epoch 5][90/749] loss 4.7049 accuracy 0.00%\n[epoch 5][120/749] loss 5.3242 accuracy 0.00%\n[epoch 5][150/749] loss 4.9069 accuracy 0.00%\n[epoch 5][180/749] loss 4.7326 accuracy 0.00%\n[epoch 5][210/749] loss 5.0288 accuracy 0.00%\n[epoch 5][240/749] loss 4.9907 accuracy 0.00%\n[epoch 5][270/749] loss 5.1202 accuracy 0.00%\n[epoch 5][300/749] loss 5.1103 accuracy 0.00%\n[epoch 5][330/749] loss 5.1646 accuracy 0.00%\n[epoch 5][360/749] loss 4.5744 accuracy 0.00%\n[epoch 5][390/749] loss 5.1699 accuracy 0.00%\n[epoch 5][420/749] loss 5.3525 accuracy 0.00%\n[epoch 5][450/749] loss 5.1683 accuracy 0.00%\n[epoch 5][480/749] loss 5.2224 accuracy 0.00%\n[epoch 5][510/749] loss 5.2184 accuracy 0.00%\n[epoch 5][540/749] loss 5.0888 accuracy 0.00%\n[epoch 5][570/749] loss 5.1733 accuracy 0.00%\n[epoch 5][600/749] loss 4.9937 accuracy 0.00%\n[epoch 5][630/749] loss 5.2453 accuracy 0.00%\n[epoch 5][660/749] loss 4.7374 accuracy 0.00%\n[epoch 5][690/749] loss 5.3886 accuracy 0.00%\n[epoch 5][720/749] loss 5.0846 accuracy 12.50%\n----------------------------------------\n\n[epoch 5] accuracy on test data: 0.98%\n\n----------------------------------------\n\nepoch 6 learning rate 0.100000\n\n[epoch 6][0/749] loss 4.9335 accuracy 0.00%\n[epoch 6][30/749] loss 5.0639 accuracy 0.00%\n[epoch 6][60/749] loss 5.0170 accuracy 0.00%\n[epoch 6][90/749] loss 5.2451 accuracy 0.00%\n[epoch 6][120/749] loss 5.0395 accuracy 0.00%\n[epoch 6][150/749] loss 5.0284 accuracy 12.50%\n[epoch 6][180/749] loss 5.3338 accuracy 0.00%\n[epoch 6][210/749] loss 5.0763 accuracy 0.00%\n[epoch 6][240/749] loss 4.6626 accuracy 0.00%\n[epoch 6][270/749] loss 5.0659 accuracy 0.00%\n[epoch 6][300/749] loss 5.2055 accuracy 0.00%\n[epoch 6][330/749] loss 5.0793 accuracy 0.00%\n[epoch 6][360/749] loss 4.9515 accuracy 0.00%\n[epoch 6][390/749] loss 4.9489 accuracy 0.00%\n[epoch 6][420/749] loss 5.0857 accuracy 0.00%\n[epoch 6][450/749] loss 5.1647 accuracy 0.00%\n[epoch 6][480/749] loss 4.8664 accuracy 0.00%\n[epoch 6][510/749] loss 5.5463 accuracy 0.00%\n[epoch 6][540/749] loss 5.0772 accuracy 0.00%\n[epoch 6][570/749] loss 4.8412 accuracy 0.00%\n[epoch 6][600/749] loss 5.0089 accuracy 0.00%\n[epoch 6][630/749] loss 4.7809 accuracy 0.00%\n[epoch 6][660/749] loss 4.9583 accuracy 0.00%\n[epoch 6][690/749] loss 4.9772 accuracy 0.00%\n[epoch 6][720/749] loss 5.1201 accuracy 0.00%\n----------------------------------------\n\n[epoch 6] accuracy on test data: 1.23%\n\n----------------------------------------\n\nepoch 7 learning rate 0.100000\n\n[epoch 7][0/749] loss 5.1735 accuracy 0.00%\n[epoch 7][30/749] loss 5.1958 accuracy 0.00%\n[epoch 7][60/749] loss 5.1979 accuracy 0.00%\n[epoch 7][90/749] loss 4.8084 accuracy 0.00%\n[epoch 7][120/749] loss 5.3686 accuracy 0.00%\n[epoch 7][150/749] loss 5.0846 accuracy 0.00%\n[epoch 7][180/749] loss 4.7485 accuracy 12.50%\n[epoch 7][210/749] loss 5.2179 accuracy 0.00%\n[epoch 7][240/749] loss 5.3400 accuracy 0.00%\n[epoch 7][270/749] loss 5.4262 accuracy 0.00%\n[epoch 7][300/749] loss 5.2351 accuracy 0.00%\n[epoch 7][330/749] loss 4.9972 accuracy 0.00%\n[epoch 7][360/749] loss 5.0319 accuracy 0.00%\n[epoch 7][390/749] loss 5.1850 accuracy 0.00%\n[epoch 7][420/749] loss 4.8878 accuracy 12.50%\n[epoch 7][450/749] loss 4.9976 accuracy 0.00%\n[epoch 7][480/749] loss 5.3991 accuracy 0.00%\n[epoch 7][510/749] loss 4.7254 accuracy 0.00%\n[epoch 7][540/749] loss 5.1020 accuracy 0.00%\n[epoch 7][570/749] loss 5.3160 accuracy 0.00%\n[epoch 7][600/749] loss 5.1302 accuracy 0.00%\n[epoch 7][630/749] loss 5.4346 accuracy 0.00%\n[epoch 7][660/749] loss 4.7312 accuracy 12.50%\n[epoch 7][690/749] loss 5.2220 accuracy 0.00%\n[epoch 7][720/749] loss 5.2268 accuracy 0.00%\n----------------------------------------\n\n[epoch 7] accuracy on test data: 1.26%\n\n----------------------------------------\n\nepoch 8 learning rate 0.100000\n\n[epoch 8][0/749] loss 4.5871 accuracy 0.00%\n[epoch 8][30/749] loss 5.0219 accuracy 0.00%\n[epoch 8][60/749] loss 5.0902 accuracy 0.00%\n[epoch 8][90/749] loss 5.2678 accuracy 0.00%\n[epoch 8][120/749] loss 4.7242 accuracy 0.00%\n[epoch 8][150/749] loss 5.0150 accuracy 0.00%\n[epoch 8][180/749] loss 5.0008 accuracy 0.00%\n[epoch 8][210/749] loss 5.1887 accuracy 0.00%\n[epoch 8][240/749] loss 5.6005 accuracy 0.00%\n[epoch 8][270/749] loss 5.0491 accuracy 0.00%\n[epoch 8][300/749] loss 4.9438 accuracy 0.00%\n[epoch 8][330/749] loss 5.6710 accuracy 0.00%\n[epoch 8][360/749] loss 5.4525 accuracy 0.00%\n[epoch 8][390/749] loss 5.3471 accuracy 0.00%\n[epoch 8][420/749] loss 5.1160 accuracy 0.00%\n[epoch 8][450/749] loss 4.9304 accuracy 0.00%\n[epoch 8][480/749] loss 4.8042 accuracy 0.00%\n[epoch 8][510/749] loss 5.1175 accuracy 0.00%\n[epoch 8][540/749] loss 5.1114 accuracy 0.00%\n[epoch 8][570/749] loss 4.9346 accuracy 0.00%\n[epoch 8][600/749] loss 4.7195 accuracy 0.00%\n[epoch 8][630/749] loss 4.9680 accuracy 0.00%\n[epoch 8][660/749] loss 5.3420 accuracy 0.00%\n[epoch 8][690/749] loss 4.8158 accuracy 0.00%\n[epoch 8][720/749] loss 4.7685 accuracy 0.00%\n----------------------------------------\n\n[epoch 8] accuracy on test data: 1.62%\n\n----------------------------------------\n\nepoch 9 learning rate 0.100000\n\n[epoch 9][0/749] loss 4.7892 accuracy 0.00%\n[epoch 9][30/749] loss 4.8312 accuracy 0.00%\n[epoch 9][60/749] loss 5.1233 accuracy 0.00%\n[epoch 9][90/749] loss 5.1113 accuracy 0.00%\n[epoch 9][120/749] loss 4.6932 accuracy 0.00%\n[epoch 9][150/749] loss 4.8738 accuracy 0.00%\n[epoch 9][180/749] loss 4.8467 accuracy 0.00%\n[epoch 9][210/749] loss 4.8252 accuracy 0.00%\n[epoch 9][240/749] loss 5.0240 accuracy 12.50%\n[epoch 9][270/749] loss 4.7901 accuracy 12.50%\n[epoch 9][300/749] loss 4.7454 accuracy 12.50%\n[epoch 9][330/749] loss 5.1035 accuracy 0.00%\n[epoch 9][360/749] loss 5.1988 accuracy 0.00%\n[epoch 9][390/749] loss 4.9034 accuracy 0.00%\n[epoch 9][420/749] loss 5.1596 accuracy 0.00%\n[epoch 9][450/749] loss 5.0355 accuracy 0.00%\n[epoch 9][480/749] loss 5.0208 accuracy 0.00%\n[epoch 9][510/749] loss 4.8467 accuracy 0.00%\n[epoch 9][540/749] loss 5.1486 accuracy 0.00%\n[epoch 9][570/749] loss 5.4386 accuracy 0.00%\n[epoch 9][600/749] loss 5.2273 accuracy 0.00%\n[epoch 9][630/749] loss 4.8392 accuracy 0.00%\n[epoch 9][660/749] loss 4.7913 accuracy 0.00%\n[epoch 9][690/749] loss 4.6642 accuracy 12.50%\n[epoch 9][720/749] loss 5.6747 accuracy 0.00%\n----------------------------------------\n\n[epoch 9] accuracy on test data: 1.57%\n\n----------------------------------------\n\nepoch 10 learning rate 0.100000\n\n[epoch 10][0/749] loss 5.1618 accuracy 0.00%\n[epoch 10][30/749] loss 5.0460 accuracy 12.50%\n[epoch 10][60/749] loss 5.3213 accuracy 0.00%\n[epoch 10][90/749] loss 5.3050 accuracy 0.00%\n[epoch 10][120/749] loss 4.8933 accuracy 0.00%\n[epoch 10][150/749] loss 5.0883 accuracy 0.00%\n[epoch 10][180/749] loss 5.3644 accuracy 0.00%\n[epoch 10][210/749] loss 4.8250 accuracy 12.50%\n[epoch 10][240/749] loss 4.5484 accuracy 25.00%\n[epoch 10][270/749] loss 4.9679 accuracy 0.00%\n[epoch 10][300/749] loss 5.2193 accuracy 0.00%\n[epoch 10][330/749] loss 5.0841 accuracy 0.00%\n[epoch 10][360/749] loss 5.0500 accuracy 0.00%\n[epoch 10][390/749] loss 4.7499 accuracy 0.00%\n[epoch 10][420/749] loss 5.1381 accuracy 0.00%\n[epoch 10][450/749] loss 5.0383 accuracy 12.50%\n[epoch 10][480/749] loss 5.0417 accuracy 12.50%\n[epoch 10][510/749] loss 5.1360 accuracy 0.00%\n[epoch 10][540/749] loss 4.5803 accuracy 12.50%\n[epoch 10][570/749] loss 5.1743 accuracy 0.00%\n[epoch 10][600/749] loss 4.8874 accuracy 12.50%\n[epoch 10][630/749] loss 4.5587 accuracy 0.00%\n[epoch 10][660/749] loss 4.8564 accuracy 0.00%\n[epoch 10][690/749] loss 4.6013 accuracy 12.50%\n[epoch 10][720/749] loss 5.2123 accuracy 0.00%\n----------------------------------------\n\n[epoch 10] accuracy on test data: 1.48%\n\n----------------------------------------\n\nepoch 11 learning rate 0.100000\n\n[epoch 11][0/749] loss 4.9114 accuracy 0.00%\n[epoch 11][30/749] loss 4.9120 accuracy 0.00%\n[epoch 11][60/749] loss 5.1881 accuracy 0.00%\n[epoch 11][90/749] loss 4.9669 accuracy 0.00%\n[epoch 11][120/749] loss 5.1710 accuracy 0.00%\n[epoch 11][150/749] loss 4.9936 accuracy 0.00%\n[epoch 11][180/749] loss 5.1514 accuracy 0.00%\n[epoch 11][210/749] loss 5.1066 accuracy 12.50%\n[epoch 11][240/749] loss 5.1327 accuracy 0.00%\n[epoch 11][270/749] loss 5.2557 accuracy 0.00%\n[epoch 11][300/749] loss 5.4408 accuracy 0.00%\n[epoch 11][330/749] loss 5.1346 accuracy 0.00%\n[epoch 11][360/749] loss 5.1446 accuracy 0.00%\n[epoch 11][390/749] loss 4.9638 accuracy 0.00%\n[epoch 11][420/749] loss 4.9014 accuracy 0.00%\n[epoch 11][450/749] loss 4.7609 accuracy 0.00%\n[epoch 11][480/749] loss 5.2662 accuracy 0.00%\n[epoch 11][510/749] loss 5.2693 accuracy 0.00%\n[epoch 11][540/749] loss 5.2413 accuracy 0.00%\n[epoch 11][570/749] loss 4.7813 accuracy 0.00%\n[epoch 11][600/749] loss 4.8746 accuracy 0.00%\n[epoch 11][630/749] loss 5.2570 accuracy 0.00%\n[epoch 11][660/749] loss 5.0962 accuracy 0.00%\n[epoch 11][690/749] loss 5.0652 accuracy 0.00%\n[epoch 11][720/749] loss 5.0172 accuracy 0.00%\n----------------------------------------\n\n[epoch 11] accuracy on test data: 1.71%\n\n----------------------------------------\n\nepoch 12 learning rate 0.100000\n\n[epoch 12][0/749] loss 4.9915 accuracy 0.00%\n[epoch 12][30/749] loss 5.5170 accuracy 0.00%\n[epoch 12][60/749] loss 5.3261 accuracy 0.00%\n[epoch 12][90/749] loss 4.9470 accuracy 0.00%\n[epoch 12][120/749] loss 4.7253 accuracy 0.00%\n[epoch 12][150/749] loss 5.5540 accuracy 0.00%\n[epoch 12][180/749] loss 4.8640 accuracy 12.50%\n[epoch 12][210/749] loss 5.0608 accuracy 0.00%\n[epoch 12][240/749] loss 4.7628 accuracy 0.00%\n[epoch 12][270/749] loss 4.8733 accuracy 0.00%\n[epoch 12][300/749] loss 5.1352 accuracy 0.00%\n[epoch 12][330/749] loss 5.0584 accuracy 0.00%\n[epoch 12][360/749] loss 4.9696 accuracy 12.50%\n[epoch 12][390/749] loss 5.3235 accuracy 12.50%\n[epoch 12][420/749] loss 5.6400 accuracy 0.00%\n[epoch 12][450/749] loss 5.3390 accuracy 0.00%\n[epoch 12][480/749] loss 5.1628 accuracy 0.00%\n[epoch 12][510/749] loss 4.7224 accuracy 0.00%\n[epoch 12][540/749] loss 4.9409 accuracy 0.00%\n[epoch 12][570/749] loss 5.1480 accuracy 0.00%\n[epoch 12][600/749] loss 4.6796 accuracy 0.00%\n[epoch 12][630/749] loss 5.0249 accuracy 0.00%\n[epoch 12][660/749] loss 4.9955 accuracy 0.00%\n[epoch 12][690/749] loss 4.6634 accuracy 0.00%\n[epoch 12][720/749] loss 5.3360 accuracy 0.00%\n----------------------------------------\n\n[epoch 12] accuracy on test data: 1.48%\n\n----------------------------------------\n\nepoch 13 learning rate 0.100000\n\n[epoch 13][0/749] loss 4.9792 accuracy 0.00%\n[epoch 13][30/749] loss 5.2782 accuracy 0.00%\n[epoch 13][60/749] loss 4.8661 accuracy 0.00%\n[epoch 13][90/749] loss 5.2244 accuracy 0.00%\n[epoch 13][120/749] loss 5.1759 accuracy 0.00%\n[epoch 13][150/749] loss 5.5651 accuracy 0.00%\n[epoch 13][180/749] loss 5.0674 accuracy 0.00%\n[epoch 13][210/749] loss 5.1999 accuracy 0.00%\n[epoch 13][240/749] loss 5.3616 accuracy 0.00%\n[epoch 13][270/749] loss 5.0116 accuracy 0.00%\n[epoch 13][300/749] loss 4.8390 accuracy 0.00%\n[epoch 13][330/749] loss 4.9742 accuracy 0.00%\n[epoch 13][360/749] loss 5.4594 accuracy 0.00%\n[epoch 13][390/749] loss 4.9442 accuracy 0.00%\n[epoch 13][420/749] loss 5.1012 accuracy 0.00%\n[epoch 13][450/749] loss 4.9825 accuracy 0.00%\n[epoch 13][480/749] loss 5.3631 accuracy 0.00%\n[epoch 13][510/749] loss 4.9138 accuracy 0.00%\n[epoch 13][540/749] loss 5.3717 accuracy 0.00%\n[epoch 13][570/749] loss 5.1221 accuracy 0.00%\n[epoch 13][600/749] loss 5.1406 accuracy 0.00%\n[epoch 13][630/749] loss 5.1237 accuracy 0.00%\n[epoch 13][660/749] loss 5.4369 accuracy 0.00%\n[epoch 13][690/749] loss 5.3873 accuracy 0.00%\n[epoch 13][720/749] loss 4.7140 accuracy 0.00%\n----------------------------------------\n\n[epoch 13] accuracy on test data: 1.54%\n\n----------------------------------------\n\nepoch 14 learning rate 0.100000\n\n[epoch 14][0/749] loss 5.2102 accuracy 0.00%\n[epoch 14][30/749] loss 5.3687 accuracy 0.00%\n[epoch 14][60/749] loss 4.5519 accuracy 0.00%\n[epoch 14][90/749] loss 5.0621 accuracy 0.00%\n[epoch 14][120/749] loss 5.0244 accuracy 0.00%\n[epoch 14][150/749] loss 5.2277 accuracy 0.00%\n[epoch 14][180/749] loss 4.8231 accuracy 0.00%\n[epoch 14][210/749] loss 5.0939 accuracy 0.00%\n[epoch 14][240/749] loss 4.7397 accuracy 12.50%\n[epoch 14][270/749] loss 4.9901 accuracy 0.00%\n[epoch 14][300/749] loss 4.9298 accuracy 12.50%\n[epoch 14][330/749] loss 4.7974 accuracy 0.00%\n[epoch 14][360/749] loss 4.8741 accuracy 0.00%\n[epoch 14][390/749] loss 4.8763 accuracy 0.00%\n[epoch 14][420/749] loss 4.6381 accuracy 0.00%\n[epoch 14][450/749] loss 5.1452 accuracy 0.00%\n[epoch 14][480/749] loss 5.0809 accuracy 0.00%\n[epoch 14][510/749] loss 4.6357 accuracy 0.00%\n[epoch 14][540/749] loss 4.7742 accuracy 0.00%\n[epoch 14][570/749] loss 5.0791 accuracy 0.00%\n[epoch 14][600/749] loss 4.8003 accuracy 0.00%\n[epoch 14][630/749] loss 5.1449 accuracy 0.00%\n[epoch 14][660/749] loss 4.9280 accuracy 0.00%\n[epoch 14][690/749] loss 4.9910 accuracy 0.00%\n[epoch 14][720/749] loss 5.6672 accuracy 0.00%\n----------------------------------------\n\n[epoch 14] accuracy on test data: 1.43%\n\n----------------------------------------\n\nepoch 15 learning rate 0.100000\n\n[epoch 15][0/749] loss 4.7891 accuracy 0.00%\n[epoch 15][30/749] loss 4.9870 accuracy 0.00%\n[epoch 15][60/749] loss 5.0397 accuracy 0.00%\n[epoch 15][90/749] loss 4.9110 accuracy 0.00%\n[epoch 15][120/749] loss 5.0625 accuracy 0.00%\n[epoch 15][150/749] loss 5.1924 accuracy 0.00%\n[epoch 15][180/749] loss 4.9761 accuracy 0.00%\n[epoch 15][210/749] loss 5.0809 accuracy 0.00%\n[epoch 15][240/749] loss 5.1535 accuracy 0.00%\n[epoch 15][270/749] loss 5.1820 accuracy 0.00%\n[epoch 15][300/749] loss 5.0465 accuracy 0.00%\n[epoch 15][330/749] loss 5.0217 accuracy 12.50%\n[epoch 15][360/749] loss 5.5858 accuracy 0.00%\n[epoch 15][390/749] loss 5.0565 accuracy 0.00%\n[epoch 15][420/749] loss 5.0010 accuracy 0.00%\n[epoch 15][450/749] loss 4.8752 accuracy 0.00%\n[epoch 15][480/749] loss 4.8172 accuracy 0.00%\n[epoch 15][510/749] loss 5.1449 accuracy 0.00%\n[epoch 15][540/749] loss 5.1012 accuracy 0.00%\n[epoch 15][570/749] loss 5.1386 accuracy 0.00%\n[epoch 15][600/749] loss 4.9217 accuracy 0.00%\n[epoch 15][630/749] loss 5.3146 accuracy 0.00%\n[epoch 15][660/749] loss 5.1223 accuracy 0.00%\n[epoch 15][690/749] loss 5.1998 accuracy 0.00%\n[epoch 15][720/749] loss 5.2449 accuracy 0.00%\n----------------------------------------\n\n[epoch 15] accuracy on test data: 1.36%\n\n----------------------------------------\n\nepoch 16 learning rate 0.100000\n\n[epoch 16][0/749] loss 5.1677 accuracy 0.00%\n[epoch 16][30/749] loss 5.2642 accuracy 0.00%\n[epoch 16][60/749] loss 4.7988 accuracy 0.00%\n[epoch 16][90/749] loss 4.8836 accuracy 0.00%\n[epoch 16][120/749] loss 5.4931 accuracy 0.00%\n[epoch 16][150/749] loss 5.2617 accuracy 0.00%\n[epoch 16][180/749] loss 5.5125 accuracy 0.00%\n[epoch 16][210/749] loss 5.1032 accuracy 0.00%\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-952352fc7a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nepoch %d learning rate %f\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# run for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\School\\Fall2020\\applied-deep-learning\\biweekly-report-5-skhadem\\attention\\learn-to-pay-attention\\cub2011.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# Targets start at 1 by default, so shift to 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attempt += 1\n",
    "step = 0\n",
    "log_freq = 30\n",
    "epochs = 100\n",
    "save_freq = 10\n",
    "writer = SummaryWriter('./runs/cub_pretrained_attempt_%s'%attempt)\n",
    "for epoch in range(epochs):\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    # run for one epoch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # warm up\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        pred, _, _, _ = model(inputs)\n",
    "\n",
    "        # backward\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display results\n",
    "        if i % log_freq == 0:\n",
    "            model.eval()\n",
    "            pred, __, __, __ = model(inputs)\n",
    "            predict = torch.argmax(pred, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = torch.eq(predict, labels).sum().double().item()\n",
    "            accuracy = correct / total\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "            writer.add_scalar('train/accuracy', accuracy, step)\n",
    "\n",
    "            print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
    "                % (epoch, i, len(train_loader)-1, loss.item(), (100*accuracy)))\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \n",
    "            \"./checkpoints/cub_attn-net_epoch_%s.pth\" % epoch\n",
    "        )\n",
    "\n",
    "    print('-'*40)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # log scalars\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images_test, labels_test = data\n",
    "            images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "            pred_test, _, _, _ = model(images_test)\n",
    "            predict = torch.argmax(pred_test, 1)\n",
    "            total += labels_test.size(0)\n",
    "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "        writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
    "    \n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}