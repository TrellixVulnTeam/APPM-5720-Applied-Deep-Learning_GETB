{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "deep-learning",
   "display_name": "deep-learning"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import ImageStat\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from attn_vgg import AttnVGG\n",
    "from cub2011 import Cub2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "train_set = Cub2011(root='../../data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "test_set = Cub2011(root='../../data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "  imagenet_stats = np.array([[0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]])\n",
    "  return img*imagenet_stats[1] + imagenet_stats[0]\n",
    "\n",
    "def show_image(img):\n",
    "  img = img.numpy().transpose(1,2,0)\n",
    "  img= denormalize(img)\n",
    "  plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-11-15T19:28:23.937455</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 250.618594 \r\nL 251.565 250.618594 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\nL 244.365 9.300469 \r\nL 26.925 9.300469 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p0957878b2c)\">\r\n    <image height=\"218\" id=\"image52f8848892\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-8.740469\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m32c434d16b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#m32c434d16b\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4cfef0b99c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4cfef0b99c\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 26.925 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 226.740469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 9.300469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0957878b2c\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "show_image(next(iter(train_set))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoints/cub_pretrained_attempt_5/cub_attn-net_epoch_50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_layer_name(filter_names, layer_name):\n",
    "    for name in filter_names:\n",
    "        if name in layer_name:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model = AttnVGG(im_size=im_size, num_classes=200, normalize_attn=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Freezing conv_block1.op.0.weight\nFreezing conv_block1.op.0.bias\nFreezing conv_block1.op.1.weight\nFreezing conv_block1.op.1.bias\nFreezing conv_block1.op.3.weight\nFreezing conv_block1.op.3.bias\nFreezing conv_block1.op.4.weight\nFreezing conv_block1.op.4.bias\nFreezing conv_block2.op.0.weight\nFreezing conv_block2.op.0.bias\nFreezing conv_block2.op.1.weight\nFreezing conv_block2.op.1.bias\nFreezing conv_block2.op.3.weight\nFreezing conv_block2.op.3.bias\nFreezing conv_block2.op.4.weight\nFreezing conv_block2.op.4.bias\nFreezing conv_block3.op.0.weight\nFreezing conv_block3.op.0.bias\nFreezing conv_block3.op.1.weight\nFreezing conv_block3.op.1.bias\nFreezing conv_block3.op.3.weight\nFreezing conv_block3.op.3.bias\nFreezing conv_block3.op.4.weight\nFreezing conv_block3.op.4.bias\nFreezing conv_block3.op.6.weight\nFreezing conv_block3.op.6.bias\nFreezing conv_block3.op.7.weight\nFreezing conv_block3.op.7.bias\nFreezing conv_block4.op.0.weight\nFreezing conv_block4.op.0.bias\nFreezing conv_block4.op.1.weight\nFreezing conv_block4.op.1.bias\nFreezing conv_block4.op.3.weight\nFreezing conv_block4.op.3.bias\nFreezing conv_block4.op.4.weight\nFreezing conv_block4.op.4.bias\nFreezing conv_block4.op.6.weight\nFreezing conv_block4.op.6.bias\nFreezing conv_block4.op.7.weight\nFreezing conv_block4.op.7.bias\nFreezing conv_block5.op.0.weight\nFreezing conv_block5.op.0.bias\nFreezing conv_block5.op.1.weight\nFreezing conv_block5.op.1.bias\nFreezing conv_block5.op.3.weight\nFreezing conv_block5.op.3.bias\nFreezing conv_block5.op.4.weight\nFreezing conv_block5.op.4.bias\nFreezing conv_block5.op.6.weight\nFreezing conv_block5.op.6.bias\nFreezing conv_block5.op.7.weight\nFreezing conv_block5.op.7.bias\nFreezing conv_block6.op.0.weight\nFreezing conv_block6.op.0.bias\nFreezing conv_block6.op.1.weight\nFreezing conv_block6.op.1.bias\nFreezing conv_block6.op.4.weight\nFreezing conv_block6.op.4.bias\nFreezing conv_block6.op.5.weight\nFreezing conv_block6.op.5.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if filter_layer_name(filter_names, name):\n",
    "        param.requires_grad = False\n",
    "        print('Freezing ' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "lr_lambda = lambda epoch : np.power(0.5, int(epoch/15))\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------\n",
      "\n",
      "[epoch 65] accuracy on test data: 37.90%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 66 learning rate 0.001000\n",
      "\n",
      "[epoch 66][0/374] loss 0.5821 accuracy 100.00%\n",
      "[epoch 66][30/374] loss 0.9571 accuracy 100.00%\n",
      "[epoch 66][60/374] loss 0.7204 accuracy 93.75%\n",
      "[epoch 66][90/374] loss 0.6090 accuracy 100.00%\n",
      "[epoch 66][120/374] loss 0.4492 accuracy 100.00%\n",
      "[epoch 66][150/374] loss 0.6569 accuracy 93.75%\n",
      "[epoch 66][180/374] loss 0.7677 accuracy 93.75%\n",
      "[epoch 66][210/374] loss 0.6266 accuracy 100.00%\n",
      "[epoch 66][240/374] loss 0.6666 accuracy 93.75%\n",
      "[epoch 66][270/374] loss 0.5953 accuracy 93.75%\n",
      "[epoch 66][300/374] loss 0.6456 accuracy 100.00%\n",
      "[epoch 66][330/374] loss 1.0347 accuracy 100.00%\n",
      "[epoch 66][360/374] loss 0.4430 accuracy 93.75%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 66] accuracy on test data: 38.61%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 67 learning rate 0.001000\n",
      "\n",
      "[epoch 67][0/374] loss 0.4916 accuracy 100.00%\n",
      "[epoch 67][30/374] loss 0.4092 accuracy 100.00%\n",
      "[epoch 67][60/374] loss 0.3692 accuracy 100.00%\n",
      "[epoch 67][90/374] loss 0.4404 accuracy 100.00%\n",
      "[epoch 67][120/374] loss 0.3858 accuracy 100.00%\n",
      "[epoch 67][150/374] loss 0.3349 accuracy 100.00%\n",
      "[epoch 67][180/374] loss 0.5656 accuracy 100.00%\n",
      "[epoch 67][210/374] loss 0.5600 accuracy 93.75%\n",
      "[epoch 67][240/374] loss 0.5293 accuracy 93.75%\n",
      "[epoch 67][270/374] loss 0.5146 accuracy 100.00%\n",
      "[epoch 67][300/374] loss 0.7386 accuracy 100.00%\n",
      "[epoch 67][330/374] loss 0.4510 accuracy 100.00%\n",
      "[epoch 67][360/374] loss 0.5002 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 67] accuracy on test data: 40.71%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 68 learning rate 0.001000\n",
      "\n",
      "[epoch 68][0/374] loss 0.3378 accuracy 100.00%\n",
      "[epoch 68][30/374] loss 0.3433 accuracy 100.00%\n",
      "[epoch 68][60/374] loss 0.3320 accuracy 100.00%\n",
      "[epoch 68][90/374] loss 0.6072 accuracy 100.00%\n",
      "[epoch 68][120/374] loss 0.2559 accuracy 100.00%\n",
      "[epoch 68][150/374] loss 0.2519 accuracy 100.00%\n",
      "[epoch 68][180/374] loss 0.2904 accuracy 100.00%\n",
      "[epoch 68][210/374] loss 0.4206 accuracy 100.00%\n",
      "[epoch 68][240/374] loss 0.2717 accuracy 100.00%\n",
      "[epoch 68][270/374] loss 0.3116 accuracy 100.00%\n",
      "[epoch 68][300/374] loss 0.2664 accuracy 100.00%\n",
      "[epoch 68][330/374] loss 0.2398 accuracy 100.00%\n",
      "[epoch 68][360/374] loss 0.4001 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 68] accuracy on test data: 40.06%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 69 learning rate 0.001000\n",
      "\n",
      "[epoch 69][0/374] loss 0.2889 accuracy 100.00%\n",
      "[epoch 69][30/374] loss 0.2611 accuracy 100.00%\n",
      "[epoch 69][60/374] loss 0.3882 accuracy 100.00%\n",
      "[epoch 69][90/374] loss 0.3355 accuracy 100.00%\n",
      "[epoch 69][120/374] loss 0.1934 accuracy 100.00%\n",
      "[epoch 69][150/374] loss 0.1425 accuracy 100.00%\n",
      "[epoch 69][180/374] loss 0.1976 accuracy 100.00%\n",
      "[epoch 69][210/374] loss 0.2950 accuracy 100.00%\n",
      "[epoch 69][240/374] loss 0.2683 accuracy 100.00%\n",
      "[epoch 69][270/374] loss 0.2084 accuracy 100.00%\n",
      "[epoch 69][300/374] loss 0.2220 accuracy 100.00%\n",
      "[epoch 69][330/374] loss 0.3484 accuracy 100.00%\n",
      "[epoch 69][360/374] loss 0.2713 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 69] accuracy on test data: 40.99%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 70 learning rate 0.000500\n",
      "\n",
      "[epoch 70][0/374] loss 0.1709 accuracy 100.00%\n",
      "[epoch 70][30/374] loss 0.1216 accuracy 100.00%\n",
      "[epoch 70][60/374] loss 0.1845 accuracy 100.00%\n",
      "[epoch 70][90/374] loss 0.1394 accuracy 100.00%\n",
      "[epoch 70][120/374] loss 0.1483 accuracy 100.00%\n",
      "[epoch 70][150/374] loss 0.1299 accuracy 100.00%\n",
      "[epoch 70][180/374] loss 0.1907 accuracy 100.00%\n",
      "[epoch 70][210/374] loss 0.1295 accuracy 100.00%\n",
      "[epoch 70][240/374] loss 0.1379 accuracy 100.00%\n",
      "[epoch 70][270/374] loss 0.1353 accuracy 100.00%\n",
      "[epoch 70][300/374] loss 0.1384 accuracy 100.00%\n",
      "[epoch 70][330/374] loss 0.2626 accuracy 100.00%\n",
      "[epoch 70][360/374] loss 0.3342 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 70] accuracy on test data: 42.82%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 71 learning rate 0.000500\n",
      "\n",
      "[epoch 71][0/374] loss 0.1439 accuracy 100.00%\n",
      "[epoch 71][30/374] loss 0.1477 accuracy 100.00%\n",
      "[epoch 71][60/374] loss 0.1409 accuracy 100.00%\n",
      "[epoch 71][90/374] loss 0.1057 accuracy 100.00%\n",
      "[epoch 71][120/374] loss 0.1438 accuracy 100.00%\n",
      "[epoch 71][150/374] loss 0.1193 accuracy 100.00%\n",
      "[epoch 71][180/374] loss 0.1650 accuracy 100.00%\n",
      "[epoch 71][210/374] loss 0.1589 accuracy 100.00%\n",
      "[epoch 71][240/374] loss 0.1375 accuracy 100.00%\n",
      "[epoch 71][270/374] loss 0.1475 accuracy 100.00%\n",
      "[epoch 71][300/374] loss 0.0993 accuracy 100.00%\n",
      "[epoch 71][330/374] loss 0.1337 accuracy 100.00%\n",
      "[epoch 71][360/374] loss 0.1336 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 71] accuracy on test data: 42.30%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 72 learning rate 0.000500\n",
      "\n",
      "[epoch 72][0/374] loss 0.0848 accuracy 100.00%\n",
      "[epoch 72][30/374] loss 0.1231 accuracy 100.00%\n",
      "[epoch 72][60/374] loss 0.1373 accuracy 100.00%\n",
      "[epoch 72][90/374] loss 0.2448 accuracy 100.00%\n",
      "[epoch 72][120/374] loss 0.0921 accuracy 100.00%\n",
      "[epoch 72][150/374] loss 0.1222 accuracy 100.00%\n",
      "[epoch 72][180/374] loss 0.0743 accuracy 100.00%\n",
      "[epoch 72][210/374] loss 0.1088 accuracy 100.00%\n",
      "[epoch 72][240/374] loss 0.1501 accuracy 100.00%\n",
      "[epoch 72][270/374] loss 0.1343 accuracy 100.00%\n",
      "[epoch 72][300/374] loss 0.1133 accuracy 100.00%\n",
      "[epoch 72][330/374] loss 0.1001 accuracy 100.00%\n",
      "[epoch 72][360/374] loss 0.1030 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 72] accuracy on test data: 43.20%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 73 learning rate 0.000500\n",
      "\n",
      "[epoch 73][0/374] loss 0.0734 accuracy 100.00%\n",
      "[epoch 73][30/374] loss 0.0948 accuracy 100.00%\n",
      "[epoch 73][60/374] loss 0.0805 accuracy 100.00%\n",
      "[epoch 73][90/374] loss 0.0971 accuracy 100.00%\n",
      "[epoch 73][120/374] loss 0.1243 accuracy 100.00%\n",
      "[epoch 73][150/374] loss 0.1095 accuracy 100.00%\n",
      "[epoch 73][180/374] loss 0.1035 accuracy 100.00%\n",
      "[epoch 73][210/374] loss 0.0909 accuracy 100.00%\n",
      "[epoch 73][240/374] loss 0.1135 accuracy 100.00%\n",
      "[epoch 73][270/374] loss 0.1406 accuracy 100.00%\n",
      "[epoch 73][300/374] loss 0.0961 accuracy 100.00%\n",
      "[epoch 73][330/374] loss 0.1006 accuracy 100.00%\n",
      "[epoch 73][360/374] loss 0.1303 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 73] accuracy on test data: 43.11%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 74 learning rate 0.000500\n",
      "\n",
      "[epoch 74][0/374] loss 0.1136 accuracy 100.00%\n",
      "[epoch 74][30/374] loss 0.0875 accuracy 100.00%\n",
      "[epoch 74][60/374] loss 0.0867 accuracy 100.00%\n",
      "[epoch 74][90/374] loss 0.0953 accuracy 100.00%\n",
      "[epoch 74][120/374] loss 0.0839 accuracy 100.00%\n",
      "[epoch 74][150/374] loss 0.0922 accuracy 100.00%\n",
      "[epoch 74][180/374] loss 0.1024 accuracy 100.00%\n",
      "[epoch 74][210/374] loss 0.1184 accuracy 100.00%\n",
      "[epoch 74][240/374] loss 0.0820 accuracy 100.00%\n",
      "[epoch 74][270/374] loss 0.0845 accuracy 100.00%\n",
      "[epoch 74][300/374] loss 0.0692 accuracy 100.00%\n",
      "[epoch 74][330/374] loss 0.1039 accuracy 100.00%\n",
      "[epoch 74][360/374] loss 0.0961 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 74] accuracy on test data: 43.44%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 75 learning rate 0.000500\n",
      "\n",
      "[epoch 75][0/374] loss 0.1165 accuracy 100.00%\n",
      "[epoch 75][30/374] loss 0.0561 accuracy 100.00%\n",
      "[epoch 75][60/374] loss 0.0506 accuracy 100.00%\n",
      "[epoch 75][90/374] loss 0.0953 accuracy 100.00%\n",
      "[epoch 75][120/374] loss 0.1023 accuracy 100.00%\n",
      "[epoch 75][150/374] loss 0.1027 accuracy 100.00%\n",
      "[epoch 75][180/374] loss 0.1160 accuracy 100.00%\n",
      "[epoch 75][210/374] loss 0.1092 accuracy 100.00%\n",
      "[epoch 75][240/374] loss 0.0775 accuracy 100.00%\n",
      "[epoch 75][270/374] loss 0.0839 accuracy 100.00%\n",
      "[epoch 75][300/374] loss 0.0704 accuracy 100.00%\n",
      "[epoch 75][330/374] loss 0.1224 accuracy 100.00%\n",
      "[epoch 75][360/374] loss 0.0770 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 75] accuracy on test data: 43.48%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 76 learning rate 0.000500\n",
      "\n",
      "[epoch 76][0/374] loss 0.1065 accuracy 100.00%\n",
      "[epoch 76][30/374] loss 0.1022 accuracy 100.00%\n",
      "[epoch 76][60/374] loss 0.0477 accuracy 100.00%\n",
      "[epoch 76][90/374] loss 0.0700 accuracy 100.00%\n",
      "[epoch 76][120/374] loss 0.0998 accuracy 100.00%\n",
      "[epoch 76][150/374] loss 0.0719 accuracy 100.00%\n",
      "[epoch 76][180/374] loss 0.0753 accuracy 100.00%\n",
      "[epoch 76][210/374] loss 0.0654 accuracy 100.00%\n",
      "[epoch 76][240/374] loss 0.0668 accuracy 100.00%\n",
      "[epoch 76][270/374] loss 0.0583 accuracy 100.00%\n",
      "[epoch 76][300/374] loss 0.0907 accuracy 100.00%\n",
      "[epoch 76][330/374] loss 0.0801 accuracy 100.00%\n",
      "[epoch 76][360/374] loss 0.1074 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 76] accuracy on test data: 42.91%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 77 learning rate 0.000500\n",
      "\n",
      "[epoch 77][0/374] loss 0.0781 accuracy 100.00%\n",
      "[epoch 77][30/374] loss 0.0905 accuracy 100.00%\n",
      "[epoch 77][60/374] loss 0.1300 accuracy 100.00%\n",
      "[epoch 77][90/374] loss 0.1154 accuracy 100.00%\n",
      "[epoch 77][120/374] loss 0.0734 accuracy 100.00%\n",
      "[epoch 77][150/374] loss 0.0788 accuracy 100.00%\n",
      "[epoch 77][180/374] loss 0.0469 accuracy 100.00%\n",
      "[epoch 77][210/374] loss 0.0519 accuracy 100.00%\n",
      "[epoch 77][240/374] loss 0.0530 accuracy 100.00%\n",
      "[epoch 77][270/374] loss 0.0888 accuracy 100.00%\n",
      "[epoch 77][300/374] loss 0.0635 accuracy 100.00%\n",
      "[epoch 77][330/374] loss 0.1173 accuracy 100.00%\n",
      "[epoch 77][360/374] loss 0.0534 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 77] accuracy on test data: 43.15%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 78 learning rate 0.000500\n",
      "\n",
      "[epoch 78][0/374] loss 0.0501 accuracy 100.00%\n",
      "[epoch 78][30/374] loss 0.1700 accuracy 100.00%\n",
      "[epoch 78][60/374] loss 0.0747 accuracy 100.00%\n",
      "[epoch 78][90/374] loss 0.0707 accuracy 100.00%\n",
      "[epoch 78][120/374] loss 0.0555 accuracy 100.00%\n",
      "[epoch 78][150/374] loss 0.1180 accuracy 100.00%\n",
      "[epoch 78][180/374] loss 0.1085 accuracy 100.00%\n",
      "[epoch 78][210/374] loss 0.0782 accuracy 100.00%\n",
      "[epoch 78][240/374] loss 0.0609 accuracy 100.00%\n",
      "[epoch 78][270/374] loss 0.0612 accuracy 100.00%\n",
      "[epoch 78][300/374] loss 0.0733 accuracy 100.00%\n",
      "[epoch 78][330/374] loss 0.0774 accuracy 100.00%\n",
      "[epoch 78][360/374] loss 0.0551 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 78] accuracy on test data: 43.10%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 79 learning rate 0.000500\n",
      "\n",
      "[epoch 79][0/374] loss 0.0680 accuracy 100.00%\n",
      "[epoch 79][30/374] loss 0.0534 accuracy 100.00%\n",
      "[epoch 79][60/374] loss 0.0780 accuracy 100.00%\n",
      "[epoch 79][90/374] loss 0.0472 accuracy 100.00%\n",
      "[epoch 79][120/374] loss 0.0490 accuracy 100.00%\n",
      "[epoch 79][150/374] loss 0.1082 accuracy 100.00%\n",
      "[epoch 79][180/374] loss 0.0721 accuracy 100.00%\n",
      "[epoch 79][210/374] loss 0.0658 accuracy 100.00%\n",
      "[epoch 79][240/374] loss 0.0804 accuracy 100.00%\n",
      "[epoch 79][270/374] loss 0.0586 accuracy 100.00%\n",
      "[epoch 79][300/374] loss 0.1149 accuracy 100.00%\n",
      "[epoch 79][330/374] loss 0.0705 accuracy 100.00%\n",
      "[epoch 79][360/374] loss 0.0872 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 79] accuracy on test data: 42.94%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 80 learning rate 0.000500\n",
      "\n",
      "[epoch 80][0/374] loss 0.0525 accuracy 100.00%\n",
      "[epoch 80][30/374] loss 0.0496 accuracy 100.00%\n",
      "[epoch 80][60/374] loss 0.0819 accuracy 100.00%\n",
      "[epoch 80][90/374] loss 0.0578 accuracy 100.00%\n",
      "[epoch 80][120/374] loss 0.1033 accuracy 100.00%\n",
      "[epoch 80][150/374] loss 0.0598 accuracy 100.00%\n",
      "[epoch 80][180/374] loss 0.0759 accuracy 100.00%\n",
      "[epoch 80][210/374] loss 0.0582 accuracy 100.00%\n",
      "[epoch 80][240/374] loss 0.0786 accuracy 100.00%\n",
      "[epoch 80][270/374] loss 0.0590 accuracy 100.00%\n",
      "[epoch 80][300/374] loss 0.0991 accuracy 100.00%\n",
      "[epoch 80][330/374] loss 0.0521 accuracy 100.00%\n",
      "[epoch 80][360/374] loss 0.0598 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 80] accuracy on test data: 42.49%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 81 learning rate 0.000500\n",
      "\n",
      "[epoch 81][0/374] loss 0.0515 accuracy 100.00%\n",
      "[epoch 81][30/374] loss 0.0674 accuracy 100.00%\n",
      "[epoch 81][60/374] loss 0.0544 accuracy 100.00%\n",
      "[epoch 81][90/374] loss 0.0472 accuracy 100.00%\n",
      "[epoch 81][120/374] loss 0.0565 accuracy 100.00%\n",
      "[epoch 81][150/374] loss 0.0592 accuracy 100.00%\n",
      "[epoch 81][180/374] loss 0.0670 accuracy 100.00%\n",
      "[epoch 81][210/374] loss 0.0574 accuracy 100.00%\n",
      "[epoch 81][240/374] loss 0.0534 accuracy 100.00%\n",
      "[epoch 81][270/374] loss 0.0554 accuracy 100.00%\n",
      "[epoch 81][300/374] loss 0.0552 accuracy 100.00%\n",
      "[epoch 81][330/374] loss 0.0534 accuracy 100.00%\n",
      "[epoch 81][360/374] loss 0.0637 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 81] accuracy on test data: 42.92%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 82 learning rate 0.000500\n",
      "\n",
      "[epoch 82][0/374] loss 0.0842 accuracy 100.00%\n",
      "[epoch 82][30/374] loss 0.0510 accuracy 100.00%\n",
      "[epoch 82][60/374] loss 0.0673 accuracy 100.00%\n",
      "[epoch 82][90/374] loss 0.0890 accuracy 100.00%\n",
      "[epoch 82][120/374] loss 0.0836 accuracy 100.00%\n",
      "[epoch 82][150/374] loss 0.0465 accuracy 100.00%\n",
      "[epoch 82][180/374] loss 0.0727 accuracy 100.00%\n",
      "[epoch 82][210/374] loss 0.0696 accuracy 100.00%\n",
      "[epoch 82][240/374] loss 0.0978 accuracy 100.00%\n",
      "[epoch 82][270/374] loss 0.0743 accuracy 100.00%\n",
      "[epoch 82][300/374] loss 0.0592 accuracy 100.00%\n",
      "[epoch 82][330/374] loss 0.0888 accuracy 100.00%\n",
      "[epoch 82][360/374] loss 0.0653 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 82] accuracy on test data: 43.23%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 83 learning rate 0.000500\n",
      "\n",
      "[epoch 83][0/374] loss 0.0777 accuracy 100.00%\n",
      "[epoch 83][30/374] loss 0.0497 accuracy 100.00%\n",
      "[epoch 83][60/374] loss 0.0451 accuracy 100.00%\n",
      "[epoch 83][90/374] loss 0.0398 accuracy 100.00%\n",
      "[epoch 83][120/374] loss 0.0429 accuracy 100.00%\n",
      "[epoch 83][150/374] loss 0.0419 accuracy 100.00%\n",
      "[epoch 83][180/374] loss 0.0598 accuracy 100.00%\n",
      "[epoch 83][210/374] loss 0.0555 accuracy 100.00%\n",
      "[epoch 83][240/374] loss 0.0417 accuracy 100.00%\n",
      "[epoch 83][270/374] loss 0.0556 accuracy 100.00%\n",
      "[epoch 83][300/374] loss 0.0483 accuracy 100.00%\n",
      "[epoch 83][330/374] loss 0.0491 accuracy 100.00%\n",
      "[epoch 83][360/374] loss 0.0588 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 83] accuracy on test data: 42.65%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 84 learning rate 0.000500\n",
      "\n",
      "[epoch 84][0/374] loss 0.0477 accuracy 100.00%\n",
      "[epoch 84][30/374] loss 0.0743 accuracy 100.00%\n",
      "[epoch 84][60/374] loss 0.0356 accuracy 100.00%\n",
      "[epoch 84][90/374] loss 0.0508 accuracy 100.00%\n",
      "[epoch 84][120/374] loss 0.0555 accuracy 100.00%\n",
      "[epoch 84][150/374] loss 0.0700 accuracy 100.00%\n",
      "[epoch 84][180/374] loss 0.0459 accuracy 100.00%\n",
      "[epoch 84][210/374] loss 0.0368 accuracy 100.00%\n",
      "[epoch 84][240/374] loss 0.0406 accuracy 100.00%\n",
      "[epoch 84][270/374] loss 0.0456 accuracy 100.00%\n",
      "[epoch 84][300/374] loss 0.0441 accuracy 100.00%\n",
      "[epoch 84][330/374] loss 0.0662 accuracy 100.00%\n",
      "[epoch 84][360/374] loss 0.0474 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 84] accuracy on test data: 43.29%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 85 learning rate 0.000250\n",
      "\n",
      "[epoch 85][0/374] loss 0.0521 accuracy 100.00%\n",
      "[epoch 85][30/374] loss 0.0480 accuracy 100.00%\n",
      "[epoch 85][60/374] loss 0.0467 accuracy 100.00%\n",
      "[epoch 85][90/374] loss 0.0512 accuracy 100.00%\n",
      "[epoch 85][120/374] loss 0.0424 accuracy 100.00%\n",
      "[epoch 85][150/374] loss 0.0612 accuracy 100.00%\n",
      "[epoch 85][180/374] loss 0.0249 accuracy 100.00%\n",
      "[epoch 85][210/374] loss 0.0424 accuracy 100.00%\n",
      "[epoch 85][240/374] loss 0.0518 accuracy 100.00%\n",
      "[epoch 85][270/374] loss 0.0443 accuracy 100.00%\n",
      "[epoch 85][300/374] loss 0.0536 accuracy 100.00%\n",
      "[epoch 85][330/374] loss 0.0349 accuracy 100.00%\n",
      "[epoch 85][360/374] loss 0.0594 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 85] accuracy on test data: 43.20%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 86 learning rate 0.000250\n",
      "\n",
      "[epoch 86][0/374] loss 0.0452 accuracy 100.00%\n",
      "[epoch 86][30/374] loss 0.0675 accuracy 100.00%\n",
      "[epoch 86][60/374] loss 0.0395 accuracy 100.00%\n",
      "[epoch 86][90/374] loss 0.0372 accuracy 100.00%\n",
      "[epoch 86][120/374] loss 0.0506 accuracy 100.00%\n",
      "[epoch 86][150/374] loss 0.0364 accuracy 100.00%\n",
      "[epoch 86][180/374] loss 0.0451 accuracy 100.00%\n",
      "[epoch 86][210/374] loss 0.0316 accuracy 100.00%\n",
      "[epoch 86][240/374] loss 0.0445 accuracy 100.00%\n",
      "[epoch 86][270/374] loss 0.0486 accuracy 100.00%\n",
      "[epoch 86][300/374] loss 0.0663 accuracy 100.00%\n",
      "[epoch 86][330/374] loss 0.0457 accuracy 100.00%\n",
      "[epoch 86][360/374] loss 0.0477 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 86] accuracy on test data: 43.53%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 87 learning rate 0.000250\n",
      "\n",
      "[epoch 87][0/374] loss 0.0371 accuracy 100.00%\n",
      "[epoch 87][30/374] loss 0.0504 accuracy 100.00%\n",
      "[epoch 87][60/374] loss 0.0396 accuracy 100.00%\n",
      "[epoch 87][90/374] loss 0.0521 accuracy 100.00%\n",
      "[epoch 87][120/374] loss 0.0405 accuracy 100.00%\n",
      "[epoch 87][150/374] loss 0.0558 accuracy 100.00%\n",
      "[epoch 87][180/374] loss 0.0459 accuracy 100.00%\n",
      "[epoch 87][210/374] loss 0.0358 accuracy 100.00%\n",
      "[epoch 87][240/374] loss 0.0745 accuracy 100.00%\n",
      "[epoch 87][270/374] loss 0.0534 accuracy 100.00%\n",
      "[epoch 87][300/374] loss 0.0583 accuracy 100.00%\n",
      "[epoch 87][330/374] loss 0.0442 accuracy 100.00%\n",
      "[epoch 87][360/374] loss 0.0723 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 87] accuracy on test data: 43.13%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 88 learning rate 0.000250\n",
      "\n",
      "[epoch 88][0/374] loss 0.0392 accuracy 100.00%\n",
      "[epoch 88][30/374] loss 0.0460 accuracy 100.00%\n",
      "[epoch 88][60/374] loss 0.0305 accuracy 100.00%\n",
      "[epoch 88][90/374] loss 0.0722 accuracy 100.00%\n",
      "[epoch 88][120/374] loss 0.0437 accuracy 100.00%\n",
      "[epoch 88][150/374] loss 0.0601 accuracy 100.00%\n",
      "[epoch 88][180/374] loss 0.0371 accuracy 100.00%\n",
      "[epoch 88][210/374] loss 0.0664 accuracy 100.00%\n",
      "[epoch 88][240/374] loss 0.0254 accuracy 100.00%\n",
      "[epoch 88][270/374] loss 0.0627 accuracy 100.00%\n",
      "[epoch 88][300/374] loss 0.0377 accuracy 100.00%\n",
      "[epoch 88][330/374] loss 0.0364 accuracy 100.00%\n",
      "[epoch 88][360/374] loss 0.0549 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 88] accuracy on test data: 43.08%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 89 learning rate 0.000250\n",
      "\n",
      "[epoch 89][0/374] loss 0.0666 accuracy 100.00%\n",
      "[epoch 89][30/374] loss 0.0582 accuracy 100.00%\n",
      "[epoch 89][60/374] loss 0.0655 accuracy 100.00%\n",
      "[epoch 89][90/374] loss 0.0291 accuracy 100.00%\n",
      "[epoch 89][120/374] loss 0.0509 accuracy 100.00%\n",
      "[epoch 89][150/374] loss 0.0416 accuracy 100.00%\n",
      "[epoch 89][180/374] loss 0.0309 accuracy 100.00%\n",
      "[epoch 89][210/374] loss 0.0366 accuracy 100.00%\n",
      "[epoch 89][240/374] loss 0.0590 accuracy 100.00%\n",
      "[epoch 89][270/374] loss 0.0320 accuracy 100.00%\n",
      "[epoch 89][300/374] loss 0.0536 accuracy 100.00%\n",
      "[epoch 89][330/374] loss 0.0446 accuracy 100.00%\n",
      "[epoch 89][360/374] loss 0.0539 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 89] accuracy on test data: 43.39%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 90 learning rate 0.000250\n",
      "\n",
      "[epoch 90][0/374] loss 0.0306 accuracy 100.00%\n",
      "[epoch 90][30/374] loss 0.0337 accuracy 100.00%\n",
      "[epoch 90][60/374] loss 0.0517 accuracy 100.00%\n",
      "[epoch 90][90/374] loss 0.0550 accuracy 100.00%\n",
      "[epoch 90][120/374] loss 0.0435 accuracy 100.00%\n",
      "[epoch 90][150/374] loss 0.0294 accuracy 100.00%\n",
      "[epoch 90][180/374] loss 0.0289 accuracy 100.00%\n",
      "[epoch 90][210/374] loss 0.0363 accuracy 100.00%\n",
      "[epoch 90][240/374] loss 0.0478 accuracy 100.00%\n",
      "[epoch 90][270/374] loss 0.0411 accuracy 100.00%\n",
      "[epoch 90][300/374] loss 0.0526 accuracy 100.00%\n",
      "[epoch 90][330/374] loss 0.0317 accuracy 100.00%\n",
      "[epoch 90][360/374] loss 0.0451 accuracy 100.00%\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 90] accuracy on test data: 43.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 91 learning rate 0.000250\n",
      "\n",
      "[epoch 91][0/374] loss 0.0424 accuracy 100.00%\n",
      "[epoch 91][30/374] loss 0.0914 accuracy 100.00%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6dfa1f4511c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nepoch %d learning rate %f\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# run for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\School\\Fall2020\\applied-deep-learning\\biweekly-report-6-skhadem\\attention\\cub2011.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1927\u001b[0m                 )\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss = checkpoint['loss']\n",
    "# last_epoch = checkpoint['epoch']\n",
    "last_epoch = 55\n",
    "step = 21_000\n",
    "log_freq = 30\n",
    "epochs = 100\n",
    "save_freq = 10\n",
    "run_name = 'cub_pretrained_attempt_%s'%attempt\n",
    "checkpoints_folder = './checkpoints/%s/'%run_name\n",
    "if not os.path.isdir(checkpoints_folder):\n",
    "    os.mkdir(checkpoints_folder)\n",
    "writer = SummaryWriter('./runs/' + run_name)\n",
    "for epoch in range(last_epoch, last_epoch + epochs + 1):\n",
    "    # adjust learning rate\n",
    "    writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    # run for one epoch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # warm up\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        pred, _, _, _ = model(inputs)\n",
    "\n",
    "        # backward\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display results\n",
    "        if i % log_freq == 0:\n",
    "            model.eval()\n",
    "            pred, __, __, __ = model(inputs)\n",
    "            predict = torch.argmax(pred, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = torch.eq(predict, labels).sum().double().item()\n",
    "            accuracy = correct / total\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "            writer.add_scalar('train/accuracy', accuracy, step)\n",
    "\n",
    "            print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
    "                % (epoch, i, len(train_loader)-1, loss.item(), (100*accuracy)))\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \n",
    "            '%s/cub_attn-net_epoch_%s.pth' % (checkpoints_folder, epoch)\n",
    "        )\n",
    "\n",
    "    print('-'*40)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # log scalars\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images_test, labels_test = data\n",
    "            images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "            pred_test, _, _, _ = model(images_test)\n",
    "            predict = torch.argmax(pred_test, 1)\n",
    "            total += labels_test.size(0)\n",
    "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "        writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
    "    \n",
    "    print('-'*40)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}