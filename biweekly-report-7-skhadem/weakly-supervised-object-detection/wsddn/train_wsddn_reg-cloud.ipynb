{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.21.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (3.12.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.31.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.35.1)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (49.6.0.post20200814)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.18.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.0.1)\n",
      "Installing collected packages: tensorboard-plugin-wit, absl-py, tensorboard\n",
      "Successfully installed absl-py-0.11.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/bes-dev/mean_average_precision.git\n",
      "  Cloning https://github.com/bes-dev/mean_average_precision.git to /tmp/pip-req-build-fab5gfy9\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from mean-average-precision==0.0.2.1) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/lib/python3.7/site-packages (from mean-average-precision==0.0.2.1) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->mean-average-precision==0.0.2.1) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->mean-average-precision==0.0.2.1) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->mean-average-precision==0.0.2.1) (1.15.0)\n",
      "Building wheels for collected packages: mean-average-precision\n",
      "  Building wheel for mean-average-precision (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mean-average-precision: filename=mean_average_precision-0.0.2.1-py3-none-any.whl size=14137 sha256=f9bfbf87b8f7d2faf4c68e4e235a4cf32ad8489173c284f66ffec4629f95ee42\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gf39v54a/wheels/32/c7/c8/fbe474b16ebb6737f48a026fd3cf0048af9ffbdb0e9be48ea5\n",
      "Successfully built mean-average-precision\n",
      "Installing collected packages: mean-average-precision\n",
      "Successfully installed mean-average-precision-0.0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/bes-dev/mean_average_precision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mean_average_precision import MetricBuilder\n",
    "\n",
    "\n",
    "from wsddn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augmentation = BoxAndImageFlip(p_horiz=0.5, p_vert=0.0)\n",
    "\n",
    "train_set = CustomVOC('../edge_boxes_model/model.yml.gz', root='../../../data', image_set='train', download=False, post_transform=transform_train, augmentation=augmentation)\n",
    "\n",
    "test_set = CustomVOC('../edge_boxes_model/model.yml.gz', eval_mode=True, root='../../../data', image_set='val', download=False, transform=transform_test)\n",
    "\n",
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    label = [item[1] for item in batch]\n",
    "    boxes = [item[2] for item in batch]\n",
    "    if (len(batch[0]) == 4):\n",
    "        gt_boxes = [item[3] for item in batch]\n",
    "        return data, label, boxes, gt_boxes\n",
    "    return data, label, boxes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0, collate_fn=my_collate)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WSDDN(21)\n",
    "model = model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15, 20], gamma=0.25)\n",
    "lam = 0.25 # spatial Regularizer weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0 learning rate 0.000010\n",
      "\n",
      "[epoch 0][0/5716] loss 4.1411\n",
      "[epoch 0][60/5716] loss 12.5677\n",
      "[epoch 0][120/5716] loss 6.8948\n",
      "[epoch 0][180/5716] loss 5.8703\n",
      "[epoch 0][240/5716] loss 7.5008\n",
      "[epoch 0][300/5716] loss 3.6120\n",
      "[epoch 0][360/5716] loss 3.4687\n",
      "[epoch 0][420/5716] loss 6.7953\n",
      "[epoch 0][480/5716] loss 4.2732\n",
      "[epoch 0][540/5716] loss 12.2147\n",
      "[epoch 0][600/5716] loss 3.2100\n",
      "[epoch 0][660/5716] loss 4.7471\n",
      "[epoch 0][720/5716] loss 8.0117\n",
      "[epoch 0][780/5716] loss 2.1110\n",
      "[epoch 0][840/5716] loss 3.6560\n",
      "[epoch 0][900/5716] loss 1.0991\n",
      "[epoch 0][960/5716] loss 3.2254\n",
      "[epoch 0][1020/5716] loss 6.9385\n",
      "[epoch 0][1080/5716] loss 4.8526\n",
      "[epoch 0][1140/5716] loss 1.5423\n",
      "[epoch 0][1200/5716] loss 3.8010\n",
      "[epoch 0][1260/5716] loss 4.0282\n",
      "[epoch 0][1320/5716] loss 2.0105\n",
      "[epoch 0][1380/5716] loss 3.2916\n",
      "[epoch 0][1440/5716] loss 2.7444\n",
      "[epoch 0][1500/5716] loss 4.9826\n",
      "[epoch 0][1560/5716] loss 7.9863\n",
      "[epoch 0][1620/5716] loss 1.9654\n",
      "[epoch 0][1680/5716] loss 2.1779\n",
      "[epoch 0][1740/5716] loss 4.9188\n",
      "[epoch 0][1800/5716] loss 2.6263\n",
      "[epoch 0][1860/5716] loss 4.3052\n",
      "[epoch 0][1920/5716] loss 5.3455\n",
      "[epoch 0][1980/5716] loss 4.3905\n",
      "[epoch 0][2040/5716] loss 1.6390\n",
      "[epoch 0][2100/5716] loss 5.5040\n",
      "[epoch 0][2160/5716] loss 7.4488\n",
      "[epoch 0][2220/5716] loss 4.0999\n",
      "[epoch 0][2280/5716] loss 1.2891\n",
      "[epoch 0][2340/5716] loss 4.8291\n",
      "[epoch 0][2400/5716] loss 5.9762\n",
      "[epoch 0][2460/5716] loss 2.5251\n",
      "[epoch 0][2520/5716] loss 1.7798\n",
      "[epoch 0][2580/5716] loss 2.0234\n",
      "[epoch 0][2640/5716] loss 2.7009\n",
      "[epoch 0][2700/5716] loss 3.7380\n",
      "[epoch 0][2760/5716] loss 1.6142\n",
      "[epoch 0][2820/5716] loss 7.0866\n",
      "[epoch 0][2880/5716] loss 0.5558\n",
      "[epoch 0][2940/5716] loss 3.7232\n",
      "[epoch 0][3000/5716] loss 4.0102\n",
      "[epoch 0][3060/5716] loss 4.5471\n",
      "[epoch 0][3120/5716] loss 4.5577\n",
      "[epoch 0][3180/5716] loss 2.2303\n",
      "[epoch 0][3240/5716] loss 2.4713\n",
      "[epoch 0][3300/5716] loss 2.1301\n",
      "[epoch 0][3360/5716] loss 1.5340\n",
      "[epoch 0][3420/5716] loss 1.6211\n",
      "[epoch 0][3480/5716] loss 1.2799\n",
      "[epoch 0][3540/5716] loss 3.4265\n",
      "[epoch 0][3600/5716] loss 4.2116\n",
      "[epoch 0][3660/5716] loss 3.1696\n",
      "[epoch 0][3720/5716] loss 7.9074\n",
      "[epoch 0][3780/5716] loss 5.0780\n",
      "[epoch 0][3840/5716] loss 1.4237\n",
      "[epoch 0][3900/5716] loss 3.7866\n",
      "[epoch 0][3960/5716] loss 1.3297\n",
      "[epoch 0][4020/5716] loss 1.4124\n",
      "[epoch 0][4080/5716] loss 3.2986\n",
      "[epoch 0][4140/5716] loss 7.2158\n",
      "[epoch 0][4200/5716] loss 0.9080\n",
      "[epoch 0][4260/5716] loss 4.6487\n",
      "[epoch 0][4320/5716] loss 3.2940\n",
      "[epoch 0][4380/5716] loss 1.9019\n",
      "[epoch 0][4440/5716] loss 3.0885\n",
      "[epoch 0][4500/5716] loss 3.3683\n",
      "[epoch 0][4560/5716] loss 3.7232\n",
      "[epoch 0][4620/5716] loss 1.2267\n",
      "[epoch 0][4680/5716] loss 4.2896\n",
      "[epoch 0][4740/5716] loss 3.1542\n",
      "[epoch 0][4800/5716] loss 6.7179\n",
      "[epoch 0][4860/5716] loss 2.6249\n",
      "[epoch 0][4920/5716] loss 4.2767\n",
      "[epoch 0][4980/5716] loss 1.4067\n",
      "[epoch 0][5040/5716] loss 4.0431\n",
      "[epoch 0][5100/5716] loss 3.4842\n",
      "[epoch 0][5160/5716] loss 4.3525\n",
      "[epoch 0][5220/5716] loss 3.0422\n",
      "[epoch 0][5280/5716] loss 0.5850\n",
      "[epoch 0][5340/5716] loss 7.2992\n",
      "[epoch 0][5400/5716] loss 2.9416\n",
      "[epoch 0][5460/5716] loss 2.2553\n",
      "[epoch 0][5520/5716] loss 1.6937\n",
      "[epoch 0][5580/5716] loss 3.5909\n",
      "[epoch 0][5640/5716] loss 3.0256\n",
      "[epoch 0][5700/5716] loss 6.5291\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 0] mAP on test data: 0.22%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 1 learning rate 0.000010\n",
      "\n",
      "[epoch 1][0/5716] loss 1.3734\n",
      "[epoch 1][60/5716] loss 1.3089\n",
      "[epoch 1][120/5716] loss 4.0036\n",
      "[epoch 1][180/5716] loss 7.6366\n",
      "[epoch 1][240/5716] loss 2.7225\n",
      "[epoch 1][300/5716] loss 1.1120\n",
      "[epoch 1][360/5716] loss 4.0355\n",
      "[epoch 1][420/5716] loss 2.0640\n",
      "[epoch 1][480/5716] loss 1.8468\n",
      "[epoch 1][540/5716] loss 0.4611\n",
      "[epoch 1][600/5716] loss 3.0722\n",
      "[epoch 1][660/5716] loss 3.1622\n",
      "[epoch 1][720/5716] loss 1.6551\n",
      "[epoch 1][780/5716] loss 3.9633\n",
      "[epoch 1][840/5716] loss 0.5962\n",
      "[epoch 1][900/5716] loss 2.7100\n",
      "[epoch 1][960/5716] loss 2.4035\n",
      "[epoch 1][1020/5716] loss 0.3344\n",
      "[epoch 1][1080/5716] loss 1.3330\n",
      "[epoch 1][1140/5716] loss 1.8256\n",
      "[epoch 1][1200/5716] loss 0.9239\n",
      "[epoch 1][1260/5716] loss 1.0076\n",
      "[epoch 1][1320/5716] loss 3.2589\n",
      "[epoch 1][1380/5716] loss 3.8726\n",
      "[epoch 1][1440/5716] loss 6.5466\n",
      "[epoch 1][1500/5716] loss 3.4023\n",
      "[epoch 1][1560/5716] loss 3.5696\n",
      "[epoch 1][1620/5716] loss 2.0502\n",
      "[epoch 1][1680/5716] loss 2.5441\n",
      "[epoch 1][1740/5716] loss 0.5275\n",
      "[epoch 1][1800/5716] loss 1.7128\n",
      "[epoch 1][1860/5716] loss 4.1863\n",
      "[epoch 1][1920/5716] loss 3.1537\n",
      "[epoch 1][1980/5716] loss 1.7745\n",
      "[epoch 1][2040/5716] loss 1.6722\n",
      "[epoch 1][2100/5716] loss 0.3391\n",
      "[epoch 1][2160/5716] loss 1.3559\n",
      "[epoch 1][2220/5716] loss 1.9069\n",
      "[epoch 1][2280/5716] loss 2.1176\n",
      "[epoch 1][2340/5716] loss 3.3996\n",
      "[epoch 1][2400/5716] loss 1.7171\n",
      "[epoch 1][2460/5716] loss 5.6420\n",
      "[epoch 1][2520/5716] loss 3.3120\n",
      "[epoch 1][2580/5716] loss 3.0409\n",
      "[epoch 1][2640/5716] loss 1.4532\n",
      "[epoch 1][2700/5716] loss 8.4566\n",
      "[epoch 1][2760/5716] loss 1.8816\n",
      "[epoch 1][2820/5716] loss 1.8307\n",
      "[epoch 1][2880/5716] loss 0.9856\n",
      "[epoch 1][2940/5716] loss 4.0915\n",
      "[epoch 1][3000/5716] loss 1.5913\n",
      "[epoch 1][3060/5716] loss 3.2016\n",
      "[epoch 1][3120/5716] loss 0.7964\n",
      "[epoch 1][3180/5716] loss 5.2546\n",
      "[epoch 1][3240/5716] loss 4.5876\n",
      "[epoch 1][3300/5716] loss 1.2726\n",
      "[epoch 1][3360/5716] loss 0.8883\n",
      "[epoch 1][3420/5716] loss 2.6919\n",
      "[epoch 1][3480/5716] loss 10.3148\n",
      "[epoch 1][3540/5716] loss 3.0715\n",
      "[epoch 1][3600/5716] loss 4.7926\n",
      "[epoch 1][3660/5716] loss 4.8738\n",
      "[epoch 1][3720/5716] loss 1.2079\n",
      "[epoch 1][3780/5716] loss 0.6603\n",
      "[epoch 1][3840/5716] loss 5.2348\n",
      "[epoch 1][3900/5716] loss 0.8394\n",
      "[epoch 1][3960/5716] loss 0.4279\n",
      "[epoch 1][4020/5716] loss 1.2325\n",
      "[epoch 1][4080/5716] loss 5.4856\n",
      "[epoch 1][4140/5716] loss 0.9017\n",
      "[epoch 1][4200/5716] loss 0.9061\n",
      "[epoch 1][4260/5716] loss 0.2068\n",
      "[epoch 1][4320/5716] loss 0.2539\n",
      "[epoch 1][4380/5716] loss 1.4863\n",
      "[epoch 1][4440/5716] loss 3.1495\n",
      "[epoch 1][4500/5716] loss 0.7153\n",
      "[epoch 1][4560/5716] loss 3.3042\n",
      "[epoch 1][4620/5716] loss 1.5361\n",
      "[epoch 1][4680/5716] loss 3.5439\n",
      "[epoch 1][4740/5716] loss 2.6184\n",
      "[epoch 1][4800/5716] loss 3.1369\n",
      "[epoch 1][4860/5716] loss 1.7906\n",
      "[epoch 1][4920/5716] loss 0.2634\n",
      "[epoch 1][4980/5716] loss 5.3710\n",
      "[epoch 1][5040/5716] loss 0.6904\n",
      "[epoch 1][5100/5716] loss 3.6078\n",
      "[epoch 1][5160/5716] loss 3.9129\n",
      "[epoch 1][5220/5716] loss 0.3182\n",
      "[epoch 1][5280/5716] loss 0.3104\n",
      "[epoch 1][5340/5716] loss 1.4439\n",
      "[epoch 1][5400/5716] loss 0.6185\n",
      "[epoch 1][5460/5716] loss 1.7615\n",
      "[epoch 1][5520/5716] loss 1.4002\n",
      "[epoch 1][5580/5716] loss 5.5814\n",
      "[epoch 1][5640/5716] loss 5.0302\n",
      "[epoch 1][5700/5716] loss 1.6639\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 1] mAP on test data: 0.25%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 2 learning rate 0.000010\n",
      "\n",
      "[epoch 2][0/5716] loss 1.3955\n",
      "[epoch 2][60/5716] loss 0.4646\n",
      "[epoch 2][120/5716] loss 0.1418\n",
      "[epoch 2][180/5716] loss 1.8375\n",
      "[epoch 2][240/5716] loss 3.2670\n",
      "[epoch 2][300/5716] loss 3.2733\n",
      "[epoch 2][360/5716] loss 10.0063\n",
      "[epoch 2][420/5716] loss 1.9367\n",
      "[epoch 2][480/5716] loss 0.4648\n",
      "[epoch 2][540/5716] loss 1.4552\n",
      "[epoch 2][600/5716] loss 2.4911\n",
      "[epoch 2][660/5716] loss 0.9741\n",
      "[epoch 2][720/5716] loss 1.4664\n",
      "[epoch 2][780/5716] loss 1.1468\n",
      "[epoch 2][840/5716] loss 0.6216\n",
      "[epoch 2][900/5716] loss 0.0719\n",
      "[epoch 2][960/5716] loss 2.6630\n",
      "[epoch 2][1020/5716] loss 1.3826\n",
      "[epoch 2][1080/5716] loss 8.1291\n",
      "[epoch 2][1140/5716] loss 0.6314\n",
      "[epoch 2][1200/5716] loss 0.5210\n",
      "[epoch 2][1260/5716] loss 8.3204\n",
      "[epoch 2][1320/5716] loss 9.5697\n",
      "[epoch 2][1380/5716] loss 3.6423\n",
      "[epoch 2][1440/5716] loss 4.1780\n",
      "[epoch 2][1500/5716] loss 5.2411\n",
      "[epoch 2][1560/5716] loss 10.1437\n",
      "[epoch 2][1620/5716] loss 2.4736\n",
      "[epoch 2][1680/5716] loss 2.9565\n",
      "[epoch 2][1740/5716] loss 0.8043\n",
      "[epoch 2][1800/5716] loss 0.6595\n",
      "[epoch 2][1860/5716] loss 3.5896\n",
      "[epoch 2][1920/5716] loss 10.5845\n",
      "[epoch 2][1980/5716] loss 0.7942\n",
      "[epoch 2][2040/5716] loss 1.0087\n",
      "[epoch 2][2100/5716] loss 1.7998\n",
      "[epoch 2][2160/5716] loss 2.0918\n",
      "[epoch 2][2220/5716] loss 0.9035\n",
      "[epoch 2][2280/5716] loss 4.4160\n",
      "[epoch 2][2340/5716] loss 2.0481\n",
      "[epoch 2][2400/5716] loss 1.0847\n",
      "[epoch 2][2460/5716] loss 0.8888\n",
      "[epoch 2][2520/5716] loss 2.2798\n",
      "[epoch 2][2580/5716] loss 5.6154\n",
      "[epoch 2][2640/5716] loss 0.4453\n",
      "[epoch 2][2700/5716] loss 0.7311\n",
      "[epoch 2][2760/5716] loss 1.0865\n",
      "[epoch 2][2820/5716] loss 2.6010\n",
      "[epoch 2][2880/5716] loss 3.9737\n",
      "[epoch 2][2940/5716] loss 4.3927\n",
      "[epoch 2][3000/5716] loss 4.7374\n",
      "[epoch 2][3060/5716] loss 0.7189\n",
      "[epoch 2][3120/5716] loss 2.0389\n",
      "[epoch 2][3180/5716] loss 7.5440\n",
      "[epoch 2][3240/5716] loss 1.2317\n",
      "[epoch 2][3300/5716] loss 1.2083\n",
      "[epoch 2][3360/5716] loss 0.4401\n",
      "[epoch 2][3420/5716] loss 3.4973\n",
      "[epoch 2][3480/5716] loss 3.7280\n",
      "[epoch 2][3540/5716] loss 6.9387\n",
      "[epoch 2][3600/5716] loss 4.2246\n",
      "[epoch 2][3660/5716] loss 1.5679\n",
      "[epoch 2][3720/5716] loss 1.2497\n",
      "[epoch 2][3780/5716] loss 0.6895\n",
      "[epoch 2][3840/5716] loss 3.0877\n",
      "[epoch 2][3900/5716] loss 0.4388\n",
      "[epoch 2][3960/5716] loss 2.3417\n",
      "[epoch 2][4020/5716] loss 4.4985\n",
      "[epoch 2][4080/5716] loss 4.0956\n",
      "[epoch 2][4140/5716] loss 0.3804\n",
      "[epoch 2][4200/5716] loss 0.6566\n",
      "[epoch 2][4260/5716] loss 0.9720\n",
      "[epoch 2][4320/5716] loss 0.4195\n",
      "[epoch 2][4380/5716] loss 2.4790\n",
      "[epoch 2][4440/5716] loss 3.0870\n",
      "[epoch 2][4500/5716] loss 0.8979\n",
      "[epoch 2][4560/5716] loss 2.9044\n",
      "[epoch 2][4620/5716] loss 3.6408\n",
      "[epoch 2][4680/5716] loss 3.5311\n",
      "[epoch 2][4740/5716] loss 2.4636\n",
      "[epoch 2][4800/5716] loss 2.3392\n",
      "[epoch 2][4860/5716] loss 1.0313\n",
      "[epoch 2][4920/5716] loss 1.2215\n",
      "[epoch 2][4980/5716] loss 1.0011\n",
      "[epoch 2][5040/5716] loss 2.7341\n",
      "[epoch 2][5100/5716] loss 1.0896\n",
      "[epoch 2][5160/5716] loss 2.3015\n",
      "[epoch 2][5220/5716] loss 2.9389\n",
      "[epoch 2][5280/5716] loss 1.9581\n",
      "[epoch 2][5340/5716] loss 2.4115\n",
      "[epoch 2][5400/5716] loss 1.6774\n",
      "[epoch 2][5460/5716] loss 0.7617\n",
      "[epoch 2][5520/5716] loss 2.5790\n",
      "[epoch 2][5580/5716] loss 4.1491\n",
      "[epoch 2][5640/5716] loss 3.4483\n",
      "[epoch 2][5700/5716] loss 0.3546\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 2] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 3 learning rate 0.000010\n",
      "\n",
      "[epoch 3][0/5716] loss 0.3778\n",
      "[epoch 3][60/5716] loss 0.7559\n",
      "[epoch 3][120/5716] loss 5.6290\n",
      "[epoch 3][180/5716] loss 4.9909\n",
      "[epoch 3][240/5716] loss 2.0223\n",
      "[epoch 3][300/5716] loss 0.2451\n",
      "[epoch 3][360/5716] loss 1.8550\n",
      "[epoch 3][420/5716] loss 1.5596\n",
      "[epoch 3][480/5716] loss 1.7085\n",
      "[epoch 3][540/5716] loss 1.4015\n",
      "[epoch 3][600/5716] loss 2.4971\n",
      "[epoch 3][660/5716] loss 1.1185\n",
      "[epoch 3][720/5716] loss 3.1452\n",
      "[epoch 3][780/5716] loss 0.8672\n",
      "[epoch 3][840/5716] loss 3.7236\n",
      "[epoch 3][900/5716] loss 0.2827\n",
      "[epoch 3][960/5716] loss 0.5728\n",
      "[epoch 3][1020/5716] loss 3.7104\n",
      "[epoch 3][1080/5716] loss 1.1073\n",
      "[epoch 3][1140/5716] loss 4.3991\n",
      "[epoch 3][1200/5716] loss 1.8760\n",
      "[epoch 3][1260/5716] loss 2.9532\n",
      "[epoch 3][1320/5716] loss 0.3978\n",
      "[epoch 3][1380/5716] loss 1.5390\n",
      "[epoch 3][1440/5716] loss 0.3718\n",
      "[epoch 3][1500/5716] loss 2.1309\n",
      "[epoch 3][1560/5716] loss 1.2447\n",
      "[epoch 3][1620/5716] loss 1.6229\n",
      "[epoch 3][1680/5716] loss 1.8640\n",
      "[epoch 3][1740/5716] loss 1.4525\n",
      "[epoch 3][1800/5716] loss 0.7485\n",
      "[epoch 3][1860/5716] loss 5.7121\n",
      "[epoch 3][1920/5716] loss 0.0142\n",
      "[epoch 3][1980/5716] loss 0.5495\n",
      "[epoch 3][2040/5716] loss 4.9322\n",
      "[epoch 3][2100/5716] loss 2.4015\n",
      "[epoch 3][2160/5716] loss 5.2598\n",
      "[epoch 3][2220/5716] loss 0.1278\n",
      "[epoch 3][2280/5716] loss 2.3025\n",
      "[epoch 3][2340/5716] loss 0.3613\n",
      "[epoch 3][2400/5716] loss 0.5496\n",
      "[epoch 3][2460/5716] loss 2.5960\n",
      "[epoch 3][2520/5716] loss 1.2805\n",
      "[epoch 3][2580/5716] loss 4.8348\n",
      "[epoch 3][2640/5716] loss 2.2459\n",
      "[epoch 3][2700/5716] loss 5.9168\n",
      "[epoch 3][2760/5716] loss 3.4814\n",
      "[epoch 3][2820/5716] loss 2.8748\n",
      "[epoch 3][2880/5716] loss 5.2255\n",
      "[epoch 3][2940/5716] loss 1.2094\n",
      "[epoch 3][3000/5716] loss 2.0093\n",
      "[epoch 3][3060/5716] loss 0.6155\n",
      "[epoch 3][3120/5716] loss 4.0035\n",
      "[epoch 3][3180/5716] loss 2.9811\n",
      "[epoch 3][3240/5716] loss 2.2886\n",
      "[epoch 3][3300/5716] loss 0.7281\n",
      "[epoch 3][3360/5716] loss 0.0466\n",
      "[epoch 3][3420/5716] loss 1.7696\n",
      "[epoch 3][3480/5716] loss 1.1332\n",
      "[epoch 3][3540/5716] loss 0.8313\n",
      "[epoch 3][3600/5716] loss 1.7595\n",
      "[epoch 3][3660/5716] loss 0.0809\n",
      "[epoch 3][3720/5716] loss 3.5405\n",
      "[epoch 3][3780/5716] loss 2.7360\n",
      "[epoch 3][3840/5716] loss 1.3628\n",
      "[epoch 3][3900/5716] loss 0.6226\n",
      "[epoch 3][3960/5716] loss 4.4640\n",
      "[epoch 3][4020/5716] loss 0.5185\n",
      "[epoch 3][4080/5716] loss 2.3749\n",
      "[epoch 3][4140/5716] loss 0.1416\n",
      "[epoch 3][4200/5716] loss 3.4613\n",
      "[epoch 3][4260/5716] loss 2.1863\n",
      "[epoch 3][4320/5716] loss 1.0061\n",
      "[epoch 3][4380/5716] loss 3.1136\n",
      "[epoch 3][4440/5716] loss 0.3932\n",
      "[epoch 3][4500/5716] loss 3.1286\n",
      "[epoch 3][4560/5716] loss 0.6177\n",
      "[epoch 3][4620/5716] loss 2.8660\n",
      "[epoch 3][4680/5716] loss 0.9839\n",
      "[epoch 3][4740/5716] loss 0.4340\n",
      "[epoch 3][4800/5716] loss 7.0036\n",
      "[epoch 3][4860/5716] loss 1.3781\n",
      "[epoch 3][4920/5716] loss 0.9021\n",
      "[epoch 3][4980/5716] loss 0.6558\n",
      "[epoch 3][5040/5716] loss 2.1014\n",
      "[epoch 3][5100/5716] loss 4.6023\n",
      "[epoch 3][5160/5716] loss 3.6677\n",
      "[epoch 3][5220/5716] loss 1.0783\n",
      "[epoch 3][5280/5716] loss 2.4173\n",
      "[epoch 3][5340/5716] loss 0.2313\n",
      "[epoch 3][5400/5716] loss 0.2629\n",
      "[epoch 3][5460/5716] loss 2.6413\n",
      "[epoch 3][5520/5716] loss 0.8551\n",
      "[epoch 3][5580/5716] loss 0.7854\n",
      "[epoch 3][5640/5716] loss 1.2989\n",
      "[epoch 3][5700/5716] loss 3.5692\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 3] mAP on test data: 0.25%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 4 learning rate 0.000010\n",
      "\n",
      "[epoch 4][0/5716] loss 7.7579\n",
      "[epoch 4][60/5716] loss 2.1526\n",
      "[epoch 4][120/5716] loss 0.3000\n",
      "[epoch 4][180/5716] loss 0.3009\n",
      "[epoch 4][240/5716] loss 0.1319\n",
      "[epoch 4][300/5716] loss 1.1623\n",
      "[epoch 4][360/5716] loss 0.1884\n",
      "[epoch 4][420/5716] loss 1.0324\n",
      "[epoch 4][480/5716] loss 1.2183\n",
      "[epoch 4][540/5716] loss 3.1316\n",
      "[epoch 4][600/5716] loss 0.0158\n",
      "[epoch 4][660/5716] loss 0.6492\n",
      "[epoch 4][720/5716] loss 0.6431\n",
      "[epoch 4][780/5716] loss 3.4957\n",
      "[epoch 4][840/5716] loss 0.8941\n",
      "[epoch 4][900/5716] loss 1.7414\n",
      "[epoch 4][960/5716] loss 0.3337\n",
      "[epoch 4][1020/5716] loss 0.0866\n",
      "[epoch 4][1080/5716] loss 1.3262\n",
      "[epoch 4][1140/5716] loss 2.8061\n",
      "[epoch 4][1200/5716] loss 0.1591\n",
      "[epoch 4][1260/5716] loss 2.5686\n",
      "[epoch 4][1320/5716] loss 0.6469\n",
      "[epoch 4][1380/5716] loss 0.7835\n",
      "[epoch 4][1440/5716] loss 0.4475\n",
      "[epoch 4][1500/5716] loss 0.7599\n",
      "[epoch 4][1560/5716] loss 5.4840\n",
      "[epoch 4][1620/5716] loss 3.7159\n",
      "[epoch 4][1680/5716] loss 0.4157\n",
      "[epoch 4][1740/5716] loss 2.2378\n",
      "[epoch 4][1800/5716] loss 1.3078\n",
      "[epoch 4][1860/5716] loss 4.8267\n",
      "[epoch 4][1920/5716] loss 2.4540\n",
      "[epoch 4][1980/5716] loss 0.5915\n",
      "[epoch 4][2040/5716] loss 0.2845\n",
      "[epoch 4][2100/5716] loss 0.4526\n",
      "[epoch 4][2160/5716] loss 0.5826\n",
      "[epoch 4][2220/5716] loss 1.7259\n",
      "[epoch 4][2280/5716] loss 1.4461\n",
      "[epoch 4][2340/5716] loss 3.4123\n",
      "[epoch 4][2400/5716] loss 1.0498\n",
      "[epoch 4][2460/5716] loss 0.3627\n",
      "[epoch 4][2520/5716] loss 0.0770\n",
      "[epoch 4][2580/5716] loss 5.2282\n",
      "[epoch 4][2640/5716] loss 0.0871\n",
      "[epoch 4][2700/5716] loss 1.2947\n",
      "[epoch 4][2760/5716] loss 2.9716\n",
      "[epoch 4][2820/5716] loss 0.0032\n",
      "[epoch 4][2880/5716] loss 1.6037\n",
      "[epoch 4][2940/5716] loss 0.8564\n",
      "[epoch 4][3000/5716] loss 2.0281\n",
      "[epoch 4][3060/5716] loss 0.3406\n",
      "[epoch 4][3120/5716] loss 0.4221\n",
      "[epoch 4][3180/5716] loss 2.5438\n",
      "[epoch 4][3240/5716] loss 0.3315\n",
      "[epoch 4][3300/5716] loss 4.4416\n",
      "[epoch 4][3360/5716] loss 2.8373\n",
      "[epoch 4][3420/5716] loss 0.6409\n",
      "[epoch 4][3480/5716] loss 2.0336\n",
      "[epoch 4][3540/5716] loss 0.7030\n",
      "[epoch 4][3600/5716] loss 0.3754\n",
      "[epoch 4][3660/5716] loss 1.1681\n",
      "[epoch 4][3720/5716] loss 0.6022\n",
      "[epoch 4][3780/5716] loss 3.8183\n",
      "[epoch 4][3840/5716] loss 2.4841\n",
      "[epoch 4][3900/5716] loss 3.4964\n",
      "[epoch 4][3960/5716] loss 1.4534\n",
      "[epoch 4][4020/5716] loss 0.8916\n",
      "[epoch 4][4080/5716] loss 1.7978\n",
      "[epoch 4][4140/5716] loss 0.5002\n",
      "[epoch 4][4200/5716] loss 2.9850\n",
      "[epoch 4][4260/5716] loss 1.7472\n",
      "[epoch 4][4320/5716] loss 10.1682\n",
      "[epoch 4][4380/5716] loss 1.8733\n",
      "[epoch 4][4440/5716] loss 3.1797\n",
      "[epoch 4][4500/5716] loss 0.6502\n",
      "[epoch 4][4560/5716] loss 0.3184\n",
      "[epoch 4][4620/5716] loss 2.9048\n",
      "[epoch 4][4680/5716] loss 0.1038\n",
      "[epoch 4][4740/5716] loss 2.5900\n",
      "[epoch 4][4800/5716] loss 0.3800\n",
      "[epoch 4][4860/5716] loss 0.2309\n",
      "[epoch 4][4920/5716] loss 0.5126\n",
      "[epoch 4][4980/5716] loss 2.7159\n",
      "[epoch 4][5040/5716] loss 4.5369\n",
      "[epoch 4][5100/5716] loss 0.2732\n",
      "[epoch 4][5160/5716] loss 2.7177\n",
      "[epoch 4][5220/5716] loss 3.8224\n",
      "[epoch 4][5280/5716] loss 0.2807\n",
      "[epoch 4][5340/5716] loss 0.1937\n",
      "[epoch 4][5400/5716] loss 2.3652\n",
      "[epoch 4][5460/5716] loss 4.0507\n",
      "[epoch 4][5520/5716] loss 5.2977\n",
      "[epoch 4][5580/5716] loss 1.0108\n",
      "[epoch 4][5640/5716] loss 0.5206\n",
      "[epoch 4][5700/5716] loss 0.7428\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 4] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 5 learning rate 0.000003\n",
      "\n",
      "[epoch 5][0/5716] loss 0.4442\n",
      "[epoch 5][60/5716] loss 2.1246\n",
      "[epoch 5][120/5716] loss 1.1930\n",
      "[epoch 5][180/5716] loss 0.6625\n",
      "[epoch 5][240/5716] loss 1.4957\n",
      "[epoch 5][300/5716] loss 1.8864\n",
      "[epoch 5][360/5716] loss 0.6882\n",
      "[epoch 5][420/5716] loss 0.4721\n",
      "[epoch 5][480/5716] loss 2.5826\n",
      "[epoch 5][540/5716] loss 1.6745\n",
      "[epoch 5][600/5716] loss 0.6809\n",
      "[epoch 5][660/5716] loss 2.5323\n",
      "[epoch 5][720/5716] loss 2.4299\n",
      "[epoch 5][780/5716] loss 1.4242\n",
      "[epoch 5][840/5716] loss 1.0652\n",
      "[epoch 5][900/5716] loss 1.2892\n",
      "[epoch 5][960/5716] loss 0.4510\n",
      "[epoch 5][1020/5716] loss 3.1120\n",
      "[epoch 5][1080/5716] loss 1.6147\n",
      "[epoch 5][1140/5716] loss 2.1649\n",
      "[epoch 5][1200/5716] loss 1.1089\n",
      "[epoch 5][1260/5716] loss 0.4445\n",
      "[epoch 5][1320/5716] loss 2.2498\n",
      "[epoch 5][1380/5716] loss 2.4493\n",
      "[epoch 5][1440/5716] loss 4.7862\n",
      "[epoch 5][1500/5716] loss 2.6145\n",
      "[epoch 5][1560/5716] loss 3.6978\n",
      "[epoch 5][1620/5716] loss 2.4231\n",
      "[epoch 5][1680/5716] loss 3.5229\n",
      "[epoch 5][1740/5716] loss 2.4050\n",
      "[epoch 5][1800/5716] loss 0.2303\n",
      "[epoch 5][1860/5716] loss 0.9868\n",
      "[epoch 5][1920/5716] loss 0.9174\n",
      "[epoch 5][1980/5716] loss 4.1797\n",
      "[epoch 5][2040/5716] loss 2.2631\n",
      "[epoch 5][2100/5716] loss 6.8270\n",
      "[epoch 5][2160/5716] loss 4.4914\n",
      "[epoch 5][2220/5716] loss 0.4080\n",
      "[epoch 5][2280/5716] loss 4.2348\n",
      "[epoch 5][2340/5716] loss 3.4675\n",
      "[epoch 5][2400/5716] loss 2.0473\n",
      "[epoch 5][2460/5716] loss 1.2127\n",
      "[epoch 5][2520/5716] loss 0.2589\n",
      "[epoch 5][2580/5716] loss 0.6628\n",
      "[epoch 5][2640/5716] loss 0.8438\n",
      "[epoch 5][2700/5716] loss 0.2806\n",
      "[epoch 5][2760/5716] loss 0.0295\n",
      "[epoch 5][2820/5716] loss 0.5682\n",
      "[epoch 5][2880/5716] loss 7.2163\n",
      "[epoch 5][2940/5716] loss 4.7481\n",
      "[epoch 5][3000/5716] loss 6.6842\n",
      "[epoch 5][3060/5716] loss 0.8944\n",
      "[epoch 5][3120/5716] loss 0.1331\n",
      "[epoch 5][3180/5716] loss 2.3584\n",
      "[epoch 5][3240/5716] loss 2.1128\n",
      "[epoch 5][3300/5716] loss 1.8292\n",
      "[epoch 5][3360/5716] loss 0.2373\n",
      "[epoch 5][3420/5716] loss 0.0106\n",
      "[epoch 5][3480/5716] loss 0.2122\n",
      "[epoch 5][3540/5716] loss 0.3475\n",
      "[epoch 5][3600/5716] loss 0.4111\n",
      "[epoch 5][3660/5716] loss 0.5158\n",
      "[epoch 5][3720/5716] loss 4.6998\n",
      "[epoch 5][3780/5716] loss 0.2763\n",
      "[epoch 5][3840/5716] loss 5.1703\n",
      "[epoch 5][3900/5716] loss 0.2249\n",
      "[epoch 5][3960/5716] loss 2.0905\n",
      "[epoch 5][4020/5716] loss 0.5380\n",
      "[epoch 5][4080/5716] loss 1.3885\n",
      "[epoch 5][4140/5716] loss 0.8051\n",
      "[epoch 5][4200/5716] loss 4.2053\n",
      "[epoch 5][4260/5716] loss 2.9420\n",
      "[epoch 5][4320/5716] loss 0.8278\n",
      "[epoch 5][4380/5716] loss 0.5581\n",
      "[epoch 5][4440/5716] loss 1.7666\n",
      "[epoch 5][4500/5716] loss 4.2531\n",
      "[epoch 5][4560/5716] loss 1.2452\n",
      "[epoch 5][4620/5716] loss 1.4332\n",
      "[epoch 5][4680/5716] loss 2.7460\n",
      "[epoch 5][4740/5716] loss 0.5947\n",
      "[epoch 5][4800/5716] loss 2.7207\n",
      "[epoch 5][4860/5716] loss 2.5865\n",
      "[epoch 5][4920/5716] loss 5.0646\n",
      "[epoch 5][4980/5716] loss 2.2040\n",
      "[epoch 5][5040/5716] loss 4.8600\n",
      "[epoch 5][5100/5716] loss 0.7243\n",
      "[epoch 5][5160/5716] loss 1.1679\n",
      "[epoch 5][5220/5716] loss 1.3477\n",
      "[epoch 5][5280/5716] loss 0.3592\n",
      "[epoch 5][5340/5716] loss 1.1484\n",
      "[epoch 5][5400/5716] loss 0.8904\n",
      "[epoch 5][5460/5716] loss 0.3468\n",
      "[epoch 5][5520/5716] loss 0.1492\n",
      "[epoch 5][5580/5716] loss 0.8612\n",
      "[epoch 5][5640/5716] loss 6.3781\n",
      "[epoch 5][5700/5716] loss 1.5619\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 5] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 6 learning rate 0.000003\n",
      "\n",
      "[epoch 6][0/5716] loss 4.3699\n",
      "[epoch 6][60/5716] loss 0.1446\n",
      "[epoch 6][120/5716] loss 4.2170\n",
      "[epoch 6][180/5716] loss 1.6279\n",
      "[epoch 6][240/5716] loss 0.7298\n",
      "[epoch 6][300/5716] loss 0.1302\n",
      "[epoch 6][360/5716] loss 3.3042\n",
      "[epoch 6][420/5716] loss 0.5280\n",
      "[epoch 6][480/5716] loss 2.8592\n",
      "[epoch 6][540/5716] loss 2.2462\n",
      "[epoch 6][600/5716] loss 4.0550\n",
      "[epoch 6][660/5716] loss 0.4249\n",
      "[epoch 6][720/5716] loss 0.1429\n",
      "[epoch 6][780/5716] loss 0.2356\n",
      "[epoch 6][840/5716] loss 1.8372\n",
      "[epoch 6][900/5716] loss 0.9941\n",
      "[epoch 6][960/5716] loss 0.7782\n",
      "[epoch 6][1020/5716] loss 1.1400\n",
      "[epoch 6][1080/5716] loss 0.7607\n",
      "[epoch 6][1140/5716] loss 1.7375\n",
      "[epoch 6][1200/5716] loss 0.0233\n",
      "[epoch 6][1260/5716] loss 0.4860\n",
      "[epoch 6][1320/5716] loss 1.7261\n",
      "[epoch 6][1380/5716] loss 1.8328\n",
      "[epoch 6][1440/5716] loss 7.0334\n",
      "[epoch 6][1500/5716] loss 1.8101\n",
      "[epoch 6][1560/5716] loss 2.1370\n",
      "[epoch 6][1620/5716] loss 3.5367\n",
      "[epoch 6][1680/5716] loss 5.1780\n",
      "[epoch 6][1740/5716] loss 2.5323\n",
      "[epoch 6][1800/5716] loss 0.8705\n",
      "[epoch 6][1860/5716] loss 4.1614\n",
      "[epoch 6][1920/5716] loss 1.8237\n",
      "[epoch 6][1980/5716] loss 2.0674\n",
      "[epoch 6][2040/5716] loss 2.9649\n",
      "[epoch 6][2100/5716] loss 3.2225\n",
      "[epoch 6][2160/5716] loss 1.9553\n",
      "[epoch 6][2220/5716] loss 0.1894\n",
      "[epoch 6][2280/5716] loss 0.0069\n",
      "[epoch 6][2340/5716] loss 1.5756\n",
      "[epoch 6][2400/5716] loss 4.0391\n",
      "[epoch 6][2460/5716] loss 1.9515\n",
      "[epoch 6][2520/5716] loss 0.3291\n",
      "[epoch 6][2580/5716] loss 1.8875\n",
      "[epoch 6][2640/5716] loss 0.0925\n",
      "[epoch 6][2700/5716] loss 0.1913\n",
      "[epoch 6][2760/5716] loss 3.1024\n",
      "[epoch 6][2820/5716] loss 0.2264\n",
      "[epoch 6][2880/5716] loss 2.5427\n",
      "[epoch 6][2940/5716] loss 0.2041\n",
      "[epoch 6][3000/5716] loss 6.6816\n",
      "[epoch 6][3060/5716] loss 0.9624\n",
      "[epoch 6][3120/5716] loss 4.5403\n",
      "[epoch 6][3180/5716] loss 0.4964\n",
      "[epoch 6][3240/5716] loss 0.9734\n",
      "[epoch 6][3300/5716] loss 0.8398\n",
      "[epoch 6][3360/5716] loss 0.0784\n",
      "[epoch 6][3420/5716] loss 7.5855\n",
      "[epoch 6][3480/5716] loss 0.6303\n",
      "[epoch 6][3540/5716] loss 7.8712\n",
      "[epoch 6][3600/5716] loss 0.0880\n",
      "[epoch 6][3660/5716] loss 2.6930\n",
      "[epoch 6][3720/5716] loss 0.9339\n",
      "[epoch 6][3780/5716] loss 2.0772\n",
      "[epoch 6][3840/5716] loss 1.4836\n",
      "[epoch 6][3900/5716] loss 0.2558\n",
      "[epoch 6][3960/5716] loss 0.3115\n",
      "[epoch 6][4020/5716] loss 8.3220\n",
      "[epoch 6][4080/5716] loss 4.8680\n",
      "[epoch 6][4140/5716] loss 1.3203\n",
      "[epoch 6][4200/5716] loss 1.3001\n",
      "[epoch 6][4260/5716] loss 2.3476\n",
      "[epoch 6][4320/5716] loss 2.7431\n",
      "[epoch 6][4380/5716] loss 1.9793\n",
      "[epoch 6][4440/5716] loss 2.5585\n",
      "[epoch 6][4500/5716] loss 5.1590\n",
      "[epoch 6][4560/5716] loss 1.3805\n",
      "[epoch 6][4620/5716] loss 1.8979\n",
      "[epoch 6][4680/5716] loss 0.3033\n",
      "[epoch 6][4740/5716] loss 2.4055\n",
      "[epoch 6][4800/5716] loss 6.2239\n",
      "[epoch 6][4860/5716] loss 1.0038\n",
      "[epoch 6][4920/5716] loss 0.7586\n",
      "[epoch 6][4980/5716] loss 0.5486\n",
      "[epoch 6][5040/5716] loss 0.5477\n",
      "[epoch 6][5100/5716] loss 1.1935\n",
      "[epoch 6][5160/5716] loss 3.8046\n",
      "[epoch 6][5220/5716] loss 3.2553\n",
      "[epoch 6][5280/5716] loss 1.1959\n",
      "[epoch 6][5340/5716] loss 0.6495\n",
      "[epoch 6][5400/5716] loss 0.4102\n",
      "[epoch 6][5460/5716] loss 0.8192\n",
      "[epoch 6][5520/5716] loss 2.8417\n",
      "[epoch 6][5580/5716] loss 0.2180\n",
      "[epoch 6][5640/5716] loss 8.1583\n",
      "[epoch 6][5700/5716] loss 3.7835\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 6] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 7 learning rate 0.000003\n",
      "\n",
      "[epoch 7][0/5716] loss 0.7237\n",
      "[epoch 7][60/5716] loss 0.9386\n",
      "[epoch 7][120/5716] loss 0.4362\n",
      "[epoch 7][180/5716] loss 0.7213\n",
      "[epoch 7][240/5716] loss 2.3568\n",
      "[epoch 7][300/5716] loss 1.0570\n",
      "[epoch 7][360/5716] loss 0.3540\n",
      "[epoch 7][420/5716] loss 2.2470\n",
      "[epoch 7][480/5716] loss 0.2796\n",
      "[epoch 7][540/5716] loss 0.3395\n",
      "[epoch 7][600/5716] loss 0.8567\n",
      "[epoch 7][660/5716] loss 1.2567\n",
      "[epoch 7][720/5716] loss 0.5012\n",
      "[epoch 7][780/5716] loss 3.6026\n",
      "[epoch 7][840/5716] loss 0.1627\n",
      "[epoch 7][900/5716] loss 2.6120\n",
      "[epoch 7][960/5716] loss 0.6139\n",
      "[epoch 7][1020/5716] loss 6.0033\n",
      "[epoch 7][1080/5716] loss 3.7470\n",
      "[epoch 7][1140/5716] loss 2.5479\n",
      "[epoch 7][1200/5716] loss 9.4435\n",
      "[epoch 7][1260/5716] loss 1.1770\n",
      "[epoch 7][1320/5716] loss 0.8385\n",
      "[epoch 7][1380/5716] loss 1.5473\n",
      "[epoch 7][1440/5716] loss 10.1372\n",
      "[epoch 7][1500/5716] loss 0.8981\n",
      "[epoch 7][1560/5716] loss 0.3556\n",
      "[epoch 7][1620/5716] loss 1.7108\n",
      "[epoch 7][1680/5716] loss 11.2710\n",
      "[epoch 7][1740/5716] loss 1.7221\n",
      "[epoch 7][1800/5716] loss 10.7625\n",
      "[epoch 7][1860/5716] loss 2.2072\n",
      "[epoch 7][1920/5716] loss 3.3507\n",
      "[epoch 7][1980/5716] loss 0.6271\n",
      "[epoch 7][2040/5716] loss 2.6263\n",
      "[epoch 7][2100/5716] loss 1.3848\n",
      "[epoch 7][2160/5716] loss 1.3825\n",
      "[epoch 7][2220/5716] loss 3.6675\n",
      "[epoch 7][2280/5716] loss 1.1839\n",
      "[epoch 7][2340/5716] loss 0.8731\n",
      "[epoch 7][2400/5716] loss 5.3431\n",
      "[epoch 7][2460/5716] loss 0.9593\n",
      "[epoch 7][2520/5716] loss 0.1872\n",
      "[epoch 7][2580/5716] loss 0.0717\n",
      "[epoch 7][2640/5716] loss 0.4286\n",
      "[epoch 7][2700/5716] loss 0.4262\n",
      "[epoch 7][2760/5716] loss 0.0465\n",
      "[epoch 7][2820/5716] loss 1.8795\n",
      "[epoch 7][2880/5716] loss 0.2070\n",
      "[epoch 7][2940/5716] loss 0.8252\n",
      "[epoch 7][3000/5716] loss 1.0876\n",
      "[epoch 7][3060/5716] loss 3.1962\n",
      "[epoch 7][3120/5716] loss 4.3425\n",
      "[epoch 7][3180/5716] loss 0.9085\n",
      "[epoch 7][3240/5716] loss 1.5939\n",
      "[epoch 7][3300/5716] loss 0.0477\n",
      "[epoch 7][3360/5716] loss 0.2657\n",
      "[epoch 7][3420/5716] loss 1.5658\n",
      "[epoch 7][3480/5716] loss 0.2695\n",
      "[epoch 7][3540/5716] loss 2.2886\n",
      "[epoch 7][3600/5716] loss 2.8305\n",
      "[epoch 7][3660/5716] loss 0.1759\n",
      "[epoch 7][3720/5716] loss 3.1761\n",
      "[epoch 7][3780/5716] loss 3.9705\n",
      "[epoch 7][3840/5716] loss 1.8601\n",
      "[epoch 7][3900/5716] loss 2.4542\n",
      "[epoch 7][3960/5716] loss 0.4913\n",
      "[epoch 7][4020/5716] loss 1.9164\n",
      "[epoch 7][4080/5716] loss 0.2560\n",
      "[epoch 7][4140/5716] loss 1.1707\n",
      "[epoch 7][4200/5716] loss 0.2505\n",
      "[epoch 7][4260/5716] loss 0.4918\n",
      "[epoch 7][4320/5716] loss 0.3980\n",
      "[epoch 7][4380/5716] loss 3.6535\n",
      "[epoch 7][4440/5716] loss 0.0377\n",
      "[epoch 7][4500/5716] loss 2.0065\n",
      "[epoch 7][4560/5716] loss 0.6202\n",
      "[epoch 7][4620/5716] loss 0.7055\n",
      "[epoch 7][4680/5716] loss 0.0209\n",
      "[epoch 7][4740/5716] loss 7.9894\n",
      "[epoch 7][4800/5716] loss 1.8899\n",
      "[epoch 7][4860/5716] loss 0.8741\n",
      "[epoch 7][4920/5716] loss 9.7138\n",
      "[epoch 7][4980/5716] loss 0.4030\n",
      "[epoch 7][5040/5716] loss 4.2218\n",
      "[epoch 7][5100/5716] loss 0.1707\n",
      "[epoch 7][5160/5716] loss 0.4213\n",
      "[epoch 7][5220/5716] loss 1.4151\n",
      "[epoch 7][5280/5716] loss 4.3399\n",
      "[epoch 7][5340/5716] loss 0.7167\n",
      "[epoch 7][5400/5716] loss 0.1303\n",
      "[epoch 7][5460/5716] loss 3.1143\n",
      "[epoch 7][5520/5716] loss 1.0163\n",
      "[epoch 7][5580/5716] loss 2.8742\n",
      "[epoch 7][5640/5716] loss 3.8713\n",
      "[epoch 7][5700/5716] loss 0.7102\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 7] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 8 learning rate 0.000003\n",
      "\n",
      "[epoch 8][0/5716] loss 2.0317\n",
      "[epoch 8][60/5716] loss 3.3109\n",
      "[epoch 8][120/5716] loss 0.9479\n",
      "[epoch 8][180/5716] loss 2.6126\n",
      "[epoch 8][240/5716] loss 0.6545\n",
      "[epoch 8][300/5716] loss 0.9733\n",
      "[epoch 8][360/5716] loss 0.9469\n",
      "[epoch 8][420/5716] loss 2.7947\n",
      "[epoch 8][480/5716] loss 1.1415\n",
      "[epoch 8][540/5716] loss 6.4959\n",
      "[epoch 8][600/5716] loss 2.7222\n",
      "[epoch 8][660/5716] loss 1.6592\n",
      "[epoch 8][720/5716] loss 5.6078\n",
      "[epoch 8][780/5716] loss 0.6930\n",
      "[epoch 8][840/5716] loss 0.9984\n",
      "[epoch 8][900/5716] loss 2.4026\n",
      "[epoch 8][960/5716] loss 0.5637\n",
      "[epoch 8][1020/5716] loss 4.3063\n",
      "[epoch 8][1080/5716] loss 0.6520\n",
      "[epoch 8][1140/5716] loss 0.0117\n",
      "[epoch 8][1200/5716] loss 2.5912\n",
      "[epoch 8][1260/5716] loss 1.1384\n",
      "[epoch 8][1320/5716] loss 5.7939\n",
      "[epoch 8][1380/5716] loss 1.8690\n",
      "[epoch 8][1440/5716] loss 2.1756\n",
      "[epoch 8][1500/5716] loss 10.2623\n",
      "[epoch 8][1560/5716] loss 0.4238\n",
      "[epoch 8][1620/5716] loss 4.1854\n",
      "[epoch 8][1680/5716] loss 2.8325\n",
      "[epoch 8][1740/5716] loss 2.1448\n",
      "[epoch 8][1800/5716] loss 2.0448\n",
      "[epoch 8][1860/5716] loss 4.1428\n",
      "[epoch 8][1920/5716] loss 2.6114\n",
      "[epoch 8][1980/5716] loss 1.4890\n",
      "[epoch 8][2040/5716] loss 0.5463\n",
      "[epoch 8][2100/5716] loss 1.2921\n",
      "[epoch 8][2160/5716] loss 0.1265\n",
      "[epoch 8][2220/5716] loss 1.6732\n",
      "[epoch 8][2280/5716] loss 0.8914\n",
      "[epoch 8][2340/5716] loss 0.2631\n",
      "[epoch 8][2400/5716] loss 2.7607\n",
      "[epoch 8][2460/5716] loss 4.4198\n",
      "[epoch 8][2520/5716] loss 2.6123\n",
      "[epoch 8][2580/5716] loss 0.6140\n",
      "[epoch 8][2640/5716] loss 0.1350\n",
      "[epoch 8][2700/5716] loss 1.7633\n",
      "[epoch 8][2760/5716] loss 0.3747\n",
      "[epoch 8][2820/5716] loss 7.1692\n",
      "[epoch 8][2880/5716] loss 3.7954\n",
      "[epoch 8][2940/5716] loss 3.3107\n",
      "[epoch 8][3000/5716] loss 2.3831\n",
      "[epoch 8][3060/5716] loss 1.1286\n",
      "[epoch 8][3120/5716] loss 1.7761\n",
      "[epoch 8][3180/5716] loss 0.8542\n",
      "[epoch 8][3240/5716] loss 1.8118\n",
      "[epoch 8][3300/5716] loss 0.5739\n",
      "[epoch 8][3360/5716] loss 2.3421\n",
      "[epoch 8][3420/5716] loss 0.2143\n",
      "[epoch 8][3480/5716] loss 0.0270\n",
      "[epoch 8][3540/5716] loss 2.3830\n",
      "[epoch 8][3600/5716] loss 1.9566\n",
      "[epoch 8][3660/5716] loss 0.3779\n",
      "[epoch 8][3720/5716] loss 1.6604\n",
      "[epoch 8][3780/5716] loss 1.2004\n",
      "[epoch 8][3840/5716] loss 2.9778\n",
      "[epoch 8][3900/5716] loss 0.4277\n",
      "[epoch 8][3960/5716] loss 5.6137\n",
      "[epoch 8][4020/5716] loss 3.0884\n",
      "[epoch 8][4080/5716] loss 0.6565\n",
      "[epoch 8][4140/5716] loss 3.1895\n",
      "[epoch 8][4200/5716] loss 0.0006\n",
      "[epoch 8][4260/5716] loss 9.8477\n",
      "[epoch 8][4320/5716] loss 1.3953\n",
      "[epoch 8][4380/5716] loss 2.0325\n",
      "[epoch 8][4440/5716] loss 2.5690\n",
      "[epoch 8][4500/5716] loss 1.0660\n",
      "[epoch 8][4560/5716] loss 0.6730\n",
      "[epoch 8][4620/5716] loss 5.4375\n",
      "[epoch 8][4680/5716] loss 1.5538\n",
      "[epoch 8][4740/5716] loss 7.0631\n",
      "[epoch 8][4800/5716] loss 0.5039\n",
      "[epoch 8][4860/5716] loss 0.0498\n",
      "[epoch 8][4920/5716] loss 2.7541\n",
      "[epoch 8][4980/5716] loss 0.1624\n",
      "[epoch 8][5040/5716] loss 1.1306\n",
      "[epoch 8][5100/5716] loss 3.4273\n",
      "[epoch 8][5160/5716] loss 0.8194\n",
      "[epoch 8][5220/5716] loss 0.9010\n",
      "[epoch 8][5280/5716] loss 1.3901\n",
      "[epoch 8][5340/5716] loss 0.3807\n",
      "[epoch 8][5400/5716] loss 0.8605\n",
      "[epoch 8][5460/5716] loss 0.6862\n",
      "[epoch 8][5520/5716] loss 0.1546\n",
      "[epoch 8][5580/5716] loss 1.4934\n",
      "[epoch 8][5640/5716] loss 3.8225\n",
      "[epoch 8][5700/5716] loss 1.6355\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 8] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 9 learning rate 0.000003\n",
      "\n",
      "[epoch 9][0/5716] loss 1.0533\n",
      "[epoch 9][60/5716] loss 0.3410\n",
      "[epoch 9][120/5716] loss 0.5655\n",
      "[epoch 9][180/5716] loss 7.8906\n",
      "[epoch 9][240/5716] loss 0.9953\n",
      "[epoch 9][300/5716] loss 0.9694\n",
      "[epoch 9][360/5716] loss 1.3827\n",
      "[epoch 9][420/5716] loss 4.8317\n",
      "[epoch 9][480/5716] loss 1.1901\n",
      "[epoch 9][540/5716] loss 7.3947\n",
      "[epoch 9][600/5716] loss 0.3348\n",
      "[epoch 9][660/5716] loss 0.9054\n",
      "[epoch 9][720/5716] loss 0.3645\n",
      "[epoch 9][780/5716] loss 4.7281\n",
      "[epoch 9][840/5716] loss 1.6651\n",
      "[epoch 9][900/5716] loss 0.0686\n",
      "[epoch 9][960/5716] loss 0.1348\n",
      "[epoch 9][1020/5716] loss 0.7058\n",
      "[epoch 9][1080/5716] loss 0.7735\n",
      "[epoch 9][1140/5716] loss 4.0193\n",
      "[epoch 9][1200/5716] loss 2.1676\n",
      "[epoch 9][1260/5716] loss 1.3923\n",
      "[epoch 9][1320/5716] loss 0.2524\n",
      "[epoch 9][1380/5716] loss 2.6817\n",
      "[epoch 9][1440/5716] loss 0.7243\n",
      "[epoch 9][1500/5716] loss 0.3337\n",
      "[epoch 9][1560/5716] loss 0.0758\n",
      "[epoch 9][1620/5716] loss 0.7756\n",
      "[epoch 9][1680/5716] loss 0.2511\n",
      "[epoch 9][1740/5716] loss 3.0489\n",
      "[epoch 9][1800/5716] loss 2.9687\n",
      "[epoch 9][1860/5716] loss 4.0550\n",
      "[epoch 9][1920/5716] loss 6.1342\n",
      "[epoch 9][1980/5716] loss 2.8381\n",
      "[epoch 9][2040/5716] loss 1.1135\n",
      "[epoch 9][2100/5716] loss 1.2268\n",
      "[epoch 9][2160/5716] loss 1.6237\n",
      "[epoch 9][2220/5716] loss 0.1601\n",
      "[epoch 9][2280/5716] loss 0.4524\n",
      "[epoch 9][2340/5716] loss 0.2396\n",
      "[epoch 9][2400/5716] loss 8.5148\n",
      "[epoch 9][2460/5716] loss 0.5848\n",
      "[epoch 9][2520/5716] loss 0.2976\n",
      "[epoch 9][2580/5716] loss 0.3496\n",
      "[epoch 9][2640/5716] loss 2.7773\n",
      "[epoch 9][2700/5716] loss 0.4155\n",
      "[epoch 9][2760/5716] loss 5.0764\n",
      "[epoch 9][2820/5716] loss 0.2947\n",
      "[epoch 9][2880/5716] loss 2.1690\n",
      "[epoch 9][2940/5716] loss 5.6673\n",
      "[epoch 9][3000/5716] loss 1.4305\n",
      "[epoch 9][3060/5716] loss 3.4371\n",
      "[epoch 9][3120/5716] loss 2.5485\n",
      "[epoch 9][3180/5716] loss 1.7752\n",
      "[epoch 9][3240/5716] loss 1.4911\n",
      "[epoch 9][3300/5716] loss 2.1013\n",
      "[epoch 9][3360/5716] loss 6.2267\n",
      "[epoch 9][3420/5716] loss 0.5895\n",
      "[epoch 9][3480/5716] loss 2.2646\n",
      "[epoch 9][3540/5716] loss 1.0855\n",
      "[epoch 9][3600/5716] loss 2.5915\n",
      "[epoch 9][3660/5716] loss 0.1661\n",
      "[epoch 9][3720/5716] loss 0.7904\n",
      "[epoch 9][3780/5716] loss 0.1552\n",
      "[epoch 9][3840/5716] loss 6.4548\n",
      "[epoch 9][3900/5716] loss 3.4322\n",
      "[epoch 9][3960/5716] loss 3.7103\n",
      "[epoch 9][4020/5716] loss 5.1204\n",
      "[epoch 9][4080/5716] loss 3.3546\n",
      "[epoch 9][4140/5716] loss 1.8183\n",
      "[epoch 9][4200/5716] loss 2.0953\n",
      "[epoch 9][4260/5716] loss 0.3289\n",
      "[epoch 9][4320/5716] loss 0.5568\n",
      "[epoch 9][4380/5716] loss 1.8142\n",
      "[epoch 9][4440/5716] loss 0.9790\n",
      "[epoch 9][4500/5716] loss 3.2772\n",
      "[epoch 9][4560/5716] loss 7.1213\n",
      "[epoch 9][4620/5716] loss 2.3345\n",
      "[epoch 9][4680/5716] loss 1.3108\n",
      "[epoch 9][4740/5716] loss 0.1139\n",
      "[epoch 9][4800/5716] loss 0.3407\n",
      "[epoch 9][4860/5716] loss 0.7210\n",
      "[epoch 9][4920/5716] loss 2.3144\n",
      "[epoch 9][4980/5716] loss 0.0099\n",
      "[epoch 9][5040/5716] loss 0.7826\n",
      "[epoch 9][5100/5716] loss 1.6882\n",
      "[epoch 9][5160/5716] loss 1.4922\n",
      "[epoch 9][5220/5716] loss 4.3428\n",
      "[epoch 9][5280/5716] loss 0.2144\n",
      "[epoch 9][5340/5716] loss 1.0535\n",
      "[epoch 9][5400/5716] loss 0.0370\n",
      "[epoch 9][5460/5716] loss 0.8557\n",
      "[epoch 9][5520/5716] loss 1.9836\n",
      "[epoch 9][5580/5716] loss 0.8698\n",
      "[epoch 9][5640/5716] loss 1.1488\n",
      "[epoch 9][5700/5716] loss 4.0229\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 9] mAP on test data: 0.26%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 10 learning rate 0.000001\n",
      "\n",
      "[epoch 10][0/5716] loss 0.5576\n",
      "[epoch 10][60/5716] loss 4.8502\n",
      "[epoch 10][120/5716] loss 0.5586\n",
      "[epoch 10][180/5716] loss 0.5888\n",
      "[epoch 10][240/5716] loss 0.8131\n",
      "[epoch 10][300/5716] loss 0.0583\n",
      "[epoch 10][360/5716] loss 0.0959\n",
      "[epoch 10][420/5716] loss 0.2523\n",
      "[epoch 10][480/5716] loss 0.9532\n",
      "[epoch 10][540/5716] loss 2.0839\n",
      "[epoch 10][600/5716] loss 2.2934\n",
      "[epoch 10][660/5716] loss 3.9655\n",
      "[epoch 10][720/5716] loss 0.0914\n",
      "[epoch 10][780/5716] loss 1.9569\n",
      "[epoch 10][840/5716] loss 0.3125\n",
      "[epoch 10][900/5716] loss 1.7294\n",
      "[epoch 10][960/5716] loss 0.6390\n",
      "[epoch 10][1020/5716] loss 2.0210\n",
      "[epoch 10][1080/5716] loss 1.2582\n",
      "[epoch 10][1140/5716] loss 1.4508\n",
      "[epoch 10][1200/5716] loss 3.1285\n",
      "[epoch 10][1260/5716] loss 2.7674\n",
      "[epoch 10][1320/5716] loss 0.1306\n",
      "[epoch 10][1380/5716] loss 3.4987\n",
      "[epoch 10][1440/5716] loss 0.2788\n",
      "[epoch 10][1500/5716] loss 0.0526\n",
      "[epoch 10][1560/5716] loss 0.4263\n",
      "[epoch 10][1620/5716] loss 0.9797\n",
      "[epoch 10][1680/5716] loss 3.5437\n",
      "[epoch 10][1740/5716] loss 4.6319\n",
      "[epoch 10][1800/5716] loss 0.4215\n",
      "[epoch 10][1860/5716] loss 0.5601\n",
      "[epoch 10][1920/5716] loss 0.6473\n",
      "[epoch 10][1980/5716] loss 0.2436\n",
      "[epoch 10][2040/5716] loss 0.3041\n",
      "[epoch 10][2100/5716] loss 0.0598\n",
      "[epoch 10][2160/5716] loss 1.1097\n",
      "[epoch 10][2220/5716] loss 1.5229\n",
      "[epoch 10][2280/5716] loss 0.1579\n",
      "[epoch 10][2340/5716] loss 1.2012\n",
      "[epoch 10][2400/5716] loss 1.1283\n",
      "[epoch 10][2460/5716] loss 0.7346\n",
      "[epoch 10][2520/5716] loss 0.6212\n",
      "[epoch 10][2580/5716] loss 2.7621\n",
      "[epoch 10][2640/5716] loss 0.9237\n",
      "[epoch 10][2700/5716] loss 1.3788\n",
      "[epoch 10][2760/5716] loss 2.6765\n",
      "[epoch 10][2820/5716] loss 4.5981\n",
      "[epoch 10][2880/5716] loss 3.2581\n",
      "[epoch 10][2940/5716] loss 1.0352\n",
      "[epoch 10][3000/5716] loss 0.2369\n",
      "[epoch 10][3060/5716] loss 0.3670\n",
      "[epoch 10][3120/5716] loss 8.0058\n",
      "[epoch 10][3180/5716] loss 2.8228\n",
      "[epoch 10][3240/5716] loss 1.2181\n",
      "[epoch 10][3300/5716] loss 0.0537\n",
      "[epoch 10][3360/5716] loss 5.4306\n",
      "[epoch 10][3420/5716] loss 0.1786\n",
      "[epoch 10][3480/5716] loss 0.2688\n",
      "[epoch 10][3540/5716] loss 0.5543\n",
      "[epoch 10][3600/5716] loss 1.4954\n",
      "[epoch 10][3660/5716] loss 0.2619\n",
      "[epoch 10][3720/5716] loss 0.6602\n",
      "[epoch 10][3780/5716] loss 5.3125\n",
      "[epoch 10][3840/5716] loss 0.5843\n",
      "[epoch 10][3900/5716] loss 4.9822\n",
      "[epoch 10][3960/5716] loss 1.5471\n",
      "[epoch 10][4020/5716] loss 0.9511\n",
      "[epoch 10][4080/5716] loss 2.0711\n",
      "[epoch 10][4140/5716] loss 0.1121\n",
      "[epoch 10][4200/5716] loss 0.1031\n",
      "[epoch 10][4260/5716] loss 0.8168\n",
      "[epoch 10][4320/5716] loss 0.5495\n",
      "[epoch 10][4380/5716] loss 2.3416\n",
      "[epoch 10][4440/5716] loss 0.6790\n",
      "[epoch 10][4500/5716] loss 1.8468\n",
      "[epoch 10][4560/5716] loss 1.9748\n",
      "[epoch 10][4620/5716] loss 1.8739\n",
      "[epoch 10][4680/5716] loss 1.3756\n",
      "[epoch 10][4740/5716] loss 0.4359\n",
      "[epoch 10][4800/5716] loss 3.1474\n",
      "[epoch 10][4860/5716] loss 2.0470\n",
      "[epoch 10][4920/5716] loss 0.7149\n",
      "[epoch 10][4980/5716] loss 0.5615\n",
      "[epoch 10][5040/5716] loss 0.1119\n",
      "[epoch 10][5100/5716] loss 0.0635\n",
      "[epoch 10][5160/5716] loss 1.3021\n",
      "[epoch 10][5220/5716] loss 0.5134\n",
      "[epoch 10][5280/5716] loss 0.9761\n",
      "[epoch 10][5340/5716] loss 1.1957\n",
      "[epoch 10][5400/5716] loss 1.5634\n",
      "[epoch 10][5460/5716] loss 1.2234\n",
      "[epoch 10][5520/5716] loss 4.9922\n",
      "[epoch 10][5580/5716] loss 4.3671\n",
      "[epoch 10][5640/5716] loss 2.0933\n",
      "[epoch 10][5700/5716] loss 0.3225\n",
      "----------------------------------------\n",
      "\n",
      "[epoch 10] mAP on test data: 0.27%\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "epoch 11 learning rate 0.000001\n",
      "\n",
      "[epoch 11][0/5716] loss 4.4479\n",
      "[epoch 11][60/5716] loss 0.4151\n",
      "[epoch 11][120/5716] loss 0.3207\n",
      "[epoch 11][180/5716] loss 0.1941\n",
      "[epoch 11][240/5716] loss 0.5060\n",
      "[epoch 11][300/5716] loss 4.1371\n",
      "[epoch 11][360/5716] loss 2.5232\n",
      "[epoch 11][420/5716] loss 2.8087\n",
      "[epoch 11][480/5716] loss 0.6927\n",
      "[epoch 11][540/5716] loss 1.5084\n",
      "[epoch 11][600/5716] loss 2.4672\n",
      "[epoch 11][660/5716] loss 0.5284\n",
      "[epoch 11][720/5716] loss 1.2534\n",
      "[epoch 11][780/5716] loss 2.0909\n",
      "[epoch 11][840/5716] loss 1.5706\n",
      "[epoch 11][900/5716] loss 2.8072\n",
      "[epoch 11][960/5716] loss 1.7856\n",
      "[epoch 11][1020/5716] loss 2.4817\n",
      "[epoch 11][1080/5716] loss 0.9909\n",
      "[epoch 11][1140/5716] loss 1.7347\n",
      "[epoch 11][1200/5716] loss 0.0208\n",
      "[epoch 11][1260/5716] loss 0.6325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2b2712bd9032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/biweekly-report-7-skhadem/weakly-supervised-object-detection/wsddn/wsddn.py\u001b[0m in \u001b[0;36mreg\u001b[0;34m(scores, fc7, boxes)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mhighest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhighest_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;31m# Get any regions with IOU > 0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0moverlap_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhighest_region\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverlap_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# For each region in the high overlapping ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# attempt += 1\n",
    "step = 0\n",
    "log_freq = 60\n",
    "epochs = 30\n",
    "save_freq = 10\n",
    "num_eval_images = 50 # Only run eval on this many images\n",
    "run_name = 'wsddn_reg_attempt_%s'%attempt\n",
    "checkpoints_folder = './checkpoints/%s/'%run_name\n",
    "if not os.path.isdir(checkpoints_folder):\n",
    "    os.mkdir(checkpoints_folder)\n",
    "writer = SummaryWriter('./runs/' + run_name)\n",
    "for epoch in range(epochs + 1):\n",
    "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    for i, stacked_data in enumerate(train_loader):\n",
    "        model.train()\n",
    "        for data in zip(stacked_data[0], stacked_data[1], stacked_data[2]):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "        \n",
    "            image, label, boxes = data\n",
    "            image = image.reshape([1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "\n",
    "            image, label, boxes = image.cuda(), label.cuda(), boxes.cuda()\n",
    "\n",
    "            # forward\n",
    "            pred, x = model.forward(image, boxes)\n",
    "\n",
    "            # backward\n",
    "            loss = model.loss(pred, label) + lam*model.reg(pred, x, boxes)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        # display results\n",
    "        if i % log_freq == 0:\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "            print(\"[epoch %d][%d/%d] loss %.4f\"%(epoch, i, len(train_loader)-1, loss.item()))\n",
    "\n",
    "        # global step\n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_freq == 0 and epoch > 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \n",
    "            '%s/wsdnn_reg_epoch_%s.pth' % (checkpoints_folder, epoch)\n",
    "        )\n",
    "\n",
    "    print('-'*40)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", num_classes=21)\n",
    "        # log scalars\n",
    "        for i, stacked_data in enumerate(test_loader):\n",
    "            for data in zip(stacked_data[0], stacked_data[1], stacked_data[2], stacked_data[3]):\n",
    "                image_test, _, boxes_test, gt_boxes = data\n",
    "                image_test = image_test.reshape([1, image_test.shape[0], image_test.shape[1], image_test.shape[2]])\n",
    "\n",
    "                image_test, boxes_test = image_test.cuda(), boxes_test.cuda()\n",
    "\n",
    "                detections = model.detect(image_test, boxes_test)\n",
    "\n",
    "                formatted_detections = test_set.format_pred(detections)\n",
    "         \n",
    "                metric_fn.add(np.array(formatted_detections), np.array(gt_boxes))\n",
    "            \n",
    "            if i == num_eval_images:\n",
    "                break\n",
    "\n",
    "\n",
    "        mAP = metric_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']\n",
    "        writer.add_scalar('test/mAP', mAP, epoch)\n",
    "        print(\"\\n[epoch %d] mAP on test data: %.2f%%\\n\" % (epoch, mAP))\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, create test set without the normalization\n",
    "test_vis_set = CustomVOC('../edge_boxes_model/model.yml.gz', eval_mode=True, root='../../../data', image_set='val', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-615a08102b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/biweekly-report-7-skhadem/weakly-supervised-object-detection/wsddn/wsddn.py\u001b[0m in \u001b[0;36mdraw_boxes\u001b[0;34m(self, image, pred_boxes, gt_boxes, thresh)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'img'"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    idx = np.random.randint(len(test_set))\n",
    "    img, _, _, _ = test_vis_set[idx]\n",
    "    image, label, boxes, gt_boxes = test_set[idx]\n",
    "    image = image.reshape([1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "    image, label, boxes = image.cuda(), label.cuda(), boxes.cuda()\n",
    "    detections = test_set.format_pred(model.detect(image, boxes))\n",
    "    display(test_set.draw_boxes(transforms.ToTensor()(img), detections, gt_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5385e-07, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reg(pred, x, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
